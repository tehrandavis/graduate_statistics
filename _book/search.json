[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Graduate Statistics Walkthroughs",
    "section": "",
    "text": "This is a landing page for the PSYC 7014 Walkthroughs. Individual weeks and sections can be found on the left sidebar."
  },
  {
    "objectID": "week00/0_1-getting_started.html#step-1-download-r",
    "href": "week00/0_1-getting_started.html#step-1-download-r",
    "title": "\n2  Getting started\n",
    "section": "\n2.1 Step 1: Download R",
    "text": "2.1 Step 1: Download R\nTo begin your journey into the world of statistics using R, you first need to download the latest version of R. Visit the Comprehensive R Archive Network (CRAN) website at https://cran.r-project.org/. This is the official repository for R, containing the latest releases and updates.\nSelect the appropriate version for your operating system (Windows, macOS, or Linux). Follow the download link and instructions provided for your specific platform:\n\nFor Windows users, click on “Download R for Windows” and then click on “base.” Finally, click on “Download R x.x.x for Windows” (where x.x.x represents the latest version number).\nFor macOS users, click on “Download R for (Mac) OS X” and then select the appropriate package based on your macOS version and hardware (Intel or Apple Silicon).\nFor Linux users, click on “Download R for Linux,” choose your distribution, and follow the instructions provided."
  },
  {
    "objectID": "week00/0_1-getting_started.html#step-2-install-r",
    "href": "week00/0_1-getting_started.html#step-2-install-r",
    "title": "\n2  Getting started\n",
    "section": "\n2.2 Step 2: Install R",
    "text": "2.2 Step 2: Install R\nOnce the R installer has been downloaded, locate the installation file in your downloads folder and double-click it to start the installation process. Follow the on-screen instructions, accepting the default settings, to complete the installation."
  },
  {
    "objectID": "week00/0_1-getting_started.html#step-3-download-rstudio",
    "href": "week00/0_1-getting_started.html#step-3-download-rstudio",
    "title": "\n2  Getting started\n",
    "section": "\n2.3 Step 3: Download RStudio",
    "text": "2.3 Step 3: Download RStudio\nRStudio is a powerful integrated development environment (IDE) that simplifies working with R. It provides a user-friendly interface, code editor, and various tools for managing your R projects. Visit the RStudio website at https://www.rstudio.com/products/rstudio/download/ to download the latest version of RStudio.\nScroll down to the “Installers for Supported Platforms” section and choose the appropriate installer for your operating system."
  },
  {
    "objectID": "week00/0_1-getting_started.html#step-4-install-rstudio",
    "href": "week00/0_1-getting_started.html#step-4-install-rstudio",
    "title": "\n2  Getting started\n",
    "section": "\n2.4 Step 4: Install RStudio",
    "text": "2.4 Step 4: Install RStudio\nAfter downloading the RStudio installer, locate the installation file in your downloads folder and double-click it to start the installation process. Follow the on-screen instructions, accepting the default settings, to complete the installation."
  },
  {
    "objectID": "week00/0_1-getting_started.html#step-5-launch-rstudio",
    "href": "week00/0_1-getting_started.html#step-5-launch-rstudio",
    "title": "\n2  Getting started\n",
    "section": "\n2.5 Step 5: Launch RStudio",
    "text": "2.5 Step 5: Launch RStudio\nOnce both R and RStudio have been installed, open RStudio from your applications or programs menu. You should see a window similar to the one below, with a console on the left and various panels on the right. In the Console pane, try typing a simple command at the prompt > to make sure R is working correctly. For example, type 2+2 and press enter. If R is working correctly, it should display 4.\n\n2 + 2\n\n[1] 4\n\n\nCongratulations! You have successfully installed R and RStudio, and you are now ready to begin exploring the world of statistics using these powerful tools. In our upcoming lessons, we will dive deeper into using R for data analysis, visualization, and statistical modeling."
  },
  {
    "objectID": "week00/0_2-resources.html#datacamp-for-classrooms",
    "href": "week00/0_2-resources.html#datacamp-for-classrooms",
    "title": "3  Links to resources",
    "section": "3.1 DataCamp for Classrooms",
    "text": "3.1 DataCamp for Classrooms\nIn this course, we will incorporate DataCamp for Classrooms as a resource to supplement your learning experience. DataCamp (http://www.datacamp.com) is an online platform that offers interactive, hands-on coding challenges and video tutorials to help you master data science skills at your own pace.\nTo provide with the best learning experience, I have carefully curated a selection of DataCamp courses that align with the course curriculum and reinforce key concepts. By using DataCamp for Classrooms, you will:\n\nLearn by doing: Practice your coding skills with real-world datasets and guided exercises designed to test your understanding of statistical concepts and R programming.\nReceive instant feedback: Get personalized feedback on your progress and identify areas for improvement with DataCamp’s smart assessment system.\nAccess expert content: Benefit from a wealth of knowledge provided by experienced instructors who are leaders in their respective fields.\nTrack your progress: Monitor your learning journey with DataCamp’s intuitive dashboard, which highlights your achievements and completed courses.\nCollaborate with peers: Engage with fellow students in a dynamic learning environment where you can share ideas, discuss course material, and learn from each other’s experiences.\n\nA sign-up link to DataCamp is posted in the courses CANVAS page. Once you have signed up, you will be able to access the DataCamp courses that I have assigned to you. You will be able to complete these courses at your own pace (save for the first 3 weeks), and you will have free access to DataCamp for the duration of the course."
  },
  {
    "objectID": "week00/0_2-resources.html#class-youtube-channel",
    "href": "week00/0_2-resources.html#class-youtube-channel",
    "title": "3  Links to resources",
    "section": "3.2 Class YouTube Channel",
    "text": "3.2 Class YouTube Channel\nIn addition to DataCamp, I have put together a YouTube channel that will serve as a supplementary resource for this course. The channel will feature a variety of content, including:\nBrief instructional videos: These concise videos will offer clear explanations and step-by-step guidance on various topics covered in the course, making it easier for you to grasp complex concepts and techniques.\nSPSS examples: To broaden your skillset, I will demonstrate how to perform similar analyses in SPSS when applicable. This will give you valuable insights into alternative statistical software and help you become more versatile in your data analysis capabilities (that and many of your PIs use SPSS).\n“Do Some Data with Me” sessions: These are in-depth, real-time data analysis sessions where you can observe my thought process and decision-making throughout the analysis. These interactive sessions will allow you to practice performing the analysis and writing the code alongside me, reinforcing your understanding of the material and building your confidence in tackling real-world data challenges."
  },
  {
    "objectID": "week01/1_1-Rmarkdown_basics.html",
    "href": "week01/1_1-Rmarkdown_basics.html",
    "title": "\n4  Rmarkdown Basics\n",
    "section": "",
    "text": "5 Hello there\nThis is an example of a quarto markdown notebook. These notebooks have some pretty nifty features that in theory allow you to perform both your analyses and your write-ups in the same environment.\nNew notebooks can be opened by: File > New File > Quarto Document.\nIf you are looking at this code, you may have wondered why in the heck did I place that previous line in double-stars. This is how you format text in markdown. For example:\nproduces bold text, and\nrenders italics text, and\nmakes code text.\nMore examples include:\nPerhaps the most important aspects of using notebooks like this is that you can marry text and code. Code is placed in chunks starting with ```{r}. For example, try executing this chunk by clicking the Run button within the chunk or by placing your cursor inside it and pressing Cmd+Shift+Enter (Mac) or Ctrl+Shift+Enter) (Win).\nYou can add a new chunk by clicking the Insert Code Chunk button on the toolbar.\nTo compile (print) the contents of this notebook you can click the Render button. Or, if you choose, you can export in into a pdf or doc using other Knit options. The first time you try this, RStudio may ask you to install a few packages. This only happens the very first time.\nYou may have notice that the resulting output also prints the code. This may not be what you want. You can elect to hide code on a by-chunk basis by adding #| echo: false to the top of your chunk. You notice that there are several option options here as well, including hiding warnings, and using a custom figure size.\nTry toying around with these options on the chunk below:\nOne last bit. By default, an Quarto Document executes all code inline, meaning the output shows up underneath the chuck. Depending on your preferences this may not be desirable. As an alternative you can elect to have the code sent to the Console. This can be accomplished the selecting the Preferences Gear button and selecting `Chunk output console. Do this and re-run each of the chunks separately."
  },
  {
    "objectID": "week01/1_1-Rmarkdown_basics.html#header-2",
    "href": "week01/1_1-Rmarkdown_basics.html#header-2",
    "title": "\n4  Rmarkdown Basics\n",
    "section": "\n6.1 Header 2",
    "text": "6.1 Header 2\n\n### Header 3\n\n\n6.1.1 Header 3\nAlso,\n\n- an unordered list\n-- can be don like this\n\n\nan unordered list\n\ncan be done like this\n\n\n\n\n1. here is an example of\n2. an ordered (numbered) list\n\n\nhere’s and example of\nan ordered (numbered) list\n\nand here is how you\n\n> blockquote some text\n\n\nblockquote some text\n\nare but a few that you may encounter. An expanded list can be found here. Which, by the way, is how you embed weblinks. Heck you can even do mathematical equations. For example, here is the binomial probability.\n\\[f(y|N,p) = \\frac{N!}{y!(N-y)!}\\cdot p^y \\cdot (1-p)^{N-y} = {{N}\\choose{y}} \\cdot p^y \\cdot (1-p)^{N-y}\\]\nand this is how it was created:\n$$f(y|N,p) = \\frac{N!}{y!(N-y)!}\\cdot p^y \\cdot (1-p)^{N-y} = {{N}\\choose{y}} \\cdot p^y \\cdot (1-p)^{N-y}$$\nGranted the syntax for this might seem daunting at first (and isn’t required for this course), but just wanted to show you some of the things you can do. At the very least learning equation syntax might be useful for superscripts\\(^2\\), subscripts\\(_2\\), and mathematical symbols like \\(\\alpha\\), \\(\\chi\\), and \\(\\sum\\). Although you can also do superscripts2 and subscripts2 inline. I suppose fractions are pretty useful too:\n\\[fraction = \\frac{numerator}{denominator}\\]"
  },
  {
    "objectID": "week01/1_2-objects_assignment_functions.html",
    "href": "week01/1_2-objects_assignment_functions.html",
    "title": "\n5  Objects, assignment, and functions\n",
    "section": "",
    "text": "Note: there is a video (maybe a little dated) that accompanies this walkthrough. It can be found at\nComputations, Operations, Data Structures @ youtube\n\n6 Assignment and Objects\nIn some of our earlier examples we ran a simple mathematical operation:\n\n2+2\n\n[1] 4\n\n\nWe can save any operation (mathematical or other) that we perform in R in an object. In R this accomplished using the <- operator. This can be accompanied by typing < and - or by a keyboard shortcut. Here’s a cheatsheet with a whole lot of info for RStudio. As you progress through the semester I recommend coming back to this cheatsheet. For now, look for the keyboard shortcuts for Insert <- and Run current line/selection.\nNow let’s save the above operation to an object named x. Run each of the lines separately\n\nx <- 2+2\nx\n\n[1] 4\n\nx^2\n\n[1] 16\n\nx*100\n\n[1] 400\n\n\nNote that objects can contain more than numbers. For example they can contain names (or “strings”)“:\n\nx <- \"Tehran\"\nx\nx^2 # this won't work... you can't square \"Tehran\"\nx*100\nclass(x)\n\nand logicals:\n\n# this is a logical and its true\nx <- 2+2==4\nclass(x)\n\n[1] \"logical\"\n\nx\n\n[1] TRUE\n\n# this is a logical and its false\ny <- 2+2==5\nclass(y)\n\n[1] \"logical\"\n\ny\n\n[1] FALSE\n\n\n\n7 Vectors and Matrices\nWhile objects can contain single values, more often they are more complex, containing multiple values:\n\nx <- c(2,4,6,8,10,12)\nx\n\n[1]  2  4  6  8 10 12\n\nx^2\n\n[1]   4  16  36  64 100 144\n\nx*100\n\n[1]  200  400  600  800 1000 1200\n\nx==4\n\n[1] FALSE  TRUE FALSE FALSE FALSE FALSE\n\nclass(x)\n\n[1] \"numeric\"\n\n\nWe call the above a vector, it has a 1-dimensional data structure (think of it as a line of data). Note that we can also hold multi-dimensional data in matrices and data frames. For example, let’s transform our vector x into a 3 (rows) x 2 (columns) matrix y:\n\ny <- matrix(data = x,nrow = 3)\ny\n\n     [,1] [,2]\n[1,]    2    8\n[2,]    4   10\n[3,]    6   12\n\n\nFor those of you that are familiar with matrix algebra, you can have fun thinking about what operations you can perform from here. However, for now we’ll just move on. However, before doing so, let’s take note of what’s been going on in the History and Environment tabs.\n\n8 Data Frames\nYou typically won’t create a data frame from scratch, but it’s probably the most important data structure you will use. Data that you import often takes this format. We’ll take more about data frames in the next workshop.\n\n9 a bit on functions\nThis week you are going to encounter a number of built in functions including c(), install.packages(), and View(). Functions are commands in R that perform some programmed operation given the input parameters. For example, taking our vector x we can find the mean using:\n\nmean(x)\n\n[1] 7\n\n\nwhere mean() is the function and x is the input parameter. Some functions take several parameters. For example:\n\ncars\n\n   speed dist\n1      4    2\n2      4   10\n3      7    4\n4      7   22\n5      8   16\n6      9   10\n7     10   18\n8     10   26\n9     10   34\n10    11   17\n11    11   28\n12    12   14\n13    12   20\n14    12   24\n15    12   28\n16    13   26\n17    13   34\n18    13   34\n19    13   46\n20    14   26\n21    14   36\n22    14   60\n23    14   80\n24    15   20\n25    15   26\n26    15   54\n27    16   32\n28    16   40\n29    17   32\n30    17   40\n31    17   50\n32    18   42\n33    18   56\n34    18   76\n35    18   84\n36    19   36\n37    19   46\n38    19   68\n39    20   32\n40    20   48\n41    20   52\n42    20   56\n43    20   64\n44    22   66\n45    23   54\n46    24   70\n47    24   92\n48    24   93\n49    24  120\n50    25   85\n\nplot(x=cars$speed, y=cars$dist, col=\"blue\")\n\n\n\n\nplot() is our function x parameter for values on x-axis y parameter for values on y-axis col color to plot the points\nOne of the most useful things about programming is that you can create your own functions. We’ll come back to more complex version of this later in the semester. But for now here is an example that takes whatever numerical value is in it and multiplies it by two:\n\nbyTwo <- function(x){\n  x*2\n}\n\nAnd now to run it.\n\nbyTwo(2)\n\n[1] 4\n\nbyTwo(4)\n\n[1] 8\n\nbyTwo(x)\n\n[1]  4  8 12 16 20 24\n\n\nFWIW, this is a totally useless function… but simple enough."
  },
  {
    "objectID": "week01/1_3-install_packages.html",
    "href": "week01/1_3-install_packages.html",
    "title": "\n6  Installing and loading an R package\n",
    "section": "",
    "text": "7 Loading packages\nOne of the great things about R is its extensibility via packages. For other great things about R (or how to sell R to your peers) see this link on simplystatistics.org. In fact this blog is a great resource for staying in contact with topics and new developments in data analysis.\nFrom Peng’s article:\nFor example, in my own work I often find myself not only using statistical techniques such as growth curve modeling (package: lmer) but also advanced quantification techniques such as cross recurrence quantificantion analysis (package: crqa), MultiFractal Detrended Fluctuation Analysis (package: MFDFA), and Sample Entropy (package: TSEntropies). When I was a grad student, performing any of these analyses required me to put together 100’s of lines of custom coding to… today, there’s an R package for that—i.e., someone else likely with far greater programming abilities than I has already done it."
  },
  {
    "objectID": "week01/1_3-install_packages.html#installing-a-package-using-the-gui",
    "href": "week01/1_3-install_packages.html#installing-a-package-using-the-gui",
    "title": "\n6  Installing and loading an R package\n",
    "section": "\n7.1 Installing a package using the GUI",
    "text": "7.1 Installing a package using the GUI\nFor the point-and-click crowd, the easiest way to install an R package is to locate the Packages tab in your Window. From here you can click install. From there you can type in the package that you wish to install. So for example, let’s install the psych package, which is useful for obtaining descriptive stats from your data (although to be honest I hardly use it). Check out an example from this video.\nAs you see the video, I left the Install dependences box ticked. You should always install with dependencies, this automatically installs other packages that may be required for psych (or whatever package of interest) to work. You may have also noticed when after you clicked OK, R entered this line of code into your Console.\n\ninstall.packages(\"psych\")\n\nWhich leads us to…"
  },
  {
    "objectID": "week01/1_3-install_packages.html#installing-packages-with-install.packages",
    "href": "week01/1_3-install_packages.html#installing-packages-with-install.packages",
    "title": "\n6  Installing and loading an R package\n",
    "section": "\n7.2 Installing packages with install.packages()\n",
    "text": "7.2 Installing packages with install.packages()\n\nA basic install of R comes with the base package which has an unbelievably large number of functions and analyses built in. However learning base R has a somewhat steep learning curve, especially for those with no programming experience. In this course we will use many of the simple functions in base R, but for more complex data wrangling (sorting, structing our data), statistical analysis, and plotting we will use the wonderful tidyverse package. tidyverse piggybacks on the basic install, replacing the sometimes bewildering R syntax with more natural language. Here, we’ll use tidyverse as an example of how to install from command line.\nInstalling tidyverse (or any package for that manner) couldn’t be easier. At the prompt (or in your notebook) simply type:\n\ninstall.packages(\"tidyverse\")\n\nCongratulations you have installed over 70 new packages to your R environment!\nYou see, tidyverse is not a single package (as is usually the case), but a collection of packages that function seamlessless with one another abiding a shared ethos of data science practices (who knew there were ethoses in stats! A simple google search of tidyverse might lead you to believe that you have joined a cult with Hadley Wickham as your leader!). The core value is that data should be structured, analyzed, presented, and shared in a manner that is as transparent as possible. In this class, we aren’t going to go the “full Hadley” (all the way down the rabbit hole) but we are going to abide many shared principles (it’s just good science!).\nNote for the crqa package I simply type install.packages(\"crqa\") and for MFDFA install.packages(\"MFDFA\"). You noticing a pattern here?"
  },
  {
    "objectID": "week01/1_3-install_packages.html#an-important-note-on-rendering-with-install.packages",
    "href": "week01/1_3-install_packages.html#an-important-note-on-rendering-with-install.packages",
    "title": "\n6  Installing and loading an R package\n",
    "section": "\n7.3 An IMPORTANT note on Rendering with install.packages:",
    "text": "7.3 An IMPORTANT note on Rendering with install.packages:\nOne quirk with knitting from your source is that you will need to specify which CRAN mirror you will download the package from. The Comprehensive R Network (CRAN) is an online repository that houses almost all things R, including packages. There are multiple mirrors set up all over the globe (https://cran.r-project.org/mirrors.html) and you need to tell R which one is your preferred choice. The quirk is that you only need to do this when knitting/compiling. You don’t need to do this for other everday use, but your source often will not compile unless you fix this. See here to read more about the error this produces.\nSince your homework is going to involve rendering you should probably get in the practice of fixing this. There are two ways to resolve this issue. The first is to simply add the repos argument to your install.packages() command like so:\ninstall.packages(package name, repos=\"http://cran.us.r-project.org\")\nExecuting this once (adding the repo) usually fixes things for the rest of your session. If you find you keep encountering this issue, there is a brute-force alternative that involves you providing a default repo at the outset. This can be accomplished by creating a new chunk at the beginning of your Quarto file (right underneath your header), and inserting the following:\n\nsetRepositories(graphics = getOption(\"menu.graphics\"),  \nind = NULL, addURLs = character())\nr <- getOption(\"repos\")\nr[\"CRAN\"] <- \"http://cran.cnr.berkeley.edu/\"\noptions(repos = r)"
  },
  {
    "objectID": "week01/1_3-install_packages.html#loading-packages-with-library",
    "href": "week01/1_3-install_packages.html#loading-packages-with-library",
    "title": "\n6  Installing and loading an R package\n",
    "section": "\n7.4 Loading packages with library()\n",
    "text": "7.4 Loading packages with library()\n\nOnce you have installed a package, it remains stored on your hard-drive until you delete it (which you’ll amost never do, packages take up so little disk space it’s really not efficient to install and uninstall unless absolutely necessary). What many new users have difficulty getting used to is that just because a package is installed does not mean that it is loaded. To load a package we typically use the library() command. For example to load tidyverse, we can type:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n\nThat’s it, easy-peasy. Note that by default, only the base packages are typically loaded when you start a new session. Therefore, a useful practice to get into is to load the necessary packages at the start of every session.\nBefore moving on, I’d like to point out two things. First, notice that when installing with install.packages() we included quotations around the package name, but with library() we did not. Knowing when to use quotes and when not to is one of the more frustrating things to grasp for beginners (and even is an annoyance for me from time to time). All I can say is with practice, it becomes automatic. In truth, in the case of library() whether you include the quotes or not doesn’t matter (the default is to not), but for many functions, including install.packages(), the proper quotes do matter (for example: install.packages(tidyverse) will result in an error).\nSecond, while installing tidyverse installs over 70 packages on your computer, library(tidyverse) only loads 8 or so primary packages. This isn’t really much of an issue as we will be mostly using this common tidyverse packages, but it’s worth mentioning for future reference. Finally, when you loaded tidyverse it likely gave you a message about Conflicts. Conflict arise when different packages use the same names for functions. For example, the message: `dplyr::filter() masks stats::filter() is telling you that both stats and dplyr have a function called filter() and that dplyr is the default. This means that if you execute filter() by itself it will default to the dplyr version. If you ever find yourself in a situation were a function you know should be working isn’t, first check to be sure R is indeed calling from the correct package. That said this can be avoided by getting in the habit of using the library::function() convention. I will typically use this format with few exceptions (out of habit): ggplot and pre-installed functions."
  },
  {
    "objectID": "week01/1_4-loading_data.html",
    "href": "week01/1_4-loading_data.html",
    "title": "\n7  Loading in Data\n",
    "section": "",
    "text": "8 Loading in data\nTypically the file types that are used by beginners in R are plain text and delimited. They may have the extension “txt”, “csv”, or “dat” for example. These may become more sophisticated you progress, for example you can load in proprietary types like SPSS and STATA, but for this course we will mostly use plain text files (although later in the course I will show you how to load in Excel files and SPSS files)."
  },
  {
    "objectID": "week01/1_4-loading_data.html#loading-in-local-data-using-a-gui",
    "href": "week01/1_4-loading_data.html#loading-in-local-data-using-a-gui",
    "title": "\n7  Loading in Data\n",
    "section": "\n8.1 Loading in local data using a GUI:",
    "text": "8.1 Loading in local data using a GUI:\nAs with loading packages, you can also load in a file containing data using the RStudio GUI. See this video.\nAgain, this is only preferred if you are not sharing your analysis. If you are sharing your analysis (as in this class) you need to do the command line. Fortunately, RStudio creates the appropriate command-line for you to copy and paste. For example, loading one of your homework data sets can be accomplished by:\n\nlibrary(tidyverse)\nLexicalDescisionData <- read_table(\"path/to/LexicalDescisionData.txt\")\n\nwhere path/to/ is the path to the folder containing the file LexicalDescisionData.txt. For example, on my computer it might be:\n\n“/Users/tehrandav/Documents/psyc7014/LexicalDescisionData.txt”\n\nI’ve uploaded this file to OneDrive in the walkthroughs and examples folder. Please feel free to download the file and follow along is you wish. Note that if you do elect to load in via the GUI you need to be sure to copy the output to your Rmd source. Otherwise, I will not be able to run it on my computer."
  },
  {
    "objectID": "week01/1_4-loading_data.html#importing-data-from-the-web",
    "href": "week01/1_4-loading_data.html#importing-data-from-the-web",
    "title": "\n7  Loading in Data\n",
    "section": "\n8.2 Importing data from the web",
    "text": "8.2 Importing data from the web\nYou might be saying to yourself, “but Tehran the entire reason you’ve got us learning R is for transparency and openness with our data. How would I be able to share in my code a data file that resides on my hard drive?!?””\nCorrect, you can’t, but you can upload it to the internet and someone can access it from an online repository. Personally, I like to use http://www.github.com, but we’ll save that for some advanced stuff later in the semester for those so inclined. Another alternative is to upload your entire folder to a project in http://www.rstudio.cloud.\nIf the data is uploaded directly from the web, we can load it in using the read_delim() function from above. This is some reaction time data taken from a website. Let’s assign it to an object RxData\n\nlibrary(tidyverse) # no need to ask if already loaded\nalcohol_use_data <- read_csv(\"https://raw.githubusercontent.com/tehrandavis/graduate_statistics/main/practice_datasets/alcuseN6.csv\", col_names = T)\n\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nRows: 18 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): id, Age, Alc.use\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nTo see what’s going on with the additional calls in this line, run the following line to get help:\n\n? read_csv\n\nA document file should show up in your help tab, containing examples and describing what different arguments are doing. Search for col_names and try to figure out what’s going on."
  },
  {
    "objectID": "week01/1_4-loading_data.html#looking-ahead",
    "href": "week01/1_4-loading_data.html#looking-ahead",
    "title": "\n7  Loading in Data\n",
    "section": "\n8.3 Looking ahead…",
    "text": "8.3 Looking ahead…\nThis is probably a good place to stop for now. In the meantime try running the following 4 commands (assuming that you have imported alcohol_use_data) and think about what they are returning:\n\nclass(alcohol_use_data)\nnames(alcohol_use_data)\nhead(alcohol_use_data)\nsummary(alcohol_use_data)"
  },
  {
    "objectID": "week01/1_5-leveling-up.html#logical-statements",
    "href": "week01/1_5-leveling-up.html#logical-statements",
    "title": "\n8  Leveling-up\n",
    "section": "\n8.1 Logical statements",
    "text": "8.1 Logical statements\n\n\n\n\nWe were introduced to logicals very briefly in this week’s DataCamp assignments. Logicals are operations that may be applied to objects in R that assess their relationship as a truth value. For example:\n\n2+2==5\n\n[1] FALSE\n\n\nThe == (double equal sign) may be understood as posing the question “is equal to?” For example typing 2+2==4 in R will return TRUE. Logicals can be applied to string objects:\n\n\"tehran\"==\"tehran\"\n\n[1] TRUE\n\n\"tehran\"==\"davis\"\n\n[1] FALSE\n\n\nto vectors:\n\ny <- 1:5\ny==3\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\n\nthe third value, where the number 3 resides in vector y is labelled TRUE.\nand can be used to compare assigned objects.\n\nx <- 3\ny <- 4\nx==y\n\n[1] FALSE\n\na <- 1:5\nb <- seq(2,10,2)\na==b/2\n\n[1] TRUE TRUE TRUE TRUE TRUE\n\n\nA list of logical operators in R can be found here, including greater-than and less-than. Using logical operators can be a powerful tool in a programmer’s repertoire. For example they can allow you to quickly highlight and select certain instances from large data sets. Below I’m highlighting numbers divisible by 5 in a list running from 1 to 50:\n\nmyList <- 1:50\nmyList[myList %% 5 == 0] # index those numbers divisible by 5\n\n [1]  5 10 15 20 25 30 35 40 45 50\n\n\nHere I’m telling R to print a word based on whether myNumber is greater than 5:\n\nmyNumber <- 4\n\nifelse(test = myNumber>5, yes = \"Yep, great than 5\", no = \"Nunca, nada, NOPE!\")\n\n[1] \"Nunca, nada, NOPE!\"\n\n\nTry re-running the code above in your own console, with a value greater than 5 assigned to myNumber. There’s a little more on ifelse() below."
  },
  {
    "objectID": "week01/1_5-leveling-up.html#loading-packages-with-require",
    "href": "week01/1_5-leveling-up.html#loading-packages-with-require",
    "title": "\n8  Leveling-up\n",
    "section": "\n8.2 Loading packages with require()\n",
    "text": "8.2 Loading packages with require()\n\nGiven our brief intro to logicals; there is in fact an alternative to library() for loading a previously installed package. We can use require():\n\nrequire(tidyverse)\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n\nrequire() has the additional benfit of returning a FALSE logical output if the requested package hasn’t been installed on your computer. This is in contrast to library() which just freaks out and reports an error (errors are bad as they can stop your execution). For example, let’s save the output of the following actions to an object called myVar.\nFor example running:\n\nmyVar <- library(xkcd)\nmyVar\n\nversus\n\nmyVar <- require(xkcd)\nmyVar\n\nIn both cases xkcd was not installed on your comp and an error was returned. However, with library(), nothing was saved to myVar, but require() returned a FALSE indicating that the requested package was not installed. Why do I bother to even bring this up?"
  },
  {
    "objectID": "week01/1_5-leveling-up.html#installing-and-loading-packages-like-a-pirate-ninja",
    "href": "week01/1_5-leveling-up.html#installing-and-loading-packages-like-a-pirate-ninja",
    "title": "\n8  Leveling-up\n",
    "section": "\n8.3 Installing and loading packages like a Pirate Ninja!",
    "text": "8.3 Installing and loading packages like a Pirate Ninja!\nThe central reason that we are using R is to get in the practice of data transparency and replicability. Ultimately, for every analysis that you perform you should be able to provide me with the appropriate syntax in your notebook file and I should be able to re-run each of your analyses step by step on my own computer. One important consideration is that you may be using packages that I don’t have installed and loaded on my computer and vice versa. To deal with this you would need to include two lines for every package:\n\ninstall.packages(\"package\")\nlibrary(package)\n\nIf you use a lot of packages, this can become very repetitive (imagine using 10 packages).\nA much more efficient way of doing things is to take advantage of that FALSE that require() returns. But first we need to install a package called pacman. Before we do this, input the next to lines seperately:\n\nrequire(pacman)\n\nLoading required package: pacman\n\n!require(pacman)\n\n[1] FALSE\n\n\nRecall from above that require() produces a FALSE if the requested package is NOT installed on your computer. So, assuming that pacman is not installed on your computer the first line will produce a FALSE and the second will produce a TRUE.\nSo what is that second line then? NEGATION.\nPlacing an exclamation point in fron of a statement essentially reads as “not true”. For now just understand that using this syntax we can create a line of code that checks to see if pacman is installed on your computer, and if not it installs it:\n\nif (!require(\"pacman\")) \n    install.packages(\"pacman\")\n\nThe above code is a conditional if-statement. It literally reads:\n\nCheck to see if the statement !require(\"pacman\") is TRUE (i.e., TRUE = pacman is not installed on your computer).\nIf the above indeed returns a TRUE, then run install.packages(\"pacman\").\n\nIn theory you could run this line for every package, but that would be tedious as well and is not really a simpler solution. Fortunately pacman contains a function that simplifies this for us, pacman::p_load() function. To get a feel for what this function does run the following line:\n\n? pacman::p_load()\n\nThe ? brings up an online help module for the named function. In this case the function is p_load() from the pacman package.\nYou’ll see that pacman::p_load() checks to see if a package is installed, if not it attempts to install the package from CRAN and/or any other repository. After that it loads all listed packages.\nLet’s try a few packages that you haven’t installed but are going to be useful to us later when we do ANOVA. FWIW I usually have a code chunk at the top of every notebook that uses this template, swapping in the various packages that I intend to use."
  },
  {
    "objectID": "week01/1_5-leveling-up.html#heres-the-cheat-code",
    "href": "week01/1_5-leveling-up.html#heres-the-cheat-code",
    "title": "\n8  Leveling-up\n",
    "section": "\n8.4 HERE’S THE CHEAT CODE!!!",
    "text": "8.4 HERE’S THE CHEAT CODE!!!\n\n# 1. check to see if pacman is on your computer and if not, let's \n# install and load it:\nif (!require(\"pacman\")) install.packages(\"pacman\")\nlibrary(pacman)\n\n# 2. install all other packages that we will be using:\npacman::p_load(afex,lmerTest,plyr,car)\n\nThis bit of code installed and loaded the afex, lmerTest, and plyr packages. To test that everything worked, try:\n\ndata(obk.long, package = \"afex\")\nafex::aov_ez(\"id\", \"value\", obk.long, between = c(\"treatment\", \"gender\"),\n             within = c(\"phase\", \"hour\"), observed = \"gender\")\n\nContrasts set to contr.sum for the following variables: treatment, gender\n\n\nAnova Table (Type 3 tests)\n\nResponse: value\n                        Effect          df   MSE         F  ges p.value\n1                    treatment       2, 10 22.81    3.94 + .198    .055\n2                       gender       1, 10 22.81    3.66 + .115    .085\n3             treatment:gender       2, 10 22.81      2.86 .179    .104\n4                        phase 1.60, 15.99  5.02 16.13 *** .151   <.001\n5              treatment:phase 3.20, 15.99  5.02    4.85 * .097    .013\n6                 gender:phase 1.60, 15.99  5.02      0.28 .003    .709\n7       treatment:gender:phase 3.20, 15.99  5.02      0.64 .014    .612\n8                         hour 1.84, 18.41  3.39 16.69 *** .125   <.001\n9               treatment:hour 3.68, 18.41  3.39      0.09 .002    .979\n10                 gender:hour 1.84, 18.41  3.39      0.45 .004    .628\n11       treatment:gender:hour 3.68, 18.41  3.39      0.62 .011    .641\n12                  phase:hour 3.60, 35.96  2.67      1.18 .015    .335\n13        treatment:phase:hour 7.19, 35.96  2.67      0.35 .009    .930\n14           gender:phase:hour 3.60, 35.96  2.67      0.93 .012    .449\n15 treatment:gender:phase:hour 7.19, 35.96  2.67      0.74 .019    .646\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\nSphericity correction method: GG \n\n\nCongrats! You’ve just run a 2x2 mixed effects ANOVA! We’ll revisit what exactly is going on here in about 10 weeks."
  },
  {
    "objectID": "week01/1_5-leveling-up.html#downloading-and-saving-data-from-the-web",
    "href": "week01/1_5-leveling-up.html#downloading-and-saving-data-from-the-web",
    "title": "\n8  Leveling-up\n",
    "section": "\n8.5 Downloading and saving data from the web",
    "text": "8.5 Downloading and saving data from the web\nThe example above just pulls data directly from a url, what if you want to download the data file directly onto your computer and load it from there?\nWell, there’s a package for that… downloader. Let’s install this package and download the data from above. Also, check out the additional notation in the code below:\n\n# within the R code sections, hashtags create comments, sections of code \n# that are not interpreted by the computer, but may serve to inform others \n# (and typically yourself later in life) about what exactly in the hell is\n# going on here. GET IN THE PRACTICE OF COMMENTING YOUR CODE. You'll thank\n# yourself later. I can't count how many times I've written code only \n# to come back to it months later wondering what in the hell I was \n# doing, thinking!!\n\n# Here I'm using comments to inform you step-by-step \n# what each line is doing:\n\n# install and load \"downloader\" package, this assumes you \n# have \"pacman\" installed and loaded *see section above:\npacman::p_load(downloader)\n\n# get the url of the file you want to download and assign it \n# to an object (\"dataURL\"):\ndataURL <- \"https://www.uvm.edu/~statdhtx/methods8/DataFiles/Tab2-1.dat\"\n\n# decide on what name you want to give the file. In this case I'm extracting\n# using the basename from the web url: Tab2-1.dat. In truth you \n# can name it whatever you want (see commented example)\n\nfilename <- basename(dataURL)\n#filename <- \"you_can_name_it_what_you_like.txt\"\n\n# download the file to your current R-project folder:\ndownload(url = dataURL, filename)\n\nKeep in mind that objects are just placeholders. So if I was so inclined I could have accomplished all of the above with just one line:\n\ndownload(url = \"https://www.uvm.edu/~statdhtx/methods8/DataFiles/Tab2-1.dat\", destfile=\"Tab2-1.dat\")\n\n# destfile is a parameter for naming what you download.\n\nFrom here I could just import the Tab2-1.dat file from my computer using the GUI method above."
  },
  {
    "objectID": "week02/2_1-structure_data.html#what-is-the-structure-of-your-data-file",
    "href": "week02/2_1-structure_data.html#what-is-the-structure-of-your-data-file",
    "title": "\n9  The structure of your data\n",
    "section": "\n9.1 What is the structure of your data file?",
    "text": "9.1 What is the structure of your data file?\nLast week we covered how to import data from file into an R data frame. Before taking any next steps with your analysis, you first should take a moment to understand the structure of your data. Every statical software wants the data to have a particular format, R included. One thing that you will need to be sensitive of is that not every bit of software (or even atype of analysis) wants the data in the same format. In order to perform the appropriate analyses, you need to have the data in the right formatting structure. One crucial issue that confronts many first time users coming from SPSS is whether the data is in WIDE or LONG format.\nLet’s load some example data and assign it to the object reading_data:\n\n# loading in the packages, see \"Week 1: Leveling up\npacman::p_load(tidyverse) \nreading_data <- read_table(\"https://raw.githubusercontent.com/tehrandavis/graduate_statistics/main/practice_datasets/reading.txt\", col_names = T)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  id = col_double(),\n  group = col_character(),\n  pretest1 = col_double(),\n  pretest2 = col_double(),\n  posttest1 = col_double(),\n  posttest2 = col_double(),\n  posttest3 = col_double()\n)\n\n\nAnd now look at the data:\n\nreading_data\n\n# A tibble: 66 × 7\n      id group   pretest1 pretest2 posttest1 posttest2 posttest3\n   <dbl> <chr>      <dbl>    <dbl>     <dbl>     <dbl>     <dbl>\n 1     1 Control        4        3         5         4        41\n 2     2 Control        6        5         9         5        41\n 3     3 Control        9        4         5         3        43\n 4     4 Control       12        6         8         5        46\n 5     5 Control       16        5        10         9        46\n 6     6 Control       15       13         9         8        45\n 7     7 Control       14        8        12         5        45\n 8     8 Control       12        7         5         5        32\n 9     9 Control       12        3         8         7        33\n10    10 Control        8        8         7         7        39\n# ℹ 56 more rows\n\n\nPausing a moment, typing out reading_data alone give us a print out of the entire data frame. This is a good way to get a sense of what the data look like. However, if you have a large data frame, this can be a bit overwhelming. Instead, we can use the head() function to look at the first 6 rows of the data frame:\n\nhead(reading_data)\n\n# A tibble: 6 × 7\n     id group   pretest1 pretest2 posttest1 posttest2 posttest3\n  <dbl> <chr>      <dbl>    <dbl>     <dbl>     <dbl>     <dbl>\n1     1 Control        4        3         5         4        41\n2     2 Control        6        5         9         5        41\n3     3 Control        9        4         5         3        43\n4     4 Control       12        6         8         5        46\n5     5 Control       16        5        10         9        46\n6     6 Control       15       13         9         8        45\n\n\nAlternatively, if we want to get a listing of all of the column names, and their data types, we can use the str() function, or if using tidyverse, the glimpse() function. Since we’ll almost always be loading in the tidyverse, we’ll use glimpse():\n\nglimpse(reading_data) #tidyverse\n\nRows: 66\nColumns: 7\n$ id        <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ group     <chr> \"Control\", \"Control\", \"Control\", \"Control\", \"Control\", \"Cont…\n$ pretest1  <dbl> 4, 6, 9, 12, 16, 15, 14, 12, 12, 8, 13, 9, 12, 12, 12, 10, 8…\n$ pretest2  <dbl> 3, 5, 4, 6, 5, 13, 8, 7, 3, 8, 7, 2, 5, 2, 2, 10, 5, 5, 3, 4…\n$ posttest1 <dbl> 5, 9, 5, 8, 10, 9, 12, 5, 8, 7, 12, 4, 4, 8, 6, 9, 3, 5, 4, …\n$ posttest2 <dbl> 4, 5, 3, 5, 9, 8, 5, 5, 7, 7, 4, 4, 6, 8, 4, 10, 3, 5, 5, 3,…\n$ posttest3 <dbl> 41, 41, 43, 46, 46, 45, 45, 32, 33, 39, 42, 45, 39, 44, 36, …\n\n\nOkay, with that out of the way, on to the data! These are data taken from a study by Baumann, Seifert-Kessell, and Jones (1992). Sixty-six students in Grade 4 were randomly assigned to receive one of three interventions designed to improve reading comprehension. The participants were evenly distributed (n = 22 per intervention group) among an ‘instructional control’ group, a ‘Directed Reading-Thinking Activity’ (DRTA) group, and a ‘Think Aloud’ (TA) group. After each group received its respective intervention, the participants completed an error-detection test designed to evaluate their reading comprehension. The 2 pretest scores represent the number of errors prior to intervention, while the 3 posttest scores were taking post-intervention.\nFor the purposes of this example, let’s trim the 7 columns in the original set to four: id, group, pretest1, and posttest1. To do this we use the select function from the tidyverse. The select function allows us to select columns from a dataframe. The first argument is the dataframe, followed by the names of the columns we want to select., in this case id, group, pretest1, and posttest1.\n\n# pull out my columns of interest\nreading_data_select <- select(reading_data, \"id\", \"group\", \"pretest1\", \"posttest1\")\n\nOkay, now let’s take a moment to reflect on the structure of the data. First, this is a repeated measurement. Both columns pretest1 and posttest1 contain the same measure, test score, and every participant went through both pre and post tests. Most important for present purposes this data is in WIDE format—each line represents a participant, and columns represent scores taken from participants at different trials or times. The same measure, test scores, is spread out amount two columns depending on the time the test was measured, pre and post.\nHowever, while this might be intuitve for tabluar visualization, many statistical softwares prefer when LONG format, where each line represents a single observation, e.g. a participant-trial or participant-time (FWIW SPSS likes some mix of WIDE and LONG).\n** [See this page for a run down of WIDE v. LONG format] (https://www.theanalysisfactor.com/wide-and-long-data/)\n** More, for those that are interested I suggest taking a look at these two wonderful courses on DataCamp:\n\nWorking with Data in the Tidyverse\nCleaning Data in R"
  },
  {
    "objectID": "week02/2_1-structure_data.html#getting-data-from-wide-to-long",
    "href": "week02/2_1-structure_data.html#getting-data-from-wide-to-long",
    "title": "\n9  The structure of your data\n",
    "section": "\n9.2 Getting data from WIDE to LONG",
    "text": "9.2 Getting data from WIDE to LONG\nSo the data are in WIDE format, each line has multiple observations of data that are being compared. Here both pretest1 scores and posttest1 scores are on the same line. In order to make life easier for analysis and plotting in ggplot, we need to get the data into LONG format (pretest1 scores and posttest1 scores are on different lines). This can be done using the pivot_longer function from the tidyr package.\nBefore pivoting your data, one thing to consider is whether or not you have a column that defines each subject. In this case we have id. This tells R that these data are coming from the same subject and will allow R to connect these data when performing analysis. This watching will be crucially important later on when we perform t-tests and ANOVAs.\nUsing pivot_longer(): This function takes a number of arguments, but for us right now, the most important are data: your dataframe; cols: which columns to gather; names_to: what do you want the header of the collaped nonminal variables to be? Here, we might ask what title would subsume both pretest1 and posttest1. I’ll choose time ; values_to: what do the values represent, here I choose reading_score:\n\npivot_longer(reading_data_select,cols = c(\"pretest1\",\"posttest1\"),names_to = \"time\", values_to = \"reading_score\")\n\n# A tibble: 132 × 4\n      id group   time      reading_score\n   <dbl> <chr>   <chr>             <dbl>\n 1     1 Control pretest1              4\n 2     1 Control posttest1             5\n 3     2 Control pretest1              6\n 4     2 Control posttest1             9\n 5     3 Control pretest1              9\n 6     3 Control posttest1             5\n 7     4 Control pretest1             12\n 8     4 Control posttest1             8\n 9     5 Control pretest1             16\n10     5 Control posttest1            10\n# ℹ 122 more rows\n\n\nIn terms of how we transformed our data, this webpage has a good representation of going from wide to long and back again.\nA good visual representation of what pivot_longer() can be found here: https://www.garrickadenbuie.com/project/tidyexplain/#pivot-wider-and-longer. The site also has some nice visualizations for other data wrangling functions, like joining data frames and filtering.\nFor most of your analyses you are going to need to put your data in long format. Most of the datasets that I provide for homework will already be in this format, but some data that you pull from the web, or your own lab data may not. If that’s the case, right after you import your data you will need to put it in the correct format (of if you prefer, do this in Excel)."
  },
  {
    "objectID": "week02/2_2-central_tendency.html#getting-measures-of-central-tendency",
    "href": "week02/2_2-central_tendency.html#getting-measures-of-central-tendency",
    "title": "\n10  Measures of central tendency\n",
    "section": "\n10.1 Getting measures of central tendency",
    "text": "10.1 Getting measures of central tendency\nWe can use simple functions to get the mean, median, and standard deviation of an individual column of data. Remember for your reading that these measures make the most sense when the measure in question is numeric (continuous / ratio scale). For example here let’s take a look at BPAQ (https://www.psytoolkit.org/survey-library/aggression-buss-perry.html):\n\nmean(aggression_data$BPAQ) #mean\n\n[1] 2.612539\n\nmedian(aggression_data$BPAQ) #median\n\n[1] 2.62069\n\nsd(aggression_data$BPAQ) #std. dev.\n\n[1] 0.5240082\n\n\nMost of the data that we deal with comes in the form of data frames like aggression_data. It might be cumbersome to get the mean, median and sd for each column separately. Instead, we can use psych::describe() to generate summary stats of all data in a data frame:\n\npsych::describe(aggression_data)\n\n        vars   n  mean    sd median trimmed   mad   min   max range  skew\nage        1 275 20.21  4.96  18.00   19.01  1.48 17.00 50.00 33.00  3.70\nBPAQ       2 275  2.61  0.52   2.62    2.61  0.56  1.34  4.03  2.69  0.01\nAISS       3 275  2.56  0.37   2.55    2.56  0.37  1.45  3.70  2.25  0.01\nalcohol    4 270 16.00 15.87  12.00   13.69 14.83  0.00 96.00 96.00  1.50\nBIS        5 275  2.28  0.35   2.27    2.27  0.40  1.42  3.15  1.73  0.36\nNEOc       6 275  3.55  0.59   3.58    3.55  0.62  1.83  4.92  3.08 -0.16\ngender     7 275  0.79  0.41   1.00    0.86  0.00  0.00  1.00  1.00 -1.44\nNEOo       8 275  3.36  0.52   3.42    3.37  0.49  1.67  4.67  3.00 -0.25\n        kurtosis   se\nage        15.43 0.30\nBPAQ       -0.41 0.03\nAISS        0.23 0.02\nalcohol     3.09 0.97\nBIS        -0.22 0.02\nNEOc       -0.11 0.04\ngender      0.06 0.02\nNEOo       -0.09 0.03\n\n\nThis provides info related to the:\n\nitem name\nitem number\nnumber of valid cases (identifies if data is missing)\nmean\nstandard deviation\ntrimmed mean (with trim defaulting to .1)\nmedian (standard or interpolated)\nmad: median absolute deviation (see Leys et al, 2013)\nminimum value\nmaximum value\nskew\nkurtosis\nstandard error"
  },
  {
    "objectID": "week02/2_2-central_tendency.html#turning-numeric-data-into-categorical",
    "href": "week02/2_2-central_tendency.html#turning-numeric-data-into-categorical",
    "title": "\n10  Measures of central tendency\n",
    "section": "\n10.2 Turning numeric data into categorical",
    "text": "10.2 Turning numeric data into categorical\nLooking at out output from above, one thing to keep in mind is that some of measures printed by psych::describe() may not make sense for specific columns of data. For example, the output table above has a mean value for gender (0.79). This is because men in this data set are designated the value 0 and women are designated 1. By default, when R encounters numbers a column it treats them as numeric. However, in this case, the values in gender are better understood as nominal (ignoring discussions about gender fluidity that really wouldn’t fit into this simplistic classification to begin with). How might we deal with this?\nWe need to tell R that these data are categories, or factors and not continuous numbers. To do so we can call on the factor() function as below.\n\n# convert a numeric to a categorical (i.e., make a factor)\nfactor(aggression_data$gender)\n\n  [1] 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0\n [38] 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1\n [75] 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0\n[112] 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1\n[149] 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n[186] 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n[223] 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1\n[260] 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0\nLevels: 0 1\n\n\nWhen you run the line above you’ll note at the end of the output it mentions Levels: 0 1. This is telling you that in this vector, there are two levels, gender = 0 and gender = 1. To connect this factorized vector to your original data set, you can either create a new column in the data set or overwrite the original gender column that you are replacing. For beginners, I would recommend adding a new column. To do this, let’s take advantage of some of those fancy tidyverse / dplyr skills you picked up from DataCamp / the Winter readings, in particular mutate(), see Winter p. 32:\n\n# 1. create a new vector attached to the original data frame\n#    using the dplyr::mutate function:\n\naggression_data <- mutate(aggression_data, fctr_Gender = factor(gender))\naggression_data\n\n# A tibble: 275 × 9\n     age  BPAQ  AISS alcohol   BIS  NEOc gender  NEOo fctr_Gender\n   <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl>  <dbl> <dbl> <fct>      \n 1    18  2.62  2.65      28  2.15  2.83      1  2.92 1          \n 2    18  2.24  2.85      NA  3.08  2.5       1  4.17 1          \n 3    20  2.72  3.05      80  3     2.75      1  3.92 1          \n 4    17  1.93  2.65      28  1.85  3.42      1  4.17 1          \n 5    17  2.72  2.95      10  2.08  3.58      0  3.5  0          \n 6    17  2.45  1.95      12  2.62  3.83      1  3.25 1          \n 7    17  1.90  2.55      21  2.19  3.67      1  4.25 1          \n 8    17  2.59  2.3        3  2.19  3.42      1  2.58 1          \n 9    17  2.48  2         21  2.35  3.08      1  3.33 1          \n10    17  1.97  2.15       0  2.15  3.42      1  3.08 1          \n# ℹ 265 more rows\n\n\nYour new column is appended to the end of the aggression_data data frame. now when we run:\n\npsych::describe(aggression_data) \n\n             vars   n  mean    sd median trimmed   mad   min   max range  skew\nage             1 275 20.21  4.96  18.00   19.01  1.48 17.00 50.00 33.00  3.70\nBPAQ            2 275  2.61  0.52   2.62    2.61  0.56  1.34  4.03  2.69  0.01\nAISS            3 275  2.56  0.37   2.55    2.56  0.37  1.45  3.70  2.25  0.01\nalcohol         4 270 16.00 15.87  12.00   13.69 14.83  0.00 96.00 96.00  1.50\nBIS             5 275  2.28  0.35   2.27    2.27  0.40  1.42  3.15  1.73  0.36\nNEOc            6 275  3.55  0.59   3.58    3.55  0.62  1.83  4.92  3.08 -0.16\ngender          7 275  0.79  0.41   1.00    0.86  0.00  0.00  1.00  1.00 -1.44\nNEOo            8 275  3.36  0.52   3.42    3.37  0.49  1.67  4.67  3.00 -0.25\nfctr_Gender*    9 275  1.79  0.41   2.00    1.86  0.00  1.00  2.00  1.00 -1.44\n             kurtosis   se\nage             15.43 0.30\nBPAQ            -0.41 0.03\nAISS             0.23 0.02\nalcohol          3.09 0.97\nBIS             -0.22 0.02\nNEOc            -0.11 0.04\ngender           0.06 0.02\nNEOo            -0.09 0.03\nfctr_Gender*     0.06 0.02\n\n\nWe see our new variable fctr_Gender. The * next to it indicates that it is a categorical variable. R is now warning us to proceed with caution when interpreting the descriptive stats related to fctr_Gender. But you’ll notice that it still gives us these measures. The lesson here is to be careful when interpreting this data.\nUltimately, it makes the most sense to change the numeric levels in gender to man and woman respectively. This can be accomplished using the dplyr::recode_factor(). The structure of this function is:\n\ndplyr::recode_factor(data_frame$column_name, \"old_level_1\" = \"new_level_1\", ...)\n\nSo given that we’ve already created fctr_Gender we can:\n\n# overwriting the fctr_Gender column that we created in the last chunk, then updating\n\naggression_data$fctr_Gender <- dplyr::recode_factor(aggression_data$fctr_Gender, \"0\" = \"Man\", \"1\" = \"Woman\")\n\nNow if we compare our original gender column:\n\naggression_data$gender\n\n  [1] 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0\n [38] 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1\n [75] 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0\n[112] 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1\n[149] 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n[186] 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n[223] 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1\n[260] 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0\n\nmean(aggression_data$gender)\n\n[1] 0.7927273\n\n\nto our re-coded fctr_Gender column:\n\naggression_data$fctr_Gender\n\n  [1] Woman Woman Woman Woman Man   Woman Woman Woman Woman Woman Woman Woman\n [13] Man   Woman Man   Woman Woman Woman Man   Man   Woman Man   Woman Woman\n [25] Man   Woman Woman Man   Woman Woman Woman Woman Woman Man   Man   Man  \n [37] Man   Woman Woman Woman Woman Woman Woman Woman Woman Man   Woman Woman\n [49] Man   Woman Woman Woman Woman Woman Woman Woman Woman Woman Woman Woman\n [61] Man   Woman Woman Woman Woman Woman Man   Woman Woman Woman Woman Woman\n [73] Woman Woman Man   Man   Woman Woman Man   Woman Man   Woman Woman Woman\n [85] Woman Woman Woman Woman Woman Woman Woman Woman Woman Woman Man   Man  \n [97] Woman Woman Woman Man   Woman Woman Woman Woman Woman Man   Man   Woman\n[109] Woman Woman Man   Man   Woman Woman Woman Woman Woman Woman Woman Woman\n[121] Woman Woman Man   Woman Woman Woman Woman Man   Woman Woman Woman Man  \n[133] Woman Woman Woman Man   Woman Woman Woman Man   Man   Man   Man   Woman\n[145] Woman Man   Man   Woman Woman Woman Man   Woman Woman Woman Man   Woman\n[157] Woman Woman Woman Woman Woman Woman Woman Woman Woman Woman Man   Woman\n[169] Woman Woman Woman Woman Woman Woman Woman Woman Woman Woman Woman Woman\n[181] Woman Woman Woman Woman Man   Woman Man   Woman Woman Woman Woman Man  \n[193] Woman Woman Woman Woman Woman Woman Woman Woman Woman Man   Woman Woman\n[205] Woman Woman Woman Woman Woman Woman Woman Woman Woman Woman Woman Woman\n[217] Woman Man   Woman Woman Woman Woman Woman Man   Woman Woman Woman Woman\n[229] Woman Woman Woman Woman Woman Woman Woman Woman Woman Woman Man   Woman\n[241] Woman Woman Woman Woman Man   Man   Woman Woman Woman Man   Man   Woman\n[253] Woman Woman Woman Woman Man   Woman Woman Woman Woman Man   Woman Woman\n[265] Woman Woman Man   Man   Woman Woman Man   Woman Woman Woman Man  \nLevels: Man Woman\n\nmean(aggression_data$fctr_Gender)\n\nWarning in mean.default(aggression_data$fctr_Gender): argument is not numeric\nor logical: returning NA\n\n\n[1] NA\n\n\nyou’ll see that the levels have been transformed AND trying to get the mean of a categorical value gives us an error (as it should!)\nYou’ll note that in order to do this we had perform two steps: 1. We created a new column of data (vector) using mutate(), and then we fixed the coding of that column using recode(). There is a more efficient way of doing this using the pipe operator (see Winter, 2.5). I’ll come back to this in the Level-up section below.\nBefore moving on, I want to stress that we can assign our summary table to an object:\n\naggression_data_summary <- psych::describe(aggression_data)\n\nand now anytime we call aggression_data_summary we get this table\n\naggression_data_summary\n\n             vars   n  mean    sd median trimmed   mad   min   max range  skew\nage             1 275 20.21  4.96  18.00   19.01  1.48 17.00 50.00 33.00  3.70\nBPAQ            2 275  2.61  0.52   2.62    2.61  0.56  1.34  4.03  2.69  0.01\nAISS            3 275  2.56  0.37   2.55    2.56  0.37  1.45  3.70  2.25  0.01\nalcohol         4 270 16.00 15.87  12.00   13.69 14.83  0.00 96.00 96.00  1.50\nBIS             5 275  2.28  0.35   2.27    2.27  0.40  1.42  3.15  1.73  0.36\nNEOc            6 275  3.55  0.59   3.58    3.55  0.62  1.83  4.92  3.08 -0.16\ngender          7 275  0.79  0.41   1.00    0.86  0.00  0.00  1.00  1.00 -1.44\nNEOo            8 275  3.36  0.52   3.42    3.37  0.49  1.67  4.67  3.00 -0.25\nfctr_Gender*    9 275  1.79  0.41   2.00    1.86  0.00  1.00  2.00  1.00 -1.44\n             kurtosis   se\nage             15.43 0.30\nBPAQ            -0.41 0.03\nAISS             0.23 0.02\nalcohol          3.09 0.97\nBIS             -0.22 0.02\nNEOc            -0.11 0.04\ngender           0.06 0.02\nNEOo            -0.09 0.03\nfctr_Gender*     0.06 0.02"
  },
  {
    "objectID": "week02/2_2-central_tendency.html#getting-summary-stats-by-groups-or-conditions",
    "href": "week02/2_2-central_tendency.html#getting-summary-stats-by-groups-or-conditions",
    "title": "\n10  Measures of central tendency\n",
    "section": "\n10.3 Getting summary stats by groups or conditions",
    "text": "10.3 Getting summary stats by groups or conditions\nYou may also get summary statistics subsetted for individual groups. In this case the fctr_Gender column that we just created above has two groups, men and women. To take a look at the summary stat by Gender, or fctr_Gender we use the psych::describeBy() function, and designate the group that we want to split by:\n\npsych::describeBy(aggression_data,group = aggression_data$fctr_Gender)\n\n\n Descriptive statistics by group \ngroup: Man\n             vars  n  mean    sd median trimmed   mad   min   max range  skew\nage             1 57 20.60  5.87  18.00   19.21  0.00 17.00 50.00 33.00  3.09\nBPAQ            2 57  2.66  0.51   2.69    2.67  0.66  1.62  3.69  2.07  0.00\nAISS            3 57  2.74  0.34   2.75    2.73  0.30  1.80  3.70  1.90  0.28\nalcohol         4 55 17.95 18.22  12.00   15.42 17.79  0.00 73.00 73.00  1.06\nBIS             5 57  2.23  0.33   2.19    2.21  0.29  1.69  3.12  1.42  0.64\nNEOc            6 57  3.64  0.56   3.67    3.64  0.62  2.33  4.75  2.42 -0.13\ngender          7 57  0.00  0.00   0.00    0.00  0.00  0.00  0.00  0.00   NaN\nNEOo            8 57  3.27  0.59   3.33    3.30  0.62  1.67  4.33  2.67 -0.41\nfctr_Gender*    9 57  1.00  0.00   1.00    1.00  0.00  1.00  1.00  0.00   NaN\n             kurtosis   se\nage             10.46 0.78\nBPAQ            -0.96 0.07\nAISS             0.95 0.05\nalcohol          0.44 2.46\nBIS              0.03 0.04\nNEOc            -0.65 0.07\ngender            NaN 0.00\nNEOo            -0.10 0.08\nfctr_Gender*      NaN 0.00\n------------------------------------------------------------ \ngroup: Woman\n             vars   n  mean    sd median trimmed   mad   min   max range  skew\nage             1 218 20.11  4.70  19.00   19.03  1.48 17.00 50.00 33.00  3.86\nBPAQ            2 218  2.60  0.53   2.62    2.60  0.56  1.34  4.03  2.69  0.02\nAISS            3 218  2.51  0.37   2.52    2.51  0.37  1.45  3.70  2.25 -0.01\nalcohol         4 215 15.50 15.22  12.00   13.36 13.34  0.00 96.00 96.00  1.63\nBIS             5 218  2.30  0.36   2.27    2.28  0.40  1.42  3.15  1.73  0.29\nNEOc            6 218  3.52  0.60   3.50    3.53  0.49  1.83  4.92  3.08 -0.15\ngender          7 218  1.00  0.00   1.00    1.00  0.00  1.00  1.00  0.00   NaN\nNEOo            8 218  3.38  0.50   3.42    3.39  0.49  1.83  4.67  2.83 -0.14\nfctr_Gender*    9 218  2.00  0.00   2.00    2.00  0.00  2.00  2.00  0.00   NaN\n             kurtosis   se\nage             17.01 0.32\nBPAQ            -0.31 0.04\nAISS             0.01 0.02\nalcohol          4.16 1.04\nBIS             -0.27 0.02\nNEOc            -0.04 0.04\ngender            NaN 0.00\nNEOo            -0.30 0.03\nfctr_Gender*      NaN 0.00\n\n\nCheck out this video briefly describing one of the quirks of the psych::describe() output: https://youtu.be/ZFHTTY9886k"
  },
  {
    "objectID": "week02/2_3-summary_tables.html#preparing-the-data",
    "href": "week02/2_3-summary_tables.html#preparing-the-data",
    "title": "\n11  Creating summary tables\n",
    "section": "\n11.1 Preparing the data",
    "text": "11.1 Preparing the data\nFirst let’s load in the packages we’ll need for this:\n\npacman::p_load(psych, tidyverse, pander)\n\nand then recreate the aggression data summary table from the previous walkthrough (alternatively, if you’ve already done so just pull up the aggression_data_summary object). Here I recreate our steps from the previous walkthrough:\n\nload in the data\n\n\nlibrary(readr)\naggression_data <- read_table(\"https://raw.githubusercontent.com/tehrandavis/graduate_statistics/main/practice_datasets/aggression.dat\", col_names = FALSE)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  X1 = col_double(),\n  X2 = col_double(),\n  X3 = col_double(),\n  X4 = col_double(),\n  X5 = col_double(),\n  X6 = col_double(),\n  X7 = col_double(),\n  X8 = col_double()\n)\n\n\n\n\n\n\nwrangle the data (in this case we only need to add names, and change gender to a category)\n\n\n# add headers:\nnames(aggression_data) <- c('age', 'BPAQ', 'AISS', 'alcohol', 'BIS', 'NEOc', 'gender', 'NEOo')\n\naggression_data <- aggression_data %>% mutate(fctr_Gender = factor(gender)) \naggression_data$fctr_Gender <- recode_factor(aggression_data$fctr_Gender, \"0\" = \"Man\", \"1\" = \"Woman\")\n# \n\n\nget summary stats using psych_describe():\n\n\npsych::describe(aggression_data)\n\n             vars   n  mean    sd median trimmed   mad   min   max range  skew\nage             1 275 20.21  4.96  18.00   19.01  1.48 17.00 50.00 33.00  3.70\nBPAQ            2 275  2.61  0.52   2.62    2.61  0.56  1.34  4.03  2.69  0.01\nAISS            3 275  2.56  0.37   2.55    2.56  0.37  1.45  3.70  2.25  0.01\nalcohol         4 270 16.00 15.87  12.00   13.69 14.83  0.00 96.00 96.00  1.50\nBIS             5 275  2.28  0.35   2.27    2.27  0.40  1.42  3.15  1.73  0.36\nNEOc            6 275  3.55  0.59   3.58    3.55  0.62  1.83  4.92  3.08 -0.16\ngender          7 275  0.79  0.41   1.00    0.86  0.00  0.00  1.00  1.00 -1.44\nNEOo            8 275  3.36  0.52   3.42    3.37  0.49  1.67  4.67  3.00 -0.25\nfctr_Gender*    9 275  1.79  0.41   2.00    1.86  0.00  1.00  2.00  1.00 -1.44\n             kurtosis   se\nage             15.43 0.30\nBPAQ            -0.41 0.03\nAISS             0.23 0.02\nalcohol          3.09 0.97\nBIS             -0.22 0.02\nNEOc            -0.11 0.04\ngender           0.06 0.02\nNEOo            -0.09 0.03\nfctr_Gender*     0.06 0.02"
  },
  {
    "objectID": "week02/2_3-summary_tables.html#generating-summary-statistics-using-tidy-functions.",
    "href": "week02/2_3-summary_tables.html#generating-summary-statistics-using-tidy-functions.",
    "title": "\n11  Creating summary tables\n",
    "section": "\n11.2 Generating summary statistics using tidy functions.",
    "text": "11.2 Generating summary statistics using tidy functions.\nOne downside is that by default, psych::describe() doesn’t give us nice pretty (APA-esque) tables, and putting this output into a cleaner format requires a little work. In order to do so, we can leverage some of the functions from the tidyverse. How you go about constructing a cleaner table depends in large part on the content. For example, if you just wanted to convey the number of observations, mean, and std. dev, then it might be best to construct a summary table using dplyr::summarize(). For example a summary of BPAQ:\n\naggression_data_table <- dplyr::summarise(aggression_data,\n                                          \"count\" = n(),\n                                          \"mean\" = mean(BPAQ),\n                                          \"sd\" = sd(BPAQ)\n                                          ) \npander(aggression_data_table,\n       caption = \"Table 1. Summary statistics of BPAQ Scores\")\n\n\nTable 1. Summary statistics of BPAQ Scores\n\n\n\n\n\n\ncount\nmean\nsd\n\n\n275\n2.613\n0.524\n\n\n\n\npander() allows us to put the table in a neat (publication friendly) format, including adding a caption or title.\nUsing the pipe operator we can simplify the code above:\n\ndplyr::summarise(aggression_data,\n                 \"count\" = n(),\n                 \"mean\" = mean(BPAQ),\n                 \"sd\" = sd(BPAQ)\n                 ) %>% # tying these two together with a pipe\n  pander(caption = \"Table 1. Summary statistics of BPAQ Scores\")\n\n\nTable 1. Summary statistics of BPAQ Scores\n\n\n\n\n\n\ncount\nmean\nsd\n\n\n275\n2.613\n0.524\n\n\n\n\nWe can also create a simple table by fctr_Gender by using dplyr::group_by()\n\naggression_data %>% \n  dplyr::group_by(fctr_Gender) %>% \n  dplyr::summarise(\"count\" = n(),\n                   \"mean\" = mean(BPAQ),\n                   \"sd\" = sd(BPAQ)\n                   ) %>% \n  pander(caption = \"Table X. Summary statistics of BPAQ Scores by group\")\n\n\nTable X. Summary statistics of BPAQ Scores by group\n\n\n\n\n\n\n\nfctr_Gender\ncount\nmean\nsd\n\n\n\nMan\n57\n2.662\n0.5096\n\n\nWoman\n218\n2.6\n0.5281\n\n\n\n\n\nThere is still some work to be done if you were sending a table like this to publication. For most of our purposes in class, however, this will do."
  },
  {
    "objectID": "week02/2_3-summary_tables.html#better-apa-using-a-custom-function",
    "href": "week02/2_3-summary_tables.html#better-apa-using-a-custom-function",
    "title": "\n11  Creating summary tables\n",
    "section": "\n11.3 Better APA, using a custom function",
    "text": "11.3 Better APA, using a custom function\nFor an alternative, check out this link: https://www.anthonyschmidt.co/post/2020-06-03-making-apa-tables-with-gt/. The author, using the gt package build his own function, apa() to approximate APA formatted tables. This would be an alternative to using pander.\nThe code chunks below leverages his work.\nFirst, his custom function:\n\n# create apa function\n\npacman::p_load(gt)\n\napa <- function(x, title = \" \") {\n  gt(x) %>%\n  tab_options(\n    table.border.top.color = \"white\",\n    heading.title.font.size = px(16),\n    column_labels.border.top.width = 3,\n    column_labels.border.top.color = \"black\",\n    column_labels.border.bottom.width = 3,\n    column_labels.border.bottom.color = \"black\",\n    table_body.border.bottom.color = \"black\",\n    table.border.bottom.color = \"white\",\n    table.width = pct(100),\n    table.background.color = \"white\"\n  ) %>%\n  cols_align(align=\"center\") %>%\n  tab_style(\n    style = list(\n      cell_borders(\n        sides = c(\"top\", \"bottom\"),\n        color = \"white\",\n        weight = px(1)\n      ),\n      cell_text(\n        align=\"center\"\n      ),\n      cell_fill(color = \"white\", alpha = NULL)\n      ),\n    locations = cells_body(\n      columns = everything(),\n      rows = everything()\n    )\n  ) %>%\n    #title setup\n    tab_header(\n    title = html(\"<i>\", title, \"</i>\")\n  ) %>%\n  opt_align_table_header(align = \"left\")\n}\n\nAnd now applying it to our own data:\n\naggression_data %>%\n  dplyr::group_by(fctr_Gender) %>% \n  dplyr::summarise(\"count\" = n(),\n                   \"mean\" = mean(BPAQ),\n                   \"sd\" = sd(BPAQ)\n                   ) %>%\n  apa(\"Table X. Summary statistics of BPAQ Scores by group\")\n\n\n\n\n\n\n\n Table X. Summary statistics of BPAQ Scores by group \n    \n\nfctr_Gender\n      count\n      mean\n      sd\n    \n\n\n\nMan\n57\n2.661827\n0.509567\n\n\nWoman\n218\n2.599652\n0.528107\n\n\n\n\n\n\nNote that the formatting occurs when you Render."
  },
  {
    "objectID": "week03/3_1-basic_ggplot.html#the-basics",
    "href": "week03/3_1-basic_ggplot.html#the-basics",
    "title": "\n12  Basic ggplotting\n",
    "section": "\n12.1 The Basics",
    "text": "12.1 The Basics\nIn general, the basic procedure for constructing a plot goes through the following steps:\n\nlink to the data and tell ggplot how to structure its output\ntell ggplot what kind of plot to make\nadjust the axes and labels if necessary\nadjust the appearance of the plot for APA (or other appropriate format)\nadd a legend if necessary\nsave or export the plot\n\nPerhaps most critically, building plot (much like running an analysis) is something that should be carefully considered and well-thought out. In order words it’s probably in your best interest early on to “sketch out” mentally of with a pen an paper, what your plot should look like—what’s on the x-axis? what’s on the y-axis, what kind of plot?, any coloring or other stylizing? is the data going to be grouped? etc.\n\n12.1.1 Step 1: Building the canvas\nHere you need to be thinking about what form you want the plot to take. Key points:\n\n\ndata =: what data set you will be pulling from. This needs to be in the form of a data_frame(), with names in the headers. For most data that we will be working with from here on out, that will be the case. However, for constructed / simulation data you may have to do this manually.\n\nIf you are unsure of the header names you can see them by:\n\nnames(aggression_data)\n\n[1] \"age\"         \"BPAQ\"        \"AISS\"        \"alcohol\"     \"BIS\"        \n[6] \"NEOc\"        \"gender\"      \"NEOo\"        \"fctr_Gender\"\n\n\nNote that depending on what guide you follow you may also include mapping=aes() here as well. This would include telling R eventually what info goes on each axis, how data is grouped, etc. BUT this info can also be relayed in Step 2, and I think that conceptually it make more sense to put it there.\nSo for Step 1, just tell ggplot what data we are using and save our Step 1 to an object called p\n\np <- ggplot(data = aggression_data)\nshow(p)\n\n\n\n\nAs you can see we’ve created a blank canvas—nothing else to see here.\n\n12.1.2 Step 2: Tell ggplot what kind of plot to make.\nPlots can take several forms, or geoms (geometries), most common include: + histogram: geom_histogram + boxplot: geom_boxplot + scatter: geom_point + bar: geom_bar + line: geom_line\nHere we are creating a histogram, so in Step 2 we add geom_histogram() to our original plot p. We also need to tell ggplot how to go about constructing our histogram. This info would be including in the mapping = aes() argument. This tells ggplot about the general layout of the plot, including: + x: what is on the x-axis + y: what’s on the y-axis (usually your dependent variable, although for histograms this ends up being frequency and x is your dependent variable) + group: how should the data be grouped? This will become important for more complex designs.\nWhat aes() options you choose in large part is determined by what kind of plot you intend to make. For our histogram we want bins of BPAQ on the x-axis and frequency ..count.. on the y-axis.\n\n# Repeating previous step for clarity:\np <- ggplot(data = aggression_data)\n# new step, take our original \"p\", add a geom, save the new plot to \"p\"\np <- p + geom_histogram(mapping = aes(x=BPAQ,y=..count..)) \n# show the result\nshow(p)\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe default geom_histogram() produces the above plot. However, we can tweak several arguments in geom_histogram() to change the presentation of data, including:\n\n\nbinwidth: the width of each bin, or…\n\nbins: the number of bins\n\ncolor: the color of the perimeter of each bin/bar (note color refers to lines)\n\nfill: the color of the bin/bar itself (note fill refers to spaces)\n\nmapping = aes(): change the mapping (see below)\n\nFor example, to get this in APA format we would like light gray bars with black lines. Rather than the default 30 bins, we elect to use the rule of thumb number of bins should equal the \\(\\sqrt{N}\\).\n\n# step 1\np <- ggplot(data = aggression_data) \n\n# before plotting get the get the number of observations\nN <- nrow(aggression_data) \n\n#step 2:\np <- p + geom_histogram(mapping = aes(x=BPAQ,y=..count..), \n                        fill=\"light gray\",\n                        color=\"black\", \n                        bins = sqrt(N))\n# show the result\nshow(p)\n\nWarning: Computation failed in `stat_bin()`\nCaused by error in `bin_breaks_bins()`:\n! `bins` must be a whole number, not the number 16.58.\n\n\n\n\n\n\n12.1.3 Step 3: Adjust the axes and labels\nThe plot at the end of Step 2 is almost there, but there are a few issues that remain. First, our axis labels could be more descriptive than “count” and “BPAQ”. This is solved by adding the arguments xlab() and ylab() to our plot. For example:\n\n#STEP 1\np <- ggplot(data = aggression_data) \n\n#STEP 2\n# get the number of observations\nN <- nrow(aggression_data) \n\n# number of bins is sqrt of N\np <- p + geom_histogram(mapping = aes(x=BPAQ,y=..count..), \n                        fill=\"light gray\",\n                        color=\"black\", \n                        bins = sqrt(N))\n# STEP 3:\np <- p + xlab(\"BPAQ score\") + ylab(\"Number of Scores\")\nshow(p)\n\nWarning: Computation failed in `stat_bin()`\nCaused by error in `bin_breaks_bins()`:\n! `bins` must be a whole number, not the number 16.58.\n\n\n\n\n\nAnother issue may be that gap that sits between the x-axis (x=0) and the axis scale. This can be remedied by adding the following to our plot p:\n\np <- p + coord_cartesian(ylim=c(0,40), expand = FALSE)\nshow(p)\n\nWarning: Computation failed in `stat_bin()`\nCaused by error in `bin_breaks_bins()`:\n! `bins` must be a whole number, not the number 16.58.\n\n\n\n\n\nThe coord_cartesian() command allows us to zoom in or zoom out of our axes. There are several arguments that this command takes including:\n\n\nxlim=: takes a pair of values c(lower,upper) for limits of x-axis (note that I didn’t modify this, but you can try it just to see what happens)\n\nylim=: same as above, but for y-axis\n\nexpand=: do you want to create additional whitespace between your data and axes?\n\nFor example if I wanted to only show Stat quiz scores with Number of Scores between 15 and 40 I can just zoom into my plot (with needing to go back and filter my original data)\n\np <- p + coord_cartesian(ylim=c(15, 40), expand = FALSE)\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\nshow(p)\n\nWarning: Computation failed in `stat_bin()`\nCaused by error in `bin_breaks_bins()`:\n! `bins` must be a whole number, not the number 16.58.\n\n\n\n\n\nOr if I instead wanted to only focus on those Students that scored between 4 and 8:\n\np <- p + coord_cartesian(xlim=c(2, 3), ylim=c(0,40), expand = FALSE)\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\nshow(p)\n\nWarning: Computation failed in `stat_bin()`\nCaused by error in `bin_breaks_bins()`:\n! `bins` must be a whole number, not the number 16.58.\n\n\n\n\n\nNot something I would in this case, but just an example. OK, let’s get back to our working plot:\n\n#STEP 1\np <- ggplot(data = aggression_data) \n\n#STEP 2\n# get the number of observations\nN <- nrow(aggression_data) \n\n# number of bins is sqrt of N\np <- p + geom_histogram(mapping = aes(x=BPAQ,y=..count..), \n                        fill=\"light gray\",\n                        color=\"black\", \n                        bins = sqrt(N))\n# STEP 3:\n\n# adjust axes labels\np <- p + xlab(\"Stats quiz score\") + ylab(\"Number of Scores\")\n# adjust scale of x,y axes if needed:\np <- p + coord_cartesian(ylim=c(0,40), expand = FALSE)\nshow(p)\n\nWarning: Computation failed in `stat_bin()`\nCaused by error in `bin_breaks_bins()`:\n! `bins` must be a whole number, not the number 16.58.\n\n\n\n\n\nOne last thing to consider regarding our axes are the location of labels on the scale. For example the y-axis has labels at every 10 and the x-axis at every 1. What if we want to change the labels on the x-axis? Here we can add scale_y_continuous to our plot p. We need to tell scale_y_continuous what our desired sequence is. You’ve done sequences before, like the sequence from 0 to 10:\n\n0:10\n\n [1]  0  1  2  3  4  5  6  7  8  9 10\n\n\nHowever, if we want to say count by 2’s the command is a little more involved\n\nseq(0,10,2)\n\n[1]  0  2  4  6  8 10\n\n\nwhere seq(start, stop, by). So to start at 0 and stop at 4 (along the x-axis) going by .5 in our plot we add breaks= seq(0, 4, .5):\n\n#STEP 1\np <- ggplot(data = aggression_data) \n\n#STEP 2\n# get the number of observations\nN <- nrow(aggression_data) \n\n# number of bins is sqrt of N\np <- p + geom_histogram(mapping = aes(x=BPAQ,y=..count..), \n                        fill=\"light gray\",\n                        color=\"black\", \n                        bins = sqrt(N))\n# STEP 3:\n\n# adjust axes labels\np <- p + xlab(\"Stats quiz score\") + ylab(\"Number of Scores\")\n# adjust scale of x,y axes if needed:\np <- p + coord_cartesian(ylim=c(0,40), expand = FALSE)\n# change x axis ticks:\np <- p + scale_x_continuous(breaks = seq(0,4,.5))\nshow(p)\n\nWarning: Computation failed in `stat_bin()`\nCaused by error in `bin_breaks_bins()`:\n! `bins` must be a whole number, not the number 16.58.\n\n\n\n\n\nCan you figure out how we make transform the Y-axis labels to count by 5’s instead of 10’s??\n\n12.1.4 Step 4: Adjusting for APA\nWhile the plot at the end of Step 4 is almost there, there are a few issues remaining, Including that gray grid in the background and the lack of axis lines. In the past these changes would need to be done line by line. But fortunately for us we are in the glorious present, and there is a library that does a lot of this cleaning for you.\nIntroducing cowplot!!!!\n\npacman::p_load(cowplot)\n\nOK, we’ve loaded cowplot. Now what? Simply re-run the previous chunk, but this time adding theme_cowplot() to the end. Note that running ? theme_cowplot shows that it includes arguments for font size, font family (type), and line size. Below I’m just going to ask for size “15” font. I typically leave the font type alone, but if I do change it I may occasionally use font_family=\"Times\":\n\np <- ggplot(data = aggression_data) \n\n#STEP 2\n# get the number of observations\nN <- nrow(aggression_data) \n\n# number of bins is sqrt of N\np <- p + geom_histogram(mapping = aes(x=BPAQ,y=..count..), \n                        fill=\"light gray\",\n                        color=\"black\", \n                        bins = sqrt(N))\n# STEP 3:\n\n# adjust axes labels\np <- p + xlab(\"Stats quiz score\") + ylab(\"Number of Scores\")\n# adjust scale of x,y axes if needed:\np <- p + coord_cartesian(ylim=c(0,40), expand = FALSE)\n# change x axis ticks:\np <- p + scale_x_continuous(breaks = 1:10)\n\n# STEP 4:\np <- p + theme_cowplot()\nshow(p)\n\nWarning: Computation failed in `stat_bin()`\nCaused by error in `bin_breaks_bins()`:\n! `bins` must be a whole number, not the number 16.58.\n\n\n\n\n\nEasy-peasy! You’ll note that cowplot adjusted your fonts, fixed your axes, and removed that nasty background. Even more, once cowplot is loaded it doesn’t need to be called to do this. It just sits in the background and automatically adjusts the format of any plot you make using ggplot. If you ever wanted to return to the default plot style you can run the following:\n\ntheme_set(theme_gray())\n\nBut for now cowplot FTW!!! (other cool cowplot things will show up in the advanced section).\n\n12.1.5 Step 5: Add and adjust the legend\nDoesn’t make sense in this context so not going to spend a lot of time here, but more on this when it’s relevant.\n\n12.1.6 Condensing the code\nFor the purposes of making the steps clear, we separated the steps out, adding to and overwriting p (previous iterations of the plot). But, this could all be accomplished in a single call, separating each step with the “+” sign\n\n# get N for a frequency plot\nN <- nrow(aggression_data) \n\n# create plot\np <- ggplot(data = aggression_data) + \n  geom_histogram(mapping = aes(x=BPAQ,y=..count..), \n                 fill=\"light gray\",\n                 color=\"black\", \n                 bins = sqrt(N)) +\n  xlab(\"Stats quiz score\") + ylab(\"Number of Scores\") + \n  coord_cartesian(ylim=c(0,40), expand = FALSE) +\n  scale_x_continuous(breaks = 1:10) + \n  theme_cowplot()\nshow(p)\n\nWarning: Computation failed in `stat_bin()`\nCaused by error in `bin_breaks_bins()`:\n! `bins` must be a whole number, not the number 16.58."
  },
  {
    "objectID": "week03/3_1-basic_ggplot.html#saving-the-ggplot.",
    "href": "week03/3_1-basic_ggplot.html#saving-the-ggplot.",
    "title": "\n12  Basic ggplotting\n",
    "section": "\n12.2 Saving the ggplot.",
    "text": "12.2 Saving the ggplot.\nIn many cases it may make sense to save your resulting plot. For example if you want to move your plot to a Word document or save a folder of plots for easy visualization.\nWithin command-line, plots can be saved using the ggsave() function. Note that both cowplot and ggplot libraries have a ggsave() function. They for the most part do the same thing. Lets get some help on ggsave() to see exactly what the parameters are:\n\n? ggsave()\n\nfrom the result the important arguments are:\n\nfilename = Filename of plot plot = Plot to save, defaults to last plot displayed. device = what kind of file, depending on extension. path = Path to save plot to (defaults to project folder). scale = Scaling factor. width = Width (defaults to the width of current plotting window). height = Height (defaults to the height of current plotting window). units = Units for width and height (in, cm, or mm). dpi = DPI to use for raster graphics.\n\nSo to save the previous plot to my project folder, p as a .pdf to a file names “histogram.pdf” with the dimensions 8-in by 6-in, and a DPI of 300 (300 is usually the min you want for print)\n\nggsave(filename = \"histogram.pdf\", \n       plot = p,\n       width = 8,\n       height = 6,\n       units = \"in\",\n       dpi = 300)\n\nWarning: Computation failed in `stat_bin()`\nCaused by error in `bin_breaks_bins()`:\n! `bins` must be a whole number, not the number 16.58.\n\n\nif I wanted an image file like a .png, I just change the filename extension:\n\nggsave(filename = \"histogram.png\", \n       plot = p,\n       width = 8,\n       height = 6,\n       units = \"in\",\n       dpi = 300)\n\nWarning: Computation failed in `stat_bin()`\nCaused by error in `bin_breaks_bins()`:\n! `bins` must be a whole number, not the number 16.58.\n\n\nNote that you can also copy and save plots that are printed in your Notebook and Plots tab. In the notebook, simply right click on top of the plot for a quick Copy or Download. For plots printed to the Plots tab, click the Export button (this gives you all the options as ggsave)."
  },
  {
    "objectID": "week03/3_1-basic_ggplot.html#advanced-stuff",
    "href": "week03/3_1-basic_ggplot.html#advanced-stuff",
    "title": "\n12  Basic ggplotting\n",
    "section": "\n12.3 Advanced stuff",
    "text": "12.3 Advanced stuff\n\n12.3.1 Customizing your plot\nLet’s rerun the example one more time for practice, but this time lets make things a little more concrete in terms of customization. Again I’m going to go through step-by-step for clarity. This time we’ll create a histogram, with an overlay of the normal distribution (more on that next week)\nFirst let’s use ggplot() to create a histogram. To get a feel for each step, run through this code line by line in RStudio:\n\n# load in tidyverse if you haven't already\npacman::p_load(tidyverse)\n\n# Step 1: identify data and grouping parameters\nhistPlot1 <- ggplot2::ggplot(data = aggression_data, aes(x=BPAQ))\nhistPlot1 # note that this produces a blank slate but the parameters are locked in.\n\n\n\n# take histPlot 1 (blank plot) and paint a layer on top of it\n# tell ggplot what to do (this is where you actually build the graphics):\n\n# take histPlot 1's parameters and...\nhistPlot2 <- histPlot1 + \n  # build a histogram \n  geom_histogram(binwidth = .25, # bins are 5 units wide\n                color = \"orange\", # what color is the outline of bars\n                fill = \"green\") # what color to fill the bars with\nhistPlot2 # show the frequency plot\n\n\n\n\nI intentionally made a hideous looking plot above to show you what the additional arguments do. Let’s turn this into something a little more pleasing to the eyes:\n\nhistPlot2 <- histPlot1 + \n  geom_histogram(binwidth = .25, \n                color = \"black\", # what color is the outline of bars\n                fill = \"white\") # what color to fill the bars with\nhistPlot2\n\n\n\n\nOkay, now to add a curve representing the normal distribution. One important thing to note is that the histogram that is ultimately produced here has probability density (think like % of scores) on the y-axis instead of frequency (# of scores). So we’ll need to convert our frequency plot to a probability density plot. Fortunately this is just one line of code. Instead of ..count.. we use ..density...\n\n# take histPlot 1 (blank plot) and paint a layer on top of it\n# tell ggplot what to do (this is where you actually build the graphics):\nhistPlot3 <- histPlot1 + \n  geom_histogram(binwidth = .25, \n                 color = \"black\",\n                 fill = \"white\",\n                 aes(y=..density..)# convert to a prob density plot\n  )\nhistPlot3 \n\n\n\n\nnotice that histPlot3 is a density plot rather frequency plot from above. Now to add the normal curve:\n\n# add a normal curve to density plot:\nhistPlot4 <- histPlot3 + \n  stat_function(fun = dnorm, # generate theoretical norm data\n                color = \"red\", # color the line red\n                args=list(\n                  mean = mean(aggression_data$BPAQ,na.rm = T), # build around mean\n                  sd = sd(aggression_data$BPAQ,na.rm = T) # st dev parameter\n                )\n  )\n\nhistPlot4\n\n\n\n\nSome suggest that a better alternative to the normal curve is a kernel density plot. To fit a kernel density plot to our histogram (histPlot3) we can invoke:\n\n# hisPlot 3 was the base histogram without curve\nhistPlot3 + geom_density()\n\n\n\n\nand let’s make the x-axis label a little more transparent with xlab()\n\nhistPlot3 + geom_density() + xlab(\"BPAQ scores\")\n\n\n\n\nKeep in mind, that although we walked-through making histPlot1, then histPlot2, histPlot3, histPlot4, we could have accomplished all of this with a single assign:\n\nhistPlot <- ggplot2::ggplot(data = aggression_data, aes(x=BPAQ)) +\n  geom_histogram(binwidth = .25, \n                color = \"black\", \n                fill = \"white\",\n                aes(y=..density..))  +\n  stat_function(fun = dnorm, # generate theoretical norm data\n                color = \"red\", # color the line red\n                args=list(\n                  mean = mean(aggression_data$BPAQ,na.rm = T), # build around mean\n                  sd = sd(aggression_data$BPAQ,na.rm = T) # st dev parameter\n                )\n  )\n\nhistPlot\n\n\n\n\nMoreover, we could facet the plot by some categorical variable, for example fctr_Gender. Conceptually, faceting is the equivalent of performing psych::describeBy(), separating the data by some grouping variable. For example, to get the summary stats of BPAQ scores by gender:\n\n# specifying $BPAQ because I don't want a print out of the entire dataframe\npsych::describeBy(x = aggression_data$BPAQ, group = aggression_data$fctr_Gender)\n\n\n Descriptive statistics by group \ngroup: Man\n   vars  n mean   sd median trimmed  mad  min  max range skew kurtosis   se\nX1    1 57 2.66 0.51   2.69    2.67 0.66 1.62 3.69  2.07    0    -0.96 0.07\n------------------------------------------------------------ \ngroup: Woman\n   vars   n mean   sd median trimmed  mad  min  max range skew kurtosis   se\nX1    1 218  2.6 0.53   2.62     2.6 0.56 1.34 4.03  2.69 0.02    -0.31 0.04\n\n\nTo separate the two histogram distributions I can simply add facet_wrap(~fctr_Gender) to my ggplot call; ~ means “by”, as in facet wrap by Gender (remember that fctr_Gender is a column we created to rid ourselves of the numeric coding):\n\nhistPlot <- ggplot2::ggplot(data = aggression_data, aes(x=BPAQ)) +\n  geom_histogram(binwidth = .25, \n                color = \"black\", \n                fill = \"white\",\n                aes(y=..density..))  +\n  stat_function(fun = dnorm, # generate theoretical norm data\n                color = \"red\", # color the line red\n                args=list(\n                  mean = mean(aggression_data$BPAQ,na.rm = T), # build around mean\n                  sd = sd(aggression_data$BPAQ,na.rm = T) # st dev parameter\n                )\n  ) +\n  facet_wrap(~fctr_Gender)\nhistPlot\n\n\n\n\n\n12.3.2 Plotting your data in a boxplot:\nNow let’s create a few boxplots of BPAQ. First the entire dataset, indifferent to the 2 different categories of fctr_Gender:\n\n# Set the parameters (data and axes)\nggplot2::ggplot(data = aggression_data) + \n  geom_boxplot(mapping = aes(y=BPAQ)) # tell R to make it a boxplot\n\n\n\n\nNote that the x-axis in this plot is meaningless. We can remove it if we want:\n\n# Set the parameters (data and axes)\nggplot2::ggplot(data = aggression_data) + \n  geom_boxplot(mapping = aes(y=BPAQ)) +    # tell R to make it a boxplot\n  theme(axis.text.x=element_blank(), # element_blank removes item, in this case x-axis text\n        axis.ticks.x = element_blank() # and ticks\n        )\n\n\n\n\nNow let’s take a look at the data that will be boxplotted as a function of fctr_Gender groups. Here I want to note that psych::describeBy has an additional argument to display the quantiles:\n\npsych::describeBy(aggression_data,group = aggression_data$fctr_Gender,quant=c(.25,.75))\n\n\n Descriptive statistics by group \ngroup: Man\n             vars  n  mean    sd median trimmed   mad   min   max range  skew\nage             1 57 20.60  5.87  18.00   19.21  0.00 17.00 50.00 33.00  3.09\nBPAQ            2 57  2.66  0.51   2.69    2.67  0.66  1.62  3.69  2.07  0.00\nAISS            3 57  2.74  0.34   2.75    2.73  0.30  1.80  3.70  1.90  0.28\nalcohol         4 55 17.95 18.22  12.00   15.42 17.79  0.00 73.00 73.00  1.06\nBIS             5 57  2.23  0.33   2.19    2.21  0.29  1.69  3.12  1.42  0.64\nNEOc            6 57  3.64  0.56   3.67    3.64  0.62  2.33  4.75  2.42 -0.13\ngender          7 57  0.00  0.00   0.00    0.00  0.00  0.00  0.00  0.00   NaN\nNEOo            8 57  3.27  0.59   3.33    3.30  0.62  1.67  4.33  2.67 -0.41\nfctr_Gender*    9 57  1.00  0.00   1.00    1.00  0.00  1.00  1.00  0.00   NaN\n             kurtosis   se Q0.25 Q0.75\nage             10.46 0.78 18.00 19.00\nBPAQ            -0.96 0.07  2.28  3.17\nAISS             0.95 0.05  2.55  2.95\nalcohol          0.44 2.46  2.50 29.50\nBIS              0.03 0.04  2.04  2.42\nNEOc            -0.65 0.07  3.25  4.08\ngender            NaN 0.00  0.00  0.00\nNEOo            -0.10 0.08  2.83  3.75\nfctr_Gender*      NaN 0.00  1.00  1.00\n------------------------------------------------------------ \ngroup: Woman\n             vars   n  mean    sd median trimmed   mad   min   max range  skew\nage             1 218 20.11  4.70  19.00   19.03  1.48 17.00 50.00 33.00  3.86\nBPAQ            2 218  2.60  0.53   2.62    2.60  0.56  1.34  4.03  2.69  0.02\nAISS            3 218  2.51  0.37   2.52    2.51  0.37  1.45  3.70  2.25 -0.01\nalcohol         4 215 15.50 15.22  12.00   13.36 13.34  0.00 96.00 96.00  1.63\nBIS             5 218  2.30  0.36   2.27    2.28  0.40  1.42  3.15  1.73  0.29\nNEOc            6 218  3.52  0.60   3.50    3.53  0.49  1.83  4.92  3.08 -0.15\ngender          7 218  1.00  0.00   1.00    1.00  0.00  1.00  1.00  0.00   NaN\nNEOo            8 218  3.38  0.50   3.42    3.39  0.49  1.83  4.67  2.83 -0.14\nfctr_Gender*    9 218  2.00  0.00   2.00    2.00  0.00  2.00  2.00  0.00   NaN\n             kurtosis   se Q0.25 Q0.75\nage             17.01 0.32 18.00 20.00\nBPAQ            -0.31 0.04  2.21  2.97\nAISS             0.01 0.02  2.25  2.75\nalcohol          4.16 1.04  3.50 23.00\nBIS             -0.27 0.02  2.04  2.54\nNEOc            -0.04 0.04  3.17  3.90\ngender            NaN 0.00  1.00  1.00\nNEOo            -0.30 0.03  3.00  3.73\nfctr_Gender*      NaN 0.00  2.00  2.00\n\n\nTo plot this we need to tell R to put different distributions at different points along the x-axis\n\nggplot2::ggplot(data = aggression_data) + \n  geom_boxplot(mapping = aes(y=BPAQ, x = fctr_Gender))\n\n\n\n\nBefore leaving I want to show what would’ve happened in the latter case if we didn’t correct fctr_Gender and left it as a numeric variable (as gender in our data frame) rather than converting to a categorical:\n\n# creating the boxplot using exact code from chunk above:\nggplot2::ggplot(data = aggression_data, aes(y=BPAQ, x = gender)) + \n  geom_boxplot()\n\nWarning: Continuous x aesthetic\nℹ did you forget `aes(group = ...)`?\n\n\n\n\n\nSince R treats the categorical variable as a numeric, all of the data gets lumped together at the mean value of Gender even though we explicitly tried to group our data by this variable. Remember, you need to be careful with having categorical data coded as numbers!\n\n12.3.3 Cowplot\nFinally, we can combine multiple plots using the cowplot package and its two commands plot_grid() and plot_wrap. For example, we can recreate a plot in the style of Winter, Figure 3.4 (p. 59) by creating a histogram of our data, then a boxplot of out data, and then aligning them:\n\nhistPlot <- ggplot2::ggplot(data = aggression_data, aes(x=BPAQ)) +\n  geom_histogram(binwidth = .25, \n                color = \"black\", \n                fill = \"white\",\n                aes(y=..density..))  +\n  stat_function(fun = dnorm, # generate theoretical norm data\n                color = \"red\", # color the line red\n                args=list(\n                  mean = mean(aggression_data$BPAQ,na.rm = T), # build around mean\n                  sd = sd(aggression_data$BPAQ,na.rm = T) # st dev parameter\n                )\n  ) +\n  theme_cowplot()\n\nboxPlot <- ggplot2::ggplot(data = aggression_data) + \n  geom_boxplot(mapping = aes(y=BPAQ),outlier.color = \"red\") +  # tell R to make it a boxplot\n  coord_flip() + #make horizontal\n  theme(axis.text.x=element_blank(), # element_blank removes item, in this case x-axis text\n        axis.ticks.x = element_blank() # and ticks\n        ) +\n  theme_void()\n  \n  \n\ncombined_plot <- cowplot::plot_grid(histPlot,boxPlot, align = \"hv\",ncol = 1,rel_heights = c(4,1))\ncombined_plot\n\n\n\n\nFor other cool stuff that can be done with cowplot() including placing multiple plots side by side, check out the vignette links below written by its author, Claus O. Wilke:\n\nIntroduction to cowplot\nChanging the axis positions\nPlot annotations\nArranging plots in a grid\nShared legends"
  },
  {
    "objectID": "week03/3_2-more_adv_ggplot.html#the-data",
    "href": "week03/3_2-more_adv_ggplot.html#the-data",
    "title": "\n13  More advanced ggPlots\n",
    "section": "\n13.1 The data",
    "text": "13.1 The data\nI’ll be using a modified dataset from Danielle Navarro’s Learning statistics with R: A tutorial for psychology students and other beginners which is a nice online text that I may use as a backbone for the course in future years. If you’re interested on Navarro’s take on stats and R, please check it out here. Coincidentally, the lsr package she provides is what we use to calculate certain values like Cohen’s D.\n\nThe following data set was used to address the relationships that exist between a newbnorn baby’s (3-6 mos) nightly sleep, their parent’s sleep, and their parent’s overall mood. Several questions modivated the collection of this data, including: - what is the relationship between the average hours of sleep for the baby and parent? - what is the relationship between the average hours of sleep for the parent and their mood? - are these relationships influence by parental role—different for mothers and fathers - does the impact of a newborn baby on sleep differ for mothers and fathers (houts of sleep, grumpiness)\n\nthe dataset parent_sleep_data contains the following variables:\n\n\nparent: 100 mothers (typically primary care giver, especially if breast feeding) v. 100 fathers (un-matched)\n\nchild: 100 parents with their first child v. 100 parents with their second child\n\nparent_sleep: average hours parent sleep 30 days prior to assessment\n\nbaby_sleep: average hours babry sleep 30 days prior to assessment\n\nparent_grumpy: parent grunmpiness score\n\n\npacman::p_load(tidyverse, cowplot)\nparent_sleep_data <- read_csv(\"https://raw.githubusercontent.com/tehrandavis/graduate_statistics/main/practice_datasets/parent_sleep.csv\")\n\nRows: 200 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): parent, child\ndbl (3): parent_sleep, baby_sleep, parent_grumpy\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nYou might imagine, this data hits a little close to the heart ;)"
  },
  {
    "objectID": "week03/3_2-more_adv_ggplot.html#correlation-regression-for-2-continuous-variables",
    "href": "week03/3_2-more_adv_ggplot.html#correlation-regression-for-2-continuous-variables",
    "title": "\n13  More advanced ggPlots\n",
    "section": "\n13.2 Correlation / Regression for 2 continuous variables",
    "text": "13.2 Correlation / Regression for 2 continuous variables\nTypically with correlations and regressions you want to present your data in scatterplot form.\nYour scatterplot should include:\n\nall original data points\nthe regression line\nthe 95% CI about the regression line\nsome like the equation (coefficients) and the \\(R^2\\) in the plot, but I find it to be busy, I’d suggest mentioning these values in the figure caption or just save them for the text.\nI like to present the points in a lighter color, with the regression line in a heavier color.\n\n\n13.2.1 Basic regression example\nfor example if we wanted to plot the relationship between the average hours of parent’s sleep and their grumpiness:\n\nggplot(parent_sleep_data, aes(x = parent_sleep, y=parent_grumpy)) +\n  geom_point(color=\"lightgray\") + # points in light gray\n  stat_smooth(method = \"lm\", fullrange = T, se = T, # regression line with 95%CI\n              color = \"black\", linetype=\"solid\") + # color and linetype for regression line\n  xlab(\"Average hours parent sleep\") +\n  ylab(\"Parent Grumpiness Score\") + \n  theme_cowplot()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n13.2.2 Independent regression by group\nIn this case we are extending the question above, by asking whether the relationship between the average hours of parent’s sleep and their grumpiness differs by parental role. We can either put both groups on the sample plot (differentiating by shape, or color, or linetype), or place them side by side.\n\n13.2.2.1 same plot\nTo put different groups on the same plot use the group aesthetic (aes):\n\nggplot(parent_sleep_data, aes(x = parent_sleep, y=parent_grumpy, group=parent)) +\n  geom_point(aes(color=parent)) + \n  stat_smooth(method = \"lm\", fullrange = T, se = T, # regression line with 95%CI\n              color = \"black\", aes(linetype=parent)) + # color and linetype for regression line\n  xlab(\"Average hours parent sleep\") +\n  ylab(\"Parent Grumpiness Score\") + \n  theme_cowplot()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n13.2.2.2 seperate plots\nIf you find the plot about to be too busy, use facet_wrap() instead\n\nggplot(parent_sleep_data, aes(x = parent_sleep, y=parent_grumpy)) +\n  geom_point(aes(color=parent)) + \n  stat_smooth(method = \"lm\", fullrange = T, se = T, # regression line with 95%CI\n              color = \"black\", aes(linetype=parent)) + # color and linetype for regression line\n  xlab(\"Average hours parent sleep\") +\n  ylab(\"Parent Grumpiness Score\") + \n  theme_cowplot() + \n  facet_wrap(~parent)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n13.2.2.3 Combination\nHere I am plotting parent grumpiness as a function of avg. hours of sleep, differentiating BOTH by parent role and child:\n\nggplot(parent_sleep_data, aes(x = parent_sleep, y=parent_grumpy, group=parent)) +\n  geom_point(aes(color=parent)) + \n  stat_smooth(method = \"lm\", fullrange = T, se = T, # regression line with 95%CI\n              color = \"black\", aes(linetype=parent)) + # color and linetype for regression line\n  xlab(\"Average hours parent sleep\") +\n  ylab(\"Parent Grumpiness Score\") + \n  theme_cowplot() +\n  facet_wrap(~child)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWithout worrying about the statsitics, what story are these plots conveying about average hours sleep, relative grumpiness, and whether its your first born?"
  },
  {
    "objectID": "week03/3_2-more_adv_ggplot.html#differences-in-groups",
    "href": "week03/3_2-more_adv_ggplot.html#differences-in-groups",
    "title": "\n13  More advanced ggPlots\n",
    "section": "\n13.3 Differences in groups",
    "text": "13.3 Differences in groups\nTypically with differences in groups, you’ll want to present your data in one of several formats, depending on what you want to emphasize.\n\nIf your want to emphasize mean differences, choose a barplot or pointrange\nIf you want to emphaseze differences in distribution, choose a boxplot or violin plot\n\n\n13.3.1 Comparing groups (levels) of a single IV\nQuestion: Which parent typically gets less sleep when their in a newborn in the family?\n\n13.3.1.1 barplot\nBarplots are typically the industry convention, although we’ve noted their issues:\n\nggplot(parent_sleep_data, aes(x = parent, y=parent_sleep)) +\n  stat_summary(fun.y = \"mean\", geom = \"col\", fill=\"grey\") + # you can change the color of the bar using `fill`; `color` changes the color of the outline.\n  stat_summary(fun.data = \"mean_cl_normal\", geom = \"errorbar\", width=.15) +\n  xlab(\"Parent\") +\n  ylab(\"Average hours sleep\") + \n  theme_cowplot()\n\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\nℹ Please use the `fun` argument instead.\n\n\n\n\n\nOn annoying thing about barplots is that by default ggplot creats a gap betweeon 0 and the x-axis like above. To fix this, you need to add scale_y_continuous(expand =  c(0,0)) to every barplot that you intend to start from 0.\n\nggplot(parent_sleep_data, aes(x = parent, y=parent_sleep)) +\n  stat_summary(fun.y = \"mean\", geom = \"col\", fill=\"grey\") + # you can change the color of the bar using `fill`; `color` changes the color of the outline.\n  stat_summary(fun.data = \"mean_cl_normal\", geom = \"errorbar\", width=.15) +\n  xlab(\"Parent\") +\n  ylab(\"Average hours sleep\") + \n  theme_cowplot() +\n  scale_y_continuous(expand =  c(0,0))\n\n\n\n\n\n13.3.1.2 barplot with datapoints\nAnother option is barplot with raw data point overlays, although these plots can look odd / busy.\n\nggplot(parent_sleep_data, aes(x = parent, y=parent_sleep)) +\n  stat_summary(fun.y = \"mean\", geom = \"col\", position = position_dodge(.9), fill=\"grey\", col=\"black\") +\n  geom_point(size=.5, position = position_jitter(.01)) +\n  xlab(\"Parent\") +\n  ylab(\"Average hours sleep\") + \n  theme_cowplot() +\n  scale_y_continuous(expand =  c(0,0))\n\n\n\n\n\n13.3.1.3 boxplot\nBoxplots are useful if you want to give your reader a little more info about the distribution of the data being compared. Here is the mother v. father data in boxplot form\n\nggplot(parent_sleep_data, aes(x = parent, y=parent_sleep)) +\n  geom_boxplot() +\n  xlab(\"Parent\") +\n  ylab(\"Average hours sleep\") + \n  theme_cowplot() \n\n\n\n\n\n13.3.2 Comparing groups in a factorial design (interaction plot)\nOn we get to factoral ANOVA, we are comparing groups along multiple dimensions (multiple IVs). For example, here we are looking a means data of the average hours of sleep as a function of BOTH parent role and child number. A more technical way of saying this is that we are looking at these values as a function of the interaction between parent role and child number. Since there are two levels in both parent and child we end up with four means to convey. We typically want these means grouped in a way that conveys the design of the analysis.\nHere I am grouping parent along the x-axis, and grouping child by the geom.\n\n13.3.2.1 barplot with error bars\n\nggplot(parent_sleep_data, aes(x = parent, y=parent_sleep, group = child)) +\n  stat_summary(fun.y = \"mean\", geom = \"col\", position = position_dodge(.9), aes(fill=child), col=\"black\") +\n  stat_summary(fun.data = \"mean_cl_normal\", geom = \"errorbar\", width=.15, position = position_dodge(.9)) +\n  xlab(\"Parent\") +\n  ylab(\"Average hours sleep\") + \n  theme_cowplot()\n\n\n\n\n\n13.3.2.2 boxplot\nNote that when you put together a boxplot, you need to note that the grouping is by the interaction between the two variables.\n\nggplot(parent_sleep_data, aes(x = parent, y=parent_sleep, fill = child)) +\n  geom_boxplot(aes(group=interaction(child,parent))) + \n  xlab(\"Parent\") +\n  ylab(\"Average hours sleep\") + \n  theme_cowplot()\n\n\n\n\n\n13.3.2.3 violin-barplot combo\n\nggplot(parent_sleep_data, aes(x = parent, y=parent_sleep, fill = child)) +\n  geom_violin(trim = T,\n              aes(group=interaction(child,parent))) +\n  geom_boxplot(width = .15,\n               position = position_dodge(.9),\n                aes(group=interaction(child,parent))\n               ) + \n  xlab(\"Parent\") +\n  ylab(\"Average hours sleep\") + \n  theme_cowplot()"
  },
  {
    "objectID": "week03/3_2-more_adv_ggplot.html#line-plots",
    "href": "week03/3_2-more_adv_ggplot.html#line-plots",
    "title": "\n13  More advanced ggPlots\n",
    "section": "\n13.4 Line plots",
    "text": "13.4 Line plots\nTypically we only use line plots when the data is “connected” somehow. For example, imagine that instead of seperate groups for child (100 parents with their first v. 100 different parents with their second) be got data from parents with two children (200 parants comparing experience with first child v second). FWIW this would end up being a mixed design—parent: mother v. father is a between variable, and child would be a within. In this case it would make sense to draw a line plot connecting first child values to the second child values to emphasize that this data reflects that these data a meaningfully connected / dependent on one another.\nIn cases like these I recommend a line and pointrange plot. I also recommend getting familiar with position_dodge as a way of dealing with overlapping pointranges. For example, without the points and lines dodged:\n\nggplot(parent_sleep_data, aes(x = parent, y=parent_sleep, group = child)) +\n  stat_summary(fun.data = \"mean_se\", geom = \"pointrange\", aes(shape=child)) +\n  stat_summary(fun.y = \"mean\", geom = \"line\", aes(linetype=child)) +\n  xlab(\"Parent\") +\n  ylab(\"Average hours sleep\") + \n  theme_cowplot()\n\n\n\n\ncompared to with them dodged:\n\nggplot(parent_sleep_data, aes(x = parent, y=parent_sleep, group = child)) +\n  stat_summary(fun.data = \"mean_se\", geom = \"pointrange\", aes(shape=child), position = position_dodge(.1)) +\n  stat_summary(fun.y = \"mean\", geom = \"line\", aes(linetype=child), position = position_dodge(.1)) +\n  xlab(\"Parent\") +\n  ylab(\"Average hours sleep\") + \n  theme_cowplot()"
  },
  {
    "objectID": "week03/3_2-more_adv_ggplot.html#a-note-on-color-and-apa-formatting",
    "href": "week03/3_2-more_adv_ggplot.html#a-note-on-color-and-apa-formatting",
    "title": "\n13  More advanced ggPlots\n",
    "section": "\n13.5 A note on color and APA formatting",
    "text": "13.5 A note on color and APA formatting\nIf you are concerned about APA formatting, I find the theme_cowplot() from the cowplot package will get you almost all of the way there depending on how complex your plot is. For example, this plot is pretty ready to go:\n\nggplot(parent_sleep_data, aes(x = parent, y=parent_sleep, group = child)) +\n  stat_summary(fun.data = \"mean_se\", geom = \"pointrange\", aes(shape=child)) +\n  stat_summary(fun.y = \"mean\", geom = \"line\", aes(linetype=child)) +\n  xlab(\"Parent\") +\n  ylab(\"Average hours sleep\") + \n  theme_cowplot()"
  },
  {
    "objectID": "week03/3_2-more_adv_ggplot.html#the-legend",
    "href": "week03/3_2-more_adv_ggplot.html#the-legend",
    "title": "\n13  More advanced ggPlots\n",
    "section": "\n13.6 The Legend",
    "text": "13.6 The Legend\n\n13.6.1 Positioning\nA few issues remain. APA asks for the legend to be within the plot itself (not off to the side). This can be accplished by adjusting the legend. With the legend.position call, imagine your plot has a coordinate system ranging from 0 to 1 along both the x and y axes. So for example, to place the legend in a position at 50% of the x-axis and 75% of the y-axis:\n\nggplot(parent_sleep_data, aes(x = parent, y=parent_sleep, group = child)) +\n  stat_summary(fun.data = \"mean_se\", geom = \"pointrange\", aes(shape=child)) +\n  stat_summary(fun.y = \"mean\", geom = \"line\", aes(linetype=child)) +\n  xlab(\"Parent\") +\n  ylab(\"Average hours sleep\") + \n  theme_cowplot() + \n  theme(legend.position = c(.5, .75))\n\n\n\n\nPlay around with this to get a feel for how position changes.\n\n13.6.2 Title-ing\nTo change the header in the legend to a capital “Child”, I need to have an appreciation for what is changing in the lengend. In this case child is designated by both shape and linetype. I then use the labs() call\n\nggplot(parent_sleep_data, aes(x = parent, y=parent_sleep, group = child)) +\n  stat_summary(fun.data = \"mean_se\", geom = \"pointrange\", aes(shape=child), position = position_dodge(.1)) +\n  stat_summary(fun.y = \"mean\", geom = \"line\", aes(linetype=child), position = position_dodge(.1)) +\n  xlab(\"Parent\") +\n  ylab(\"Average hours sleep\") + \n  theme_cowplot() + \n  theme(legend.position = c(.5, .75)) +\n  labs(shape = \"Child\", linetype=\"Child\")\n\n\n\n\nThis is typically good enough for the legend, but I invite you to go this page on Datanova if you really want to get into customizing the legend, including reversing the order of items, and presenting the legend in wide format.\n\n13.6.3 Color\nA convention of print publication is to present your figures in black-white-greyscale (cheaper ink!). Since most people read articles by way of pdf you might find this a bit antiquated.\nMy Pro-tip:  - For simple plots like these, use APA format (black-white-grey) if your intend to submit for print. For complex plots this may mean you need to differentiate your group data by shape or linetype as in the previous plot.\n\nI’d offer for presentations and posters, use color to make your figures pop! i.e., highlighting important aspectes of the data.\n\nggplot() has a default color scheme that’s pretty terrible. For example, you recreate the previous plot differntiating child (First v. Second) by color, I can add color to first line.\n\nggplot(parent_sleep_data, aes(x = parent, y=parent_sleep, group = child, color=child)) +\n  stat_summary(fun.data = \"mean_se\", geom = \"pointrange\", aes(shape=child), position = position_dodge(.1)) +\n  stat_summary(fun.y = \"mean\", geom = \"line\", aes(linetype=child), position = position_dodge(.1)) +\n  xlab(\"Parent\") +\n  ylab(\"Average hours sleep\") + \n  theme_cowplot() + \n  theme(legend.position = c(.5, .75))\n\n\n\n\nDifferentiaing the same thing, child by shape, linetype, and color is a bit redundent. So there’s that. At the same time those default color choices are awful! Fortunately I can change this using scale_color_manual\n\n\n\n\nggplot(parent_sleep_data, aes(x = parent, y=parent_sleep, group = child, color=child)) +\n  stat_summary(fun.data = \"mean_se\", geom = \"pointrange\", position = position_dodge(.1)) +\n  stat_summary(fun.y = \"mean\", geom = \"line\", position = position_dodge(.1)) +\n  xlab(\"Parent\") +\n  ylab(\"Average hours sleep\") + \n  theme_cowplot() + \n  theme(legend.position = c(.5, .75)) +\n  scale_color_manual(values = c(\"black\", \"red\"))"
  },
  {
    "objectID": "week03/3_2-more_adv_ggplot.html#advanced-stuff",
    "href": "week03/3_2-more_adv_ggplot.html#advanced-stuff",
    "title": "\n13  More advanced ggPlots\n",
    "section": "\n13.7 Advanced stuff",
    "text": "13.7 Advanced stuff\nFor other cool stuff that can be done with cowplot() including placing multiple plots side by side, check out the signette links below written by its author, Claus O. Wilke:\n\nIntroduction to cowplot\nChanging the axis positions\nPlot annotations\nArranging plots in a grid\nShared legends"
  },
  {
    "objectID": "week02/2_4-means_other_models.html#basic-form-of-statistical-models-and-error",
    "href": "week02/2_4-means_other_models.html#basic-form-of-statistical-models-and-error",
    "title": "\n14  Means and other models\n",
    "section": "\n14.1 Basic form of statistical models and error",
    "text": "14.1 Basic form of statistical models and error\nStatistical models take the basic form:\n\\[model = data + error\\]\nWe’ll see the mathematical notation later, but for now this will do. The main thing to consider is that all models contain a certain amount of error. In fact, as we’ll discuss later on, a certain amount of error is good for us… models that perfectly fit the data (i.e., error = 0 in the equation above) don’t generalize well when we try to take the model derived from one set of data and apply it to other situations.\nBut error is something we seek to minimize (though not eliminate) in our statistical models. As it turns out, as a first pass, including the mean as a parameter, or predictor, in our model does a reasonable job. That’s why you may have heard me remark that if you know that your data is normally distributed, the mean is the best guess for any individual score (assuming that you are going to take a large number of guesses, choosing the mean is your best bet).\nHere we will use R expand on this notion. We’ll also use this to get some more practice with variable assignment and working with vectors.\nLet’s take a look at some age data (combined classes from previous years)…\n\nnames <- c(\"Tehran\", \"Carlos\", \"Jaquana\", \"Frenchy\", \"Taraneh\",\"Corinne\", \"Cherish\", \"Sabrina\", \"Stephanie\", \"Taylor\", \"Sandra\", \"Emmanuel\", \"Jamie\", \"Heather\", \"Christine\", \"Dylanne\", \"Emily\", \"Tarcisio\", \"nabiha\", \"james\", \"chris\", \"margaret\", \"emily\", \"sarah\", \"nate\", \"tyra\", \"julia\", \"angela\", \"allie\", \"daniel\", \"sierra\")\nages <- c(41, 31, 31, 21, 35, 31, 22, 22, 22, 24, 25, 31, 23, 22, 22, 23, 22, 37, 28,24,22,27,22,24,26,22,24,28,24,29,24)\n\nWe can take these two vectors and turn them into a data frame, using tibble() from tidyr (tidyverse).\n\nlibrary(tidyverse)\ncohorts <- tibble(\"person\" = names,\n                    \"age\" = ages)\ncohorts\n\n# A tibble: 31 × 2\n   person      age\n   <chr>     <dbl>\n 1 Tehran       41\n 2 Carlos       31\n 3 Jaquana      31\n 4 Frenchy      21\n 5 Taraneh      35\n 6 Corinne      31\n 7 Cherish      22\n 8 Sabrina      22\n 9 Stephanie    22\n10 Taylor       24\n# ℹ 21 more rows\n\n\nEveryone’s age (numerical) is assigned to an object (their name). From here, we can use psych::describe() to get info about the class distribution of ages:\n\npsych::describe(cohorts)\n\n        vars  n mean   sd median trimmed   mad min max range skew kurtosis   se\nperson*    1 31 16.0 9.09     16   16.00 11.86   1  31    30 0.00    -1.32 1.63\nage        2 31 26.1 5.02     24   25.24  2.97  21  41    20 1.26     0.84 0.90"
  },
  {
    "objectID": "week02/2_4-means_other_models.html#wait-isnt-the-mode-the-highest-probability",
    "href": "week02/2_4-means_other_models.html#wait-isnt-the-mode-the-highest-probability",
    "title": "\n14  Means and other models\n",
    "section": "\n14.2 wait, isn’t the mode the highest probability?",
    "text": "14.2 wait, isn’t the mode the highest probability?\npsych::describe() give us our mean and median, but what about the mode? Didn’t we say the mode represents the greatest number of observations in the distribution? Whats the function for that?\nAs it turns out there is no pre-installed function for the mode. There is a mode() function, but ? mode tells us that it does something entirely different— it tells us what kind of data is in the assigned object. A quick Google search for function for mode in R reveals a number of options. We can either build our own function, or install one of several packages. Let’s do the latter, and install DescTools:\n\npacman::p_load(DescTools) # load the package\nDescTools::Mode(cohorts$age) # get the mode\n\n[1] 22\nattr(,\"freq\")\n[1] 9\n\n\nSo now that we have these three values related to our distribution lets do some work!\nFirst let’s plot our cohorts age data to a density histogram.\n\n# creating a histogram:\n# note I did this all in one step:\n\nclassAgesPlot <- ggplot2::ggplot(data = cohorts, aes(x=age)) + \n  geom_histogram(binwidth = 1, \n                 color = \"black\",\n                 fill = \"white\",\n                 aes(y=..density..)) +\n  stat_function(fun = dnorm, # generate theoretical norm data\n                color = \"red\", # color the line red\n                args=list(mean = mean(cohorts$age), # build around mean \n                sd = sd(cohorts$age)))\nclassAgesPlot\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\nWe can add vertical lines to the previous plot, indicating the mean, median, and mode values (using geom_vline():\n\nclassAgesPlot + \n  # mean is a red line\n  geom_vline(xintercept=mean(cohorts$age),\n             color=\"red\",\n             linetype=\"dotted\",\n             size = 1.5) +\n  # median is a blue line\n  geom_vline(xintercept=median(cohorts$age),\n             color=\"blue\",\n             linetype=\"solid\",\n             size = 1) +\n  # mode is an orange line\n  geom_vline(xintercept=DescTools::Mode(cohorts$age),\n             color=\"orange\",\n             linetype=\"dashed\",\n             size = .5)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nSo the mode has the highest probability. But that doesn’t mean that it is the best starting point to describe this data.\nHow far would you be off on a person by person basis if you selected the mode?:\n\nclass_ages_mode <- DescTools::Mode(cohorts$age)\ncohorts$age - class_ages_mode\n\n [1] 19  9  9 -1 13  9  0  0  0  2  3  9  1  0  0  1  0 15  6  2  0  5  0  2  4\n[26]  0  2  6  2  7  2\n\n\nThe sum of this error:\n\nsum(cohorts$age - class_ages_mode)\n\n[1] 127\n\n\nOkay, now assume you chose the mean:\n\nclass_ages_mean <- mean(cohorts$age)\ncohorts$age - class_ages_mean\n\n [1] 14.90322581  4.90322581  4.90322581 -5.09677419  8.90322581  4.90322581\n [7] -4.09677419 -4.09677419 -4.09677419 -2.09677419 -1.09677419  4.90322581\n[13] -3.09677419 -4.09677419 -4.09677419 -3.09677419 -4.09677419 10.90322581\n[19]  1.90322581 -2.09677419 -4.09677419  0.90322581 -4.09677419 -2.09677419\n[25] -0.09677419 -4.09677419 -2.09677419  1.90322581 -2.09677419  2.90322581\n[31] -2.09677419\n\nsum(cohorts$age - class_ages_mean)\n\n[1] -2.4869e-14\n\n\nThe summed error of the mean is much less than then mode. In fact it’s going to be less than any other value—its going to be zero!"
  },
  {
    "objectID": "week02/2_4-means_other_models.html#mean-as-fulcrum-point",
    "href": "week02/2_4-means_other_models.html#mean-as-fulcrum-point",
    "title": "\n14  Means and other models\n",
    "section": "\n14.3 mean as fulcrum point",
    "text": "14.3 mean as fulcrum point\nThe next two sections involve a little bit of complex coding that will be rehearsed in future workshops. For now I have two goals, to:\n\ndemonstrate the mean produces the least amount of summed error and\nby virtue of that minimized the sum-of-squared differences, or SS.\n\nThe latter is arguably more important as SS is used to calculate variability and is most directly used in assessing inferential models.\nThis week we also described the mean as the balance point for our data—that the sum of all deviation scores from the mean = 0: \\(\\sum{X-\\bar{X}} = 0\\). That is, the mean values produces the least amount of summed error, 0.\nImagine that you are encountering someone in class and are asked to guess their age (and WIN! WIN! WIN!). The only info that you have is the range of our ages:\n\nrange(cohorts$age)\n\n[1] 21 41\n\n\nIn what follows, I’m going to iterate through each possible age (guess) within our range and get the sum of difference scores, or sumDiff. sumDiff is obtained by subtracting each age from a given value. Typically this value is the mean. For example say I guessed 27:\n\n# 1. guessed age:\nguess <- 27\n# 2. resulting difference scores, squared\ndiffScores <- (ages-guess)\n# 3. the sum of squares:\nsumDiff <- sum(diffScores)\n\n# 4. show the sumDiff (example)\nshow(sumDiff)\n\n[1] -28\n\n\nNow, I’m going to iterate through every possible age in the range, and save the resulting sumDiffs to a vector, sumDiffguess. It might be good for you to check to output on a line-by-line basis here.\n\n# create a vector of all possible integers within our range:\nrangeAges <- min(cohorts$age):max(cohorts$age)\n\n# create an empty vector to save our resulting sum of differences\nsumDiffguess <- vector()\nguessedAge <- vector()\n\n# here \"i\" indexes which value from the range to pull, for example when i=1, we are pulling the first number in the sequence\nfor(i in 1:length(rangeAges)){\n  guess <- rangeAges[i]\n  diffScores <- (ages-guess)\n  sumDiff <- sum(diffScores)\n  \n  # save results to sumDiffguess\n  guessedAge[i] <- guess\n  sumDiffguess[i] <- sumDiff\n}\n\ndiffAges_df <- tibble(guessedAge,sumDiffguess) #combine the two vectors to single data frame\ndiffAges_df\n\n# A tibble: 21 × 2\n   guessedAge sumDiffguess\n        <int>        <dbl>\n 1         21          158\n 2         22          127\n 3         23           96\n 4         24           65\n 5         25           34\n 6         26            3\n 7         27          -28\n 8         28          -59\n 9         29          -90\n10         30         -121\n# ℹ 11 more rows\n\n\nNow let’s plot the resulting sum of difference scores as a function of guessed age. The vertical line represents mean age. As you can see this is where our function crosses 0 on the y-axis (thick red line).\n\n# note I did this all in one step:\n\nsumDiffPlot <- ggplot2::ggplot(data = diffAges_df, \n                               aes(x=guessedAge, \n                                   y=sumDiffguess)) + \n  geom_point() + \n  geom_line() +\n  geom_vline(xintercept=mean(cohorts$age), # using actual ages from \n             color=\"red\",\n             linetype=\"dashed\") +\n  geom_hline(yintercept=0, color = \"red\", size=2)+\n  theme_minimal()\n\n\nsumDiffPlot"
  },
  {
    "objectID": "week02/2_4-means_other_models.html#mean-as-minimizing-random-error",
    "href": "week02/2_4-means_other_models.html#mean-as-minimizing-random-error",
    "title": "\n14  Means and other models\n",
    "section": "\n14.4 mean as minimizing random error",
    "text": "14.4 mean as minimizing random error\nOf course, in our statistics the sum of squared-differences (or SS) \\(\\sum{(X-\\bar{X})^2}\\) is our primary measure rather than the sum of difference in the last example. SS is used to calculate variance \\(\\frac{\\sum{(X-\\bar{X})^2}}{N-1}\\) and in turn standard deviation, which is just the square root of the variance. Both measures we use to describe variability in a distribution of data. When we know nothing about our data we assume that the scores, and therefore the resulting difference or error from the mean is randomly distributed. Hence the term random error.\nFirst, let’s calculate the resulting SS for each possible guess (similar to above):\n\n# create a vector of all possible integers within our range:\nrangeAges <- min(cohorts$age):max(cohorts$age)\n\n# create an empty vector to save our resulting sum of differences\nSSguess <- vector()\nguessedAge <- vector()\n\n# here \"i\" indexes which value from the range to pull, for example when i=1, we are pulling the first number in the sequence\nfor(i in 1:length(rangeAges)){\n  guess <- rangeAges[i]\n  diffScores <- (ages-guess)\n  SSDiff <- sum(diffScores^2)\n  # save SS to SSguess\n  guessedAge[i] <- guess\n  SSguess[i] <- SSDiff\n}\n\nSS_Ages_df <- tibble(guessedAge,SSguess) #combine the two vectors to single data frame\nshow(SS_Ages_df)\n\n# A tibble: 21 × 2\n   guessedAge SSguess\n        <int>   <dbl>\n 1         21    1562\n 2         22    1277\n 3         23    1054\n 4         24     893\n 5         25     794\n 6         26     757\n 7         27     782\n 8         28     869\n 9         29    1018\n10         30    1229\n# ℹ 11 more rows\n\n\nNow, let’s see what happens when we plot the resulting SS as a function of guessed age:\n\nSS_Plot <- ggplot2::ggplot(data = SS_Ages_df, \n                           aes(x=guessedAge, \n                               y=SSguess)) + \n  geom_point() + \n  geom_smooth() +\n  geom_vline(xintercept=mean(cohorts$age), # using actual ages from \n             color=\"red\",\n             linetype=\"dashed\") +\n  theme_minimal()\n\n\nSS_Plot\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nAs you can see we get parabola with a minima at our mean."
  },
  {
    "objectID": "week02/2_4-means_other_models.html#but-we-can-do-better-than-that",
    "href": "week02/2_4-means_other_models.html#but-we-can-do-better-than-that",
    "title": "\n14  Means and other models\n",
    "section": "\n14.5 but we can do better than that",
    "text": "14.5 but we can do better than that\nThe mean is good as a stab in the dark, but as you can see our SS is still high.\n\n# minimum value of SSguess is the SS about the mean\nmin(SS_Ages_df$SSguess)\n\n[1] 757\n\n\nIf we have other useful information about the nature of our age data, then maybe we can use that to make better predictions. These extra bits of info are our predictors. For example, let’s combine our age data with info on how many years everyone has been out of undergraduate. For the purposes of this example, I’m just going to make this data up (sort one, some of this may be true).\nLet’s create a vector of years since UG:\n\nyearsUG <- c(17,8,7,1,15,12,1,2,3,2,5,10,4,2,6,4,3,15,5,2,1,3,1,4,2,1,3,4,4,5,3)\n\nLets combine yearsUG to our cohorts data frame:\n\ncohorts <- cohorts %>% mutate(\"yearsUG\" = yearsUG)\n\nand create a scatterplot (geom_point) of age as a function of yearsUG:\n\nggplot2::ggplot(cohorts, aes(x=yearsUG,y=ages)) + geom_point()\n\n\n\n\nKeeping in mind that our focus this semester will be in linear models. As a simplification, this means that we are looking for an equation of the line that best fits our data (minimizes the resulting SS). The equation for the line that represents the mean would be:\n\\[\\hat{Y} = mean\\ age + 0 * yearsUG\\]\nThis means years since undergrad does not figure into the mean model (it gets multiplied by zero. This results in a horizontal line with mean age as the y-intercept. We would define this model as “predicting age (\\(\\hat{Y}\\)) as a function of the mean age of the distribution.\nThis is still a couple weeks out, but in R we would run this model, aka the means_model, as:\n\nmeans_model <- lm(formula = ages~1,data = cohorts)\n\nand we can add the regression line (means_fitted) for this model to our the previous plot. Note that the dotted vertical lines (in red) represent the residuals—the differences between what the model predicts and actual data:\n\n# add the predicteds and residuals from the means model to our cohorts dataframe:\ncohorts <- cohorts %>% mutate(means_fitted = means_model$fitted.values,\n                                means_resids = means_model$residuals)\n\n\nggplot2::ggplot(cohorts, aes(x=ages,y=ages)) + \n  geom_point() + # add actual data points\n  geom_line(aes(y = means_fitted), color = \"blue\") + # Add the predicted values\n  geom_segment(aes(xend = ages, yend = means_fitted), color=\"red\", linetype=\"dotted\")\n\n\n\n\nThe squared sum of the length of those dotted lines is your…. wait for it Sum of Squares!!!.\nNow lets compare this to a model predicting age as a function of years since undergrad yearsUG. In this case our model would be:\n\nyears_model <- lm(formula = ages~yearsUG, data = cohorts)\n\nAnd we can add the regression line and residuals like above:\n\n# add the predicteds and residuals from the years model to our df:\ncohorts <- cohorts %>% mutate(years_fitted = years_model$fitted.values,\n                                years_resids = years_model$residuals)\n\nggplot2::ggplot(cohorts, aes(x=yearsUG,y=ages)) + \n  geom_point() + # add actual data points\n  geom_line(aes(y = years_fitted), color = \"blue\") + # Add the predicted values\n  geom_segment(aes(xend = yearsUG, yend = years_fitted), color=\"red\", linetype=\"dotted\")\n\n\n\n\nThose residuals have shrunk quite a bit. Indeed comparing the two:\n\ntibble(\"means_model_SS\"=cohorts$means_resids^2 %>% sum(), #sum of squares\n       \"years_model_SS\"=cohorts$years_resids^2 %>% sum() #sum of squares\n       ) %>% pander()\n\n\n\n\n\n\n\nmeans_model_SS\nyears_model_SS\n\n\n756.7\n115.3\n\n\n\n\nand visually:\n\nmeans_model_plot <- ggplot2::ggplot(cohorts, aes(x=ages,y=ages)) + \n  geom_point() + # add actual data points\n  geom_line(aes(y = means_fitted), color = \"blue\") + # Add the predicted values\n  geom_segment(aes(xend = ages, yend = means_fitted), color=\"red\", linetype=\"dotted\")\n\nyears_model_plot <- ggplot2::ggplot(cohorts, aes(x=yearsUG,y=ages)) + \n  geom_point() + # add actual data points\n  geom_line(aes(y = years_fitted), color = \"blue\") + # Add the predicted values\n  geom_segment(aes(xend = yearsUG, yend = years_fitted), color=\"red\", linetype=\"dotted\")\n\ncowplot::plot_grid(means_model_plot,years_model_plot, labels = c(\"mean only\", \"mean+yearsUG\"))\n\n\n\nMeans model v means + years UG\n\n\n\n\nAs you may have intuited the lm in the above call stands for “linear model”. Congrats, you just plotted a linear regression!!!!\nWhat’s important here is that from a visual inspection the yearsUG model provides a better fit. Why, because it’s reduced the resulting difference scores (of SS). The sum of the squared distances from each point to the regression line with yearsUG as a predictor is less than the regression model with the mean as the only predictor. This is what we mean by “accounting for variance”. The yearsUG model accounts for more variance than the means model—why because it reduces the SS.\nThis sort of comparison (between a means_model and a means+predictors model is the backbone of many of the tests you will perform this semester. Indeed in a few weeks when we say that the “regression is significant” or that the “F-ratio for the ANOVA is significant” this is essentially the comparison that we are making!!!\nOf course, a critical question that we strive to answer with our statistical tests (like the F-test) is whether the model using additional predictors accounts for more variance above a critical threshold that it may be deemed significant, or unlikely due to chance. This is something that will become critically important in a few weeks. For now, if you can wrap your head around this walkthrough, then you’ve got an intuitive understanding of almost every test we will run this semester."
  },
  {
    "objectID": "week04/4_1-the_normal_distribution.html",
    "href": "week04/4_1-the_normal_distribution.html",
    "title": "\n15  The normal distribution\n",
    "section": "",
    "text": "16 Normality Assumption\nIn class we discussed that one of the first and important steps when taking a look at your data is to assess whether the data conform to the assumptions of your analysis of choice. For most parametric tests (e.g., regression, t-tests, ANOVA families), one of the key assumptions is that scores of your dependent variable(s) approximate some known distribution. The distribution that gets the most play in this course is the normal distribution. That said, real world data, especially from the social sciences, is rarely perfectly normal, but rather “normal enough” to proceed with parametric tests. With this in mind, there are other distributions that are important to know about, including the binomial, Poisson, and exponential distributions that may more readly approximate your data. We’ll discuss these in class when the time comes. For now, we’ll focus on the normal distribution.\nIn the interest of having a unified resource, I’m putting together a brief vignette highlighting methods for assessing whether your data is normal or not. I stress the word “assess” and not “test” for reasons that will be made apparent below. My best advice is to take several tools to make the decision of whether your data is normal or not.\nIn brief there are three “tools” for assessment:\nIt may be temping to believe that #3, significance testing, would be a magic bullet. However, many (including Field, Section 5.6, we need to be cautious with tests like Kolmorogorov and Shapiro-Wilks. In my own practice, I typically use them last in the case of extreme ambiguity, but more often to confirm what I already believe (journals like “tests to confirm”).\nBefore reaching your conclusion you should employ multiple methods!!"
  },
  {
    "objectID": "week04/4_1-the_normal_distribution.html#structure-of-this-vignette",
    "href": "week04/4_1-the_normal_distribution.html#structure-of-this-vignette",
    "title": "\n15  The normal distribution\n",
    "section": "\n16.1 Structure of this vignette",
    "text": "16.1 Structure of this vignette\nIn the first half of this vignette I’ll discuss how to apply each of these tests in their most general form. In the interest of time, in cases where these methods are discussed in your textbooks I will point there. When warranted, I’ll offer additional exposition.\nIn the second half of this vignette I’ll outline use cases for several of the scenarios that we will encounter this semester, offering tips and code templates for how to address each scenario. This will be populated in upcoming weeks\nBefore we start, let’s generate a normal_sample of 60 observations that we know will be approximately normal and a positively skewed distribution skewed_sample:\n\nset.seed(920)\n# create a normal distribution mean 25, sd = 5\nnormal_sample <- tibble(\"observation\" = 1:100,\n                        \"score\" = rnorm(n = 100, mean = 25, sd = 5))\n\n# create a skewed distribution using SimDesign package\npacman::p_load(\"SimDesign\")\n\nset.seed(920)\n  \nskewed_sample <- \n  tibble(\"observation\" = 1:100,\n          \"score\" = SimDesign::rValeMaurelli(100, mean=25, sigma=5, skew=1.8,\n                                             kurt=4) %>% as.vector()\n         )"
  },
  {
    "objectID": "week04/4_1-the_normal_distribution.html#part-1-visual-assessment-tools",
    "href": "week04/4_1-the_normal_distribution.html#part-1-visual-assessment-tools",
    "title": "\n15  The normal distribution\n",
    "section": "\n16.2 Part 1: Visual assessment tools",
    "text": "16.2 Part 1: Visual assessment tools\nI can’t say it enough, one of the first things to do is plot your data. On of the first plots you should generate is a plot of the distribution of your variables as they fit within your design (more on this in Part 2). You friends here are the histogram plot and the QQ-plot.\n\n16.2.1 Histogram plot\nA histogram plot is simply a plot of the distribution of score. Typically this takes the form of the number, or raw count) of scores within a number of preset bin (sizes), but can also take the form of the density, or proportion of scores. You’ll need to create a density plot to overlay with a normal curve (a least using ggplot). I’ll revisit the basics here, but the last 2 plot examples here are most useful to you.\n\n16.2.1.1 What to look for\nThe degree to which your data are (1) asymmetrical around the mean and median and (2) fits with a theoretical normal curve with the same mean and sd parameters.\n\n16.2.1.2 Basic histogram (raw counts)\nUsing ggplot we’ll plot a histogram of the counts of scores, placing vertical lines at the mean and median scores:\n\nggplot(normal_sample, aes(x = score)) + \n  geom_histogram(bins = 10,\n                 fill = \"white\",\n                 color = \"black\") + \n  geom_vline(xintercept = mean(normal_sample$score), color=\"red\") + \n  geom_vline(xintercept = median(normal_sample$score), color=\"blue\") +\n  theme_minimal()\n\n\n\n\nA good rule of thumb is to overlay the plot with a normal curve (theoretical distribution). The easiest way to do this is to transform your count histogram into a proportion, or density, histogram first:\n\n16.2.1.3 Density histogram (proportions)\nTo get a histogram with proportions instead of raw values, you simply add aes(y=..density..) to geom_histogram()\n\nggplot(normal_sample, aes(x = score)) + \n  geom_histogram(bins = 10,\n                 fill = \"white\",\n                 color = \"black\",\n                 aes(y=..density..) # turns raw scores to proportions\n                 ) + \n  geom_vline(xintercept = mean(normal_sample$score), color=\"red\") + \n  geom_vline(xintercept = median(normal_sample$score), color=\"blue\") +\n  theme_minimal()\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n16.2.1.4 Histogram with normal curve overlay\nTo overlay a normal curve we need to add stat_function to our ggplot. Note that the arguments are essentially running a dnorm with a unit mean and sd from our scores. If we are simply adding the normal curve to a density plot we add the following:\n\n# create a density plot\nggplot(normal_sample, aes(x = score)) + \n  geom_histogram(bins = 10,\n                 fill = \"white\",\n                 color = \"black\",\n                 aes(y=..density..) # turns raw scores to proportions\n                 ) + \n  geom_vline(xintercept = mean(normal_sample$score), color=\"red\") + \n  geom_vline(xintercept = median(normal_sample$score), color=\"blue\") +\n  stat_function(fun = dnorm, # generate theoretical norm data\n                  color = \"red\", # color the line red\n                  args=list(\n                    mean = mean(normal_sample$score,na.rm = T), # build around mean\n                    sd = sd(normal_sample$score,na.rm = T) # st dev parameter\n                    )\n                  ) +\n  theme_minimal()\n\n\n\n\n\n16.2.1.5 Density curve with with normal curve overlay\nNote that it may actually be simpler to simply create a density curve of the data and overlay it with a normal curve. The resulting plot is a smooth-function interpretation of the distribution of the data:\n\n# create a raw plot\nggplot(normal_sample, aes(x = score)) + \n  geom_density() + \n    stat_function(fun = dnorm, # generate theoretical norm data\n                  color = \"red\", # color the line red\n                  args=list(\n                    mean = mean(normal_sample$score,na.rm = T), # build around mean\n                    sd = sd(normal_sample$score,na.rm = T) # st dev parameter\n                    )\n                  ) +\n  geom_vline(xintercept = mean(normal_sample$score), color=\"red\") + \n  geom_vline(xintercept = median(normal_sample$score), color=\"blue\") +\n  labs(\n    title = \"Density curve plot\",\n    caption = \"black = real data; red = hypothetical normal curve\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n16.2.1.6 Non-normal example\nI’m going to recreate the density histogram using the skewed_sample to give you a feel for what this looks like.\nRecall that we generated the skewed sample using:\n\nskewed_sample <- \n  tibble(\"observation\" = 1:100,\n          \"score\" = SimDesign::rValeMaurelli(100, mean=25, sigma=5, skew=1.5,\n                                             kurt=3) %>% as.vector()\n         )\n\nVisually:\n\n# create a raw plot\nggplot(skewed_sample, aes(x = score)) + \n  geom_density() + \n    stat_function(fun = dnorm, # generate theoretical norm data\n                  color = \"red\", # color the line red\n                  args=list(\n                    mean = mean(skewed_sample$score,na.rm = T), # build around mean\n                    sd = sd(skewed_sample$score,na.rm = T) # st dev parameter\n                    )\n                  ) +\n  geom_vline(xintercept = mean(skewed_sample$score), color=\"red\") + \n  geom_vline(xintercept = median(skewed_sample$score), color=\"blue\") +\n  labs(\n    title = \"Density curve plot\",\n    caption = \"black = real data; red = hypothetical normal curve\"\n  ) +\n  theme_minimal()\n\n\n\n\nAgain, notice how the data are both shifted and do not cleanly fit with the normal curve.\n\n16.2.2 Quantile-quantile plots\nNormality can also be visually assessed using a quantile-quantile plot. A quantile-quantile plot compares raw scores with theoretical scores that assume a normal distribution. If both are roughly equal then the result should be that the points fall along a line of equality (qq-line). In a quantile-quantile plot deviations from the line indicate deviations from normality.\n\n16.2.2.1 What to look for\nA large number of significant deviations (outside the bands) is a hint that the data is non-normal. This link on CrossValidated is pretty good in terms of interpreting Q-Q plots. Note that for more stringent criteria one can take a look at the correlation between the raw sample data and the theoretical normed data.\n\n16.2.2.2 Basic qqplot\nThere are several ways to put together a QQ-plot; my preferred are (1) using the car::qqPlot function when just taking a quick look; (2) using ggplot when I need something “publication worthy” or when I’m conducting more complex analysis. A third method that I’ll used here regclass::qq is useful as a learning example.\nFirst, lets take a look at normal_sample$score using car::qqPlot:\n\nlibrary(car)\ncar::qqPlot(normal_sample$score, distribution=\"norm\")\n\n\n\n\n[1] 92 12\n\n\nThis produces a QQ-plot with a confidence envelope based on the SEs of the order statistics of an independent random sample from the comparison distribution (see Fox 2016, Applied Regression Analysis and Generalized Linear Models, Chapter 12). The highlighted observations, in this case 12 and 92 are the 2 scores with the largest model residuals—in this case deviation from the mean, but we can through linear model outputs into this function (see Week 6 of class: Modeling with continuous predictors / simple regression)\nThe x-values are the normed quantiles, while the y-values are our actual raw scores. The normed quantiles values are essentially normed stand-ins for “what the raw values should be if the data was normal. To demonstrate this let’s use the regclass:qq function:\n\nlibrary(regclass)\nregclass::qq(normal_sample$score)\n\n\n\n\nAs you can see this is the exact same plot. We typically focus on the quantile values as they allow us to better compare data across different distributions.\nTaking a look both our normal_sample and skewed_sample.\nFinally we can use ggplot to construct better looking plots.\n\nggplot(normal_sample, aes(sample=score)) + \n  geom_qq() + geom_qq_line() +\n  theme_minimal()\n\n\n\n\n\nggplot(skewed_sample, aes(sample=score)) + \n  geom_qq() + geom_qq_line() +\n  theme_minimal()\n\n\n\n\n\n16.2.2.3 Advanced qqplot with 95% CI bands\nWe can also create bands around these plots using the qqplotr library in conjuction with ggplot:\n\npacman::p_load(qqplotr)\n\nggplot(normal_sample, aes(sample=score)) + \n  stat_qq_band(fill=\"lightgray\") + stat_qq_point() + stat_qq_line() +\n  theme_minimal()\n\n\n\nggplot(skewed_sample, aes(sample=score)) + \n  stat_qq_band(fill=\"lightgray\") + stat_qq_point() + stat_qq_line() +\n  theme_minimal()\n\n\n\n\n\n16.2.2.4 Advanced qq-plot & correlation\n** note that some of what you see here may not be covered until our Week 6 discussion on correlation.\nI usually stop with visual inspection, BUT if you want to be more rigorous, you can take the data from the qq_plot can measure the correlation, \\(r\\) between the actual sample distribution and the theoretical distribution that make up the qqplot. Keep in mind that under a perfectly ideal situation the sample distribution should equal the theoretical distribution resulting in \\(r = 1\\) (and all points falling along the qq_line). Thus, deviations away from 1 tell us how “off” this fit is, where lower \\(r\\) values indicate poorer fits between sample and theoretical.\nFirst, let’s recreate the plot:\n\nnorm_qq_qplot <- ggplot(normal_sample, aes(sample=score)) + \n  stat_qq_band(fill=\"lightgray\") + stat_qq_point() + stat_qq_line() +\n  theme_minimal()\n\nThe data used to create the plot can be extracted from our ggplot using ggplot_build.\n\n# run this, this prints out a lot in the console!!!\n# you may need to scroll up\nggplot_build(norm_qq_qplot)\n\nAs you can see ggplot_build pulls several list items related to the construction of the plot. From this list we are most concerned with $data[[2]] which contains the sample and theoretical values uses to populate the x and y axes of the plot. From here we can save the data from ggplot_build to an object and get the correlation between sample and theoretical. You’ll note I didn’t use cor.test() as I’m not interested in how far away from 0 I am (the null hypothesis of cor.test()). Take from that what you will.\n\nqq_plot_data <- ggplot_build(norm_qq_qplot)$data[[2]]\ncor_qq_Normal <- cor(qq_plot_data$sample,qq_plot_data$theoretical)\ncor_qq_Normal\n\n[1] 0.9922899\n\n\nIf we wanted to be extra judicious we could then compare this value against a correlation of 1:\n\ncor_norm <- cor(qq_plot_data$sample,qq_plot_data$theoretical)\nDescTools::FisherZ(1-cor_norm) %>% pnorm()\n\n[1] 0.5030759\n\n\nCompare this value to the skewed_sample:\n\nskew_qq_qplot <- ggplot(skewed_sample, aes(sample=score)) + \n  stat_qq_band(fill=\"lightgray\") + stat_qq_point() + stat_qq_line() +\n  theme_minimal()\n\nqq_plot_data <- ggplot_build(skew_qq_qplot)$data[[2]]\ncor(qq_plot_data$sample,qq_plot_data$theoretical)\n\n[1] 0.9453017\n\ncor_skew <- cor(qq_plot_data$sample,qq_plot_data$theoretical)\nDescTools::FisherZ(1-cor_skew) %>% pnorm()\n\n[1] 0.5218324"
  },
  {
    "objectID": "week04/4_1-the_normal_distribution.html#measures-of-skew-and-kurtosis",
    "href": "week04/4_1-the_normal_distribution.html#measures-of-skew-and-kurtosis",
    "title": "\n15  The normal distribution\n",
    "section": "\n16.3 Measures of skew and kurtosis",
    "text": "16.3 Measures of skew and kurtosis\nMeasures of skew and kurtosis may be obtained using the psych::describe() function. There is some debate concerning the interpretation of these values. First let’s obtain skew and kurtosis using the psych::describe() function. From here well walk-though the specifics of what these values are and how to interpret them (and limitations). I’ll finally offer a recommendation I’ll point you in the direction of several resources in this debate as well as summarize key issues as I see them.\nFirst normal_sample:\n\npacman::p_load(psych)\npsych::describe(normal_sample$score, type=2)\n\n   vars   n  mean   sd median trimmed  mad   min   max range skew kurtosis   se\nX1    1 100 25.04 5.22  24.77   24.84 5.15 11.46 36.39 24.93 0.15    -0.35 0.52\n\n\nAlternatively, you can call skew() and kurtosi() seperately:\n\nnormal_sample$score %>% skew()\n\n[1] 0.1451337\n\nnormal_sample$score %>% kurtosi()\n\n[1] -0.4427668\n\n\nNow skewed_sample:\n\npsych::describe(skewed_sample$score, type = 2)\n\n   vars   n mean   sd median trimmed  mad   min  max range skew kurtosis   se\nX1    1 100 24.9 2.11  24.55    24.6 2.21 22.62 31.9  9.28 1.09     0.76 0.21\n\n\n\n16.3.1 using the non-transformed values\nThe values of skew and kurtosis are calculated using the three types outlined by Joanes and Gill (1998) here. By default, for psych::describe(), type=3, which corresponds to \\(b_1\\) and \\(b_2\\) from Joanes and Gill. Note that type=2 corresponds to the values obtained using SPSS. For the purposes of comparison we will use type=2, though note that each (type=2 or type=3) has its advantages, see Joanes and Gill, 1998 for more detail. As relayed by Kim (2013) here, if one wants to assess skew and kurtosis based upon these raw values, then any value > 2 for skew or > 4 for kurtosis should be viewed as suspect.\n\n16.3.2 using the standardized values\nHowever, as Kim notes, one can apply a test of standardized test for normality by dividing the skew and kurtosis by their respective standard errors. Note that SPSS will provide you with this standard error estimate. R by default does not. You may elect to replicate the SPSS method, again based on type=2 calculations, by importing the function found below (taken from Howard Seltman, here). Once you have imported the function spssSkewKurtosis you can simply call spssSkewKurtosis(vector). For example, with skewed_sample:\n\n# Skewness and kurtosis and their standard errors as implemented by SPSS\n#\n# Reference: pp 451-452 of\n# http://support.spss.com/ProductsExt/SPSS/Documentation/Manuals/16.0/SPSS 16.0 Algorithms.pdf\n# \n# See also: Suggestion for Using Powerful and Informative Tests of Normality,\n# Ralph B. D'Agostino, Albert Belanger, Ralph B. D'Agostino, Jr.,\n# The American Statistician, Vol. 44, No. 4 (Nov., 1990), pp. 316-321\n\nspssSkewKurtosis <- function(x) {\n  w=length(x)\n  m1=mean(x)\n  m2=sum((x-m1)^2)\n  m3=sum((x-m1)^3)\n  m4=sum((x-m1)^4)\n  s1=sd(x)\n  skew=w*m3/(w-1)/(w-2)/s1^3\n  sdskew=sqrt( 6*w*(w-1) / ((w-2)*(w+1)*(w+3)) )\n  kurtosis=(w*(w+1)*m4 - 3*m2^2*(w-1)) / ((w-1)*(w-2)*(w-3)*s1^4)\n  sdkurtosis=sqrt( 4*(w^2-1) * sdskew^2 / ((w-3)*(w+5)) )\n  mat=matrix(c(skew,kurtosis, sdskew,sdkurtosis), 2,\n        dimnames=list(c(\"skew\",\"kurtosis\"), c(\"estimate\",\"se\")))\n  return(mat)\n}\n\n# skewed_sample\nspssSkewKurtosis(skewed_sample$score)\n\n          estimate        se\nskew     1.0902510 0.2413798\nkurtosis 0.7607103 0.4783311\n\n\nAlso note that you can source this custom code to your environment whenever you want using:\n\n# I typically source files in the first chunk as well\nsource(\"http://www.stat.cmu.edu/~hseltman/files/spssSkewKurtosis.R\")\n\nFrom here we could divide each estimate by their se and apply the critical values as recommended by Kim:\n\nspssSkewKurtosis(skewed_sample$score)[,1]/\n  spssSkewKurtosis(skewed_sample$score)[,2]\n\n    skew kurtosis \n4.516745 1.590342 \n\n\nAccording to Kim:\n\nif \\(N\\) < 50, any value over 1.96\nif (50 < \\(N\\) < 300), any value over 3.29\nif \\(N\\) > 300, no transform: any value of 2 for skew and 4 for kurtosis.\n\nBoth skew and kurtosis fail the normal test. Compare this to normal_sample:\n\nspssSkewKurtosis(normal_sample$score)[,1]/\n  spssSkewKurtosis(normal_sample$score)[,2]\n\n      skew   kurtosis \n 0.6197350 -0.7288419 \n\n\nBoth absolute values are below 3.29.\n\n16.3.3 using a bootstrapped standard errors (recommended)\nThere is a caveat to the test method from the previous section. The standard errors calculated above work for normal distributions, but not for other distributions. Obviously this is a problem, as by definition we are using the above method, including the standard errors, to test for non-normality. If our distribution is non-normal, then the standard error that we are using to test for non-normality is suspect!! See here for a brief discussion. And a solution, DescTools!!\nThe solution involves bootstrapping the 95% confidence interval and estimating the standard error. DescTools::Skew() and DescTools::Kurt() both provide this option, using ci.type=\"bca\". This method is also recommended by Joanes and Gill. Note that here method refers to the type of skew and kurtosis calculation (again, keeping it 2 for the sake of consistency, although 3 is fine as well).\n\npacman::p_load(DescTools)\n# bootstrap with 1000 replications (R=1000)\nDescTools::Skew(x = skewed_sample$score, method = 2, ci.type = \"bca\", conf.level = 0.95, R = 1000)\n\n     skew    lwr.ci    upr.ci \n1.0902510 0.7315529 1.5456300 \n\n\nfrom here, the standard error may be estimated by taking the range of the bootstrapped confidence interval and dividing it by 3.92\n\nbca_skew <- DescTools::Skew(x = skewed_sample$score, method = 2, ci.type = \"bca\", conf.level = 0.95, R = 1000)\n\nses <- (bca_skew[3]-bca_skew[2])/3.92\n\nses %>% unname() # unname simply removes the name\n\n[1] 0.2064468\n\n\nFrom here we can take the obtained skew and divide it by the standard error, ses.\n\nDescTools::Skew(x = skewed_sample$score, ci.type = \"bca\", conf.level = 0.95, R = 1000)[1] / ses\n\n    skew \n5.123651 \n\n\nSimilar steps can be taken for kurtosis:\n\nbca_kurtosis <- DescTools::Kurt(x = skewed_sample$score, ci.type = \"bca\", conf.level = 0.95, R = 1000)\nses <- (bca_kurtosis[3]-bca_kurtosis[2])/3.92\n\nDescTools::Kurt(x = skewed_sample$score, ci.type = \"bca\", conf.level = 0.95, R = 1000)[1]/ses\n\n    kurt \n0.795541 \n\n\nThe logic of this. Consider that we have bootstrapped a distribution of skews and or kurtosi just as we have done in the past with means. From class we know that the standard error equals the standard deviation of this distribution. The 95% CI obtained from bootstrapping are equivalent to ±1.96 * the standard deviation. Therefore we can obtain the standard error by dividing the width of the 95% CI by 2 * 1.96 or 3.92.\nAdmittedly, coding this by hand is a bit of a chore, but the beauty of programming is that we can create our own functions for processes that we routinely perform.\nIn this case I want to input a vector like skewed_sample$score and have it return the bootstrapped ses and the resulting skew and kurtosis.\nI’ll name this function skew_kurtosis_ses. Notice how I’m just taking the steps that I’ve performed before and wrapping them in the function:\n\n# Skewness and kurtosis and their standard errors as determined \n# using boostrapped confidence intervals\n\n# Arguments:\n#   - x: vector of data you wish to analyze\n#   - calc_method: method of calculating skew and kurtosis, used by DescTools,                    defaults to \"2\"\n#   - calc_ci: how to derive conf. intervals, for DescTools, default \"bca\"\n#   - reps: How many bootstrap repetitions, recommend at least 1000\n\n# Result: a 2x2 matrix with standardized skew and kurtosis (z) \n# as well as critical values to compare against. \n\n# Values as recommended by Kim (2013) Statistical notes for clinical \n# researchers: assessing normal distribution (2) using skewness and kurtosis.\n\n\nskew_kurtosis_z <- function(x, calc_method = 2, calc_ci = \"bca\", reps = 10000){\n  \n  # get skew and kurt and their cis \n  skew_ci <- DescTools::Skew(x, method = calc_method, ci.type = calc_ci,\n                             conf.level = 0.95, R = reps)\n  kurt_ci <- DescTools::Kurt(x, method = calc_method, ci.type = calc_ci,\n                             conf.level = 0.95, R = reps)\n  \n  # calculate ses from ci\n  skew_ses <- (skew_ci[3] - skew_ci[2])/3.92\n  kurt_ses <- (kurt_ci[3] - kurt_ci[2])/3.92\n  \n  # calculate standardized valuess\n  skew_z<- skew_ci/skew_ses \n  kurt_z<- kurt_ci/kurt_ses\n  \n  # remove names and ci columns\n  skew_z <- unname(skew_z)[1]\n  kurt_z <- unname(kurt_z)[1]\n  \n  values <- data.frame(\"values\" = rbind(skew_z,kurt_z))\n  \n  # what are the critical values?\n  N <- length(x)\n  crit_vals<- ifelse(N<50, 1.96,\n                     ifelse(N<300, 3.29)\n  )\n  \n  round(cbind(values,crit_vals),digits = 2) # round to 2 digits\n  \n}\n\nNow to test my function:\n\nskew_kurtosis_z(x = skewed_sample$score, calc_method = 2, calc_ci = \"bca\", reps = 1000)\n\n       values crit_vals\nskew_z   5.49      3.29\nkurt_z   0.98      3.29\n\n\nFor those interested this function can be found on my github.com page, here, and sourced like so:\n\nsource(\"https://raw.githubusercontent.com/tehrandavis/PSYC7014/master/custom_functions/skew_kurtosis_z.R\")\n\nYou can also save it to your own computer and source it from there as needed.\nKeep in mind since we are bootstrapping from random selection your absolute magnitudes will vary."
  },
  {
    "objectID": "week04/4_1-the_normal_distribution.html#significance-tests",
    "href": "week04/4_1-the_normal_distribution.html#significance-tests",
    "title": "\n15  The normal distribution\n",
    "section": "\n16.4 Significance Tests",
    "text": "16.4 Significance Tests\n\n16.4.1 Shapiro-Wilks\nThe formal normality tests including Shapiro-Wilks test may be used from small to medium sized samples (e.g., n < 300), but may be unreliable for large samples. It outputs a test statistic W that is tested against deviations from 1. This can be run by:\n\nshapiro.test(normal_sample$score)\n\n\n    Shapiro-Wilk normality test\n\ndata:  normal_sample$score\nW = 0.98327, p-value = 0.2369"
  },
  {
    "objectID": "week04/4_2-building_intuitions_about_probability_and_nhst.html",
    "href": "week04/4_2-building_intuitions_about_probability_and_nhst.html",
    "title": "\n16  Building intuitions about probability and NHST\n",
    "section": "",
    "text": "17 Simulating distributions\nFor example, we can generate a random, normal distribution, we’ll name it normDist using the rnorm() function:\nand we can convert these normDist scores to a standard distribution by submitting them to scale():\nWe can use pnorm() to assess the cumulative probability of a given score. In other words what is the likelihood of getting that score or lower in our distribution. The pnorm() function defaults to the standard distribution, that is it assumes that you have a mean=0 and sd=1. Taking a look at its input parameters:\npnorm(q, mean, sd, lower.tail) where q is the value in question; mean and sd relate known values of the distribution, and lower.tail is a logical telling you whether you are looking at the cumulative probability (TRUE) or looking at the upper tail (FALSE). For example, run each line separately and see the output.\nOne important consideration is whether your value is greater or less than the mean. For example, with the standard distribution, you are asking if q>0 or if q<0. If q>0 then everything above holds. If q<0 then the reverse holds (essentially, what constitutes the upper and lower tail is switched):\nWe can also do this with raw scores in an assumed normal distribution. Here we simply change the default values of the pnorm() parameters. For example assessing the likelihood scores of 130 and 80 assuming mean=100 and sd=15:\nWe can take advantage of the Rule of subtraction to assess the likelihood of getting a score between 120 and 130.\nBecause we typically deal with cumulative probabilities, pnorm() is the major player here (along with rnorm() to generate simulated data). It’s much rarer that you’ve be asked to give the exact probability of a score. For example the probability of having a score exactly 130. That said, if the need arises this can be accomplished in R using dnorm():\nNote that dnorm() takes vectors of scores. In fact this is how we generated our approximate normal curve to overlay our histograms last week!\nFinally, qnorm() can be used to ask the reverse of pnorm(). What score has a cumulative probability of p given the mean and sd? For example:"
  },
  {
    "objectID": "week04/4_2-building_intuitions_about_probability_and_nhst.html#a-few-words-on-where-we-are-at-so-far",
    "href": "week04/4_2-building_intuitions_about_probability_and_nhst.html#a-few-words-on-where-we-are-at-so-far",
    "title": "\n16  Building intuitions about probability and NHST\n",
    "section": "\n16.1 A few words on where we are at so far…",
    "text": "16.1 A few words on where we are at so far…\nLast week we introduced the notion of a distribution of data, and highlighted how to assess important characteristics of data distributions. This week, we will discuss sampling distributions, how they differ from population distributions, and perhaps most important for this semester’s focus, the notion of the null sampling distribution, where a key point is the assumption that the that the null sampling distribution approximates the standard distribution, with a mean \\(\\approx\\) 0, and a SD \\(\\approx\\) 1.\nWe can do wonderful things the standard distribution, including assessing the probability of an obtained value within that distribution. For example, we know that a z-score greater than 1.96 has a p-value of less than .05. Given what you know about tests for significance, you can bet your bottom dollar that this is crucial for us!!\nBefore jumping to that, however, lets build some intuitions about probability. There are several tools at our disposal for simulating and understanding probability as it relates to the normal and standard distributions in R. First we need to load in our packages:\n\npacman::p_load(tidyverse)"
  },
  {
    "objectID": "week04/4_2-building_intuitions_about_probability_and_nhst.html#thinking-about-probability-and-assumptions-of-our-data.",
    "href": "week04/4_2-building_intuitions_about_probability_and_nhst.html#thinking-about-probability-and-assumptions-of-our-data.",
    "title": "\n16  Building intuitions about probability and NHST\n",
    "section": "\n17.1 Thinking about probability and assumptions of our data.",
    "text": "17.1 Thinking about probability and assumptions of our data.\nThis week, we unpack probability and how it directly factors into our assessments on important aspects of our data. This will lead into our future discussions regarding statistics tests and inference making.\nFor the next few weeks, these two critical assumptions about the nature of our observed data will be paramount:\n\nthat the data (its distribution) is randomly obtained from a normal population.\nthat our measures of the data (including both the tools we use to get our measures and the resulting descriptions of the data) are independent of one another.\n\nWe’ve spent some time at length with Assumption #1. What do I mean in Assumption #2? Here’s an example that I use in my Perception course:\n\nSay I step on a scale and it says that I’m 180 lbs. I then walk over to a stadiometer and get my height measured and it tells me I’m 71 inches tall. Right after this I return to the original scale. If the measurement of my weight and height were independent of one another, then the scale should again read 180 lbs. However, if in some stange reality, the act of measuring my height somehow changes the measurement of my weight, then all bets are off—the second time on the scale it could say I’m 250 lbs, 85 lbs, who knows.\n\nOne of the lessons of quantum mechanics (see Stern-Gerlach experiments is that we do live in that strange reality—at least at the quantum level. Most of you have probably encountered the uncertainty principle which is a physical manifestation of this issue. More pertinent for us consider the example in class:\n\nWe have a theory that adults with children are more likely to wear seatbelts than adults without children. Our null hypothesis is that adults with children are no more likely to wear seatbelts than adults without children.\n\nHere, everyone is being measured in two ways, 1. the seatbelt measurement (do you wear a seatbelt, yes or no?) and 2. the parenthood measurement (do you have a child, yes or no?). If the two measures are independent of one another, then the seatbelt measurement should not have an effect on the parenthood measurement and vice versa. If they do, then the measures are not independent of one another. Of course, in this case, it doesn’t mean that the being measured for seatbelt-wearing may instantaneously turn you into a parent (YIKES!!!), but it does mean that being identified as a parent may increase the probability that you are a seatbelt wearer and vice versa.\nHopefully from this example, you now see the relationship between the null and research (alternative) hypotheses—the research hypothesis is structured in such a way that it assumes a violation of the null, in this case a violation of the independence assumption. In many cases we will be testing whether the evidence supports such a violation. What’s critical is that this assumption is not already violated in our null hypothesis. That is, if the null hypothesis is based on a relationship that is assumed to be independent, but in reality is not, then any measures we take run the risk of being confounded\nSo returning to why probability is so important, the probabilities related to the null distribution have been worked out by women and men far smarter than I. THESE PROBABILITIES ARE KNOWN and tell us the likelihood that our null assumptions have not been violated given our present set of data."
  },
  {
    "objectID": "week04/4_2-building_intuitions_about_probability_and_nhst.html#okay-on-to-this-weeks-example-analyses",
    "href": "week04/4_2-building_intuitions_about_probability_and_nhst.html#okay-on-to-this-weeks-example-analyses",
    "title": "\n16  Building intuitions about probability and NHST\n",
    "section": "\n17.2 Okay on to this week’s example analyses:",
    "text": "17.2 Okay on to this week’s example analyses:\nFor the following examples, we will be using functions that come pre-installed in R (base library) as well as the tidyverse. Let’s go ahead and install the tidyverse.\n\npacman::p_load(tidyverse)"
  },
  {
    "objectID": "week04/4_2-building_intuitions_about_probability_and_nhst.html#classic-example-the-coin-flip",
    "href": "week04/4_2-building_intuitions_about_probability_and_nhst.html#classic-example-the-coin-flip",
    "title": "\n16  Building intuitions about probability and NHST\n",
    "section": "\n17.3 Classic example: The coin flip",
    "text": "17.3 Classic example: The coin flip\nWhile we are primarily dealing with continuous data, I find that intuitions about probability are best made using examples from discrete outcomes (see Cohen Chapter 19 if you want to look ahead at the Binomial Distribution; if not we’ll get there at the end of the semester). The classical examples are rolling die and flipping a coin. So..\nLet’s make a wager. I’ll let you flip a single coin 10 times, and if you get 6 or more heads I’ll give you $250. If not, you have to give me $250. Should you take this bet? To answer this question you’ll want to assess the likelihood of getting an observed outcome, in this case \\(Heads \\geq 6\\), provided a sample size (10).\nWe assume that the likelihood of getting Heads on any single flip is .5, (assuming that the coin is far and not a cheat). We also assume that the coin flips are independent of one another, that is any single flip does not influence or change the likelihood of an outcome on subsequent flips. In light of our discussion in the opening section, I hope you see that depending on the outcome, it may cause us to re-evaluation these assumptions.\nFor now, let’s assume that both are indeed true and that I’m not a cheat.\n\n17.3.1 How many heads would we get if we flipped a fair coin 10 times?\nIn class we noted that with a known sample space and a known sample size we can calculate the expected outcome. In this case our sample space is {Heads, Tails} and our sample size is 10 (flips). The probability of observing a Heads in our sample space is .5. Therefore:\n\\[expected\\ Heads = p(Heads) \\times N(flips)\\\\\nexpected\\ Heads = .5 \\times 10 = 5\\]\nOf course, reality does not often abide our expectations, and given what we know about sampling error you’re more likely to not get exactly 5 flips than you are. So, what we’re really interested in is what is the likelihood of our expectation, and fortunately for us, over the long-run independent/random outcomes can be predicted probabilistically. There are several ways to address this in R.\n\n17.3.1.1 Simulation method\nThe first method involves running a large number of simulations and assessing the probability based upon the outcomes. To run a single simulation, you can use the rbinom() function. Below I am running a simulation of 1 coin (n), with a sample size of ten flips (size), and a known (or really assumed) probability of .5 (prob).\n\n# set our seeds to get the same numbers:\nset.seed(1)\n\n# n = number of coins\n# size = number of flips\n# prob(ability)\nrbinom(n = 1,size = 10,prob = .5) \n\n[1] 4\n\n\nIn this simulation you got 4 flips and I’m off to cop a new pair of retro Air Jordans with your money… or I suppose if I’m being sensible, you’ve paid for this week’s daycare. In any event, you lost. But we know that not much can be learned about the likelihood of an outcome with a sample size of 1. Let’s try running this scenario using a simulation of 100K coins.\nTo run this simulation, you modify the rbinom() call:\n\nset.seed(1)\nnumberHeads <- rbinom(n = 100000,size = 10,prob = .5)\n\nLet’s plot this using ggplot():\n\n# must convert vector to a data frame\nnumberHeads_df <- tibble(numberHeads)\n\np <- ggplot2::ggplot(data = numberHeads_df, aes(x=numberHeads)) + \n        geom_histogram(binwidth=1,boundary=-0.5,fill=\"red\", col=\"grey\") + \n        scale_x_continuous(limits=c(0,10), breaks = 1:10)\nshow(p)\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\nAs you can see in the plot above, we get exactly 5 heads a little less than 25K out of 100K, or about 25% of the time. That’s the eyeball test, How might we go about obtaining the exact probabilities from this simulation? We can take advantage of logical statements. Logicals produce outputs that are either TRUE or FALSE. More, and this is terribly useful for us, R also reads those TRUE or FALSE outcomes as 1 or 0 where TRUE=1 and FALSE=0. Take the following vector for example:\n\n\n [1]  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE\n\n\nR reads this as a numeric:\n\n\n [1] 1 1 0 0 0 1 0 0 1 0\n\n\nWhat this means is that you can get info about the number of TRUEs by performing mathematical operations on this vector. For example, above you see that there are 4 TRUEs out of 10 possible. That means that we get TRUE 40% of the time. We can obtain this probability in R using the mean() function. How? Remember that the mean is the sum of scores divided by the number of scores. In this case the sum is 4; the number of scores is 10; and 4/10 is .40.\nReturning to our simulation of 100K samples, numberHeads, we can do the following to get the probability of 5 (see Demos, Ch. 3 on logicals for explanation of operators like ==):\n\nmean(numberHeads==5)\n\n[1] 0.24477\n\n\nSo the probability of getting exactly 5 heads is 0.245.\nReturning to our wager, based on our simulation what’s the probability of getting 6 or more heads?\n\nmean(numberHeads>=6)\n\n[1] 0.37743\n\n\nAbout 38%. You probably shouldn’t take my bet.\n\n17.3.1.2 Non-simulation method\nSimulations are useful for helping us visualize our scenarios and use pseudo real data to test our underlying assumptions. But most times you just want the answer… in fact often that will suffice because as I mentioned, in most normal circumstances the probabilities have already been worked out. How can we get the results above without simulating 100K coins? By using two functions in R that belong to the same family as rbinom(): dbinom() and pbinom().\ndbinom() returns the exact probability of an outcome, provided a generated probability density function. It takes in arguments related to the number of successful outcomes (x), the sample size (size) and the independent probability of a successful outcome (prob).\nSo the probability of 5 Heads (successes) on 10 flips with a fair coin would be entered:\n\ndbinom(x = 5,size = 10,prob = .5)\n\n[1] 0.2460938\n\n\nWe see that this value is pretty close to our simulated outcome, confirming that the simulation was indeed correct, if not entirely necessary.\nWe can also use this function to build a table for the individual probabilities of all possible outcomes. To see how, first lets consider the space of possible outcomes in this scenario. At one extreme, I could flip a coin 10 times and get no Heads. At the other I could get all Heads. So the set of possible outcomes can be express as a sequence from 0 to 10. Recall that you can create such a sequence using the : operator:\n\n0:10\n\n [1]  0  1  2  3  4  5  6  7  8  9 10\n\n\nWith this in mind you can modify the previous code like so:\n\ndbinom(x = 0:10,size = 10,prob = .5)\n\n [1] 0.0009765625 0.0097656250 0.0439453125 0.1171875000 0.2050781250\n [6] 0.2460937500 0.2050781250 0.1171875000 0.0439453125 0.0097656250\n[11] 0.0009765625\n\n\nThe output gives be the resulting probabilities in order from 0 Heads to 10 Heads.\nLet’s create a table to make this easier to digest. First I’m going to create a vector of numberHeads to show all possibilities from 0 to 10. Second, I will run dbinom() as above to test each possibility, saving that to a vector probHead. Finally I will combine the two into a data frame called probTable:\n\n# 1. range of possibilities\nnumberHeads <- 0:10\n# 2. prob of outcome\nprobHead <- dbinom(x = numberHeads,size = 10,prob = .5)\n# 3. combine to data frame\nprobTable <- tibble(numberHeads,probHead)\n# 4. Show the data frame (table)\nshow(probTable)\n\n# A tibble: 11 × 2\n   numberHeads probHead\n         <int>    <dbl>\n 1           0 0.000977\n 2           1 0.00977 \n 3           2 0.0439  \n 4           3 0.117   \n 5           4 0.205   \n 6           5 0.246   \n 7           6 0.205   \n 8           7 0.117   \n 9           8 0.0439  \n10           9 0.00977 \n11          10 0.000977\n\n\nCongrats you’ve just created a Binomial Distribution Probability Table for this scenario. We’re a bit ahead of the game here as we deal with the Binomial Distribution at the end of the semester, but the general intuitions hold regarding the Normal Distribution as well.\nReturning to our wager of 6 or more heads, we would use pbinom(). pbinom() returns probabilities according to the cumulative density function, where the output is the likelihood of obtaining that score or less. Note that pbinom() takes similar arguments to dbinom() but asks for q instead of x. For our purposes q and x are the same thing… number of outcomes.\nSo for example the probability of obtaining 5 or less Heads provided our scenario may be calculated:\n\npbinom(q = 5,size = 10,prob = .5)\n\n[1] 0.6230469\n\n\nGiven what we know about the Compliment Law, we can compute the probability of 6 or more Heads as:\n\n1 - pbinom(q = 5,size = 10,prob = .5)\n\n[1] 0.3769531\n\n\nwhich again matches with our simulation."
  },
  {
    "objectID": "week04/4_2-building_intuitions_about_probability_and_nhst.html#changing-the-parameters",
    "href": "week04/4_2-building_intuitions_about_probability_and_nhst.html#changing-the-parameters",
    "title": "\n16  Building intuitions about probability and NHST\n",
    "section": "\n17.4 Changing the parameters",
    "text": "17.4 Changing the parameters\nSo obviously if the coin is a fair coin, your shouldn’t take the bet. Let’s imagine that we kept the same wager, but to entice you to bet, I tell you that this coin lands on Heads 65% of the time? Should you take the bet?\nTo test this new scenario all you need to do is change the probability parameter. Let’s just skip the simulations and just assess using dbinom() and pbinom().\n\n17.4.1 Constructing a table of outcome probabilities:\nAs before we’ll use dbinom() to create a table, simply modifying the prob argument to .65:\n\n# 1. range of possibilities\nnumberHeads <- 0:10\n# 2. prob of outcome\nprobHead <- dbinom(x = numberHeads,size = 10,prob = .65)\n# 3. combine to data frame\nprobTable <- tibble(numberHeads,probHead)\n# 4. Show the data frame (table)\nshow(probTable)\n\n# A tibble: 11 × 2\n   numberHeads  probHead\n         <int>     <dbl>\n 1           0 0.0000276\n 2           1 0.000512 \n 3           2 0.00428  \n 4           3 0.0212   \n 5           4 0.0689   \n 6           5 0.154    \n 7           6 0.238    \n 8           7 0.252    \n 9           8 0.176    \n10           9 0.0725   \n11          10 0.0135   \n\n\nHow does this table compare to the one above?\n\n17.4.2 Assessing cummulative probability\nAnd now, to get the probability of 6 or greater:\n\n1 - pbinom(q = 5,size = 10,prob = .65)\n\n[1] 0.7514955\n\n\nAhhh… the odds are ever so slightly in your favor.\nWhat if we changed the bet: you get 60 Heads out of 100 flips?\n\n1 - pbinom(q = 60,size = 100,prob = .65)\n\n[1] 0.827585\n\n\nYea, you should really take that bet! Fortunately for me, I wasn’t born yesterday.\nHow about 12 out of 20?\n\n1 - pbinom(q = 12,size = 20,prob = .65)\n\n[1] 0.6010266\n\n\nNope, I’m not taking that bet either. Does 3 out of 5 interest you?\n\n1 - pbinom(q = 3,size = 5,prob = .65)\n\n[1] 0.428415"
  },
  {
    "objectID": "week04/4_2-building_intuitions_about_probability_and_nhst.html#catching-a-cheat",
    "href": "week04/4_2-building_intuitions_about_probability_and_nhst.html#catching-a-cheat",
    "title": "\n16  Building intuitions about probability and NHST\n",
    "section": "\n17.5 Catching a cheat",
    "text": "17.5 Catching a cheat\nOK. Last scenario. Let’s imagine that I am not a total sucker, and we reach a compromise on “30 or more Heads out of 50 flips”. You run to your computer and calculate your odds and like your chances (maybe I am a sucker)!\n\n1-pbinom(q = 30,size = 50,prob = .65)\n\n[1] 0.7264363\n\n\nYou flip 50 times but only get 27 heads. Astounded, because those odds really were in your favor, you label me a liar and a thief. Are you justified in doing so? This scenario essentially captures our discussion at the outset, how far of a deviation warrants us being skeptical that our original assumptions were true. In this case the original assumptions were that 1. each coin flip is independent and 2. the independent probability of getting a Heads is 0.65. We typically set our threshold at \\(p<.05\\), which remember for a two tailed test means that we are on the lookout of extreme values with a \\(p<.025\\).\nSo essentially we are asking if the probability of obtaining exactly 27 Heads given this scenario is less than 2.5%:\n\ndbinom(x = 27,size = 50,prob = .65)\n\n[1] 0.03132011\n\n\nYou Madam/Sir have besmirched my honor!\nOK, well is 27 isn’t enough, then how low (or high) do you need to go to pass the critical threshold? To answer this we need to construct a table of probabilities:\n\n# 1. range of possibilities\nnumberHeads <- 0:50\n# 2. prob of outcome\nprobHead <- dbinom(x = numberHeads,size = 50,prob = .65)\n# 3. combine to data frame\nprobTable <- tibble(numberHeads,probHead)\n# 4. Show the data frame (table)\nshow(probTable)\n\n# A tibble: 51 × 2\n   numberHeads probHead\n         <int>    <dbl>\n 1           0 1.60e-23\n 2           1 1.48e-21\n 3           2 6.75e-20\n 4           3 2.01e-18\n 5           4 4.38e-17\n 6           5 7.48e-16\n 7           6 1.04e-14\n 8           7 1.22e-13\n 9           8 1.21e-12\n10           9 1.05e-11\n# ℹ 41 more rows\n\n\nOne could visually inspect the entire table noting which outcomes have a corresponding probability less than .025. But the whole point of learning R is to let it do the heavy lift for you. In this case you can ask R:\n\n“Hey, R. Which outcomes in have a probability less than .025”\n\nor more exactly:\n\n“Which rows in my data frame have a probHead less than .025”\n\nThis is accomplished using the which() function:\n\nwhich(probTable$probHead<0.025)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 40 41 42 43 44 45 46 47 48 49 50 51\n\n\nThis gives us indices of all of the rows that meet this criteria. BUT KEEP IN MIND, our list of possible outcomes started at 0 not 1. But there’s no such thing in R as row 0 (now Python on the other hand…). That means in the output above that 1 actually refers to 0 Heads, 2 to 1 Head and so on. You could stop here and just know that you need to subtract the above output by 1 to get the correct result. Or you could address this in R in the following ways:\nFirst, the quick/dirty/limited/bad way. Keep in mind this only works because we know that we can apply a subtraction rule:\n\nwhich(probTable$probHead<0.025)-1\n\n [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n[26] 25 26 39 40 41 42 43 44 45 46 47 48 49 50\n\n\nThe correct below way will work for cases in which adjusting the index is unknown. Recall from Winter Chapter 2 that you can get a specific value from a vector or data frame by using its index using the [] operator such that [row,column]. Think of this like how you would get a certain cell in Excel.\nFor example the get the 17th row of our data frame probTable:\n\nprobTable[17,]\n\n# A tibble: 1 × 2\n  numberHeads   probHead\n        <int>      <dbl>\n1          16 0.00000157\n\n\nNote that if you want all rows or columns, you leave that index blank (as I did above where I wanted both columns). Keeping in mind that which(probTable$probHead<.025) gave us a vector of indicies, we can rewrite the previous chunk like so to get the important rows in a data frame structure:\n\ncrticalValues <- which(probTable$probHead<.025)\nprobTable[crticalValues,]\n\n# A tibble: 39 × 2\n   numberHeads probHead\n         <int>    <dbl>\n 1           0 1.60e-23\n 2           1 1.48e-21\n 3           2 6.75e-20\n 4           3 2.01e-18\n 5           4 4.38e-17\n 6           5 7.48e-16\n 7           6 1.04e-14\n 8           7 1.22e-13\n 9           8 1.21e-12\n10           9 1.05e-11\n# ℹ 29 more rows\n\n\nAgain this would ask us to look through the whole table, but we can apply the same logic to just show us the corresponding numberHeads as a vector (Remember that you’ll need to specify the data frame when referring to the columns):\n\ncrticalValues <- which(probTable$probHead<.025)\nprobTable$numberHeads[crticalValues]\n\n [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n[26] 25 26 39 40 41 42 43 44 45 46 47 48 49 50\n\n\nNote that since vectors don’t properly have rows or columns, you just use a single value in the [ ].\nSo, if you got 1 less head, then you can call me a cheat. Conversely if you managed to get 39 heads or above, I might have reason to believe that you are somehow gaming me."
  },
  {
    "objectID": "week04/4_3-sampling_distributions_and_seeing_the_logic_of_nhst.html#the-sampling-distribution",
    "href": "week04/4_3-sampling_distributions_and_seeing_the_logic_of_nhst.html#the-sampling-distribution",
    "title": "\n17  Sampling distributions and seeing the logic of NHST\n",
    "section": "\n17.1 The sampling distribution",
    "text": "17.1 The sampling distribution\nTo begin let’s start off with a control population. For the sake of example, let’s imagine that this population represents the obtained IQ scores of students in Tehran J. Davis High School (apparently I’m feeling myself) that were taught using a standard curriculum. The 5000 students (it’s a really big school) in this population have a mean IQ of 103 with a standard deviation of 14. Note that this mean and sd are FACTS… they are true values that exist independent of my measure. Also note that in real work, these truths about the population are rarely known.\nWe can create a simulated population using the follow chunk of code; note that anytime you are generating “random” distributions or “random” sampling in R you’ll need to input the set.seed(1) command where I do to ensure that you get the same numbers I do:\n\n# population of 5000 with an approximate mean = 100 and sd = 14\nset.seed(2021)\npop_basic <- rnorm(n = 5000,mean = 103,sd = 14)\n\nNote Technically, there is an issue using these simulated scores for IQ. rnorm() assumes a continuous distribution, including decimals. We typically do not report IQ scores to the decimal. To clean this up I would need to take the obtained scores and apply the round() function\n\npop_basic <- pop_basic %>% round()\n\nWhen we take a look at what the above simulation generates, we see that the obtained values are sightly different than requested (it is random after all). BUT AGAIN, for the purposes of example these are still the facts of the population.\n\nmean(pop_basic)\n\n[1] 103.3048\n\nradiant.data::sdpop(pop_basic)\n\n[1] 14.24815\n\n\nWait… what’s that radiant.data::sdpop() mess?!?!?\nRemember that calculating the standard deviation of a population is different than calculating for a sample, where the sample SD requires an adjustment for the degrees of freedom. Simply, the population SD uses \\(N\\) in the denominator, where the sample SD uses \\(N-1\\). The sd() function calculates sample SD. To get population SD, we can use the function popsd() from the multicon package.\nFrom this population, the Principal of TJD-HS tells us that we can only administer our IQ test to 250 students. This represents our sample:\n\nmySample <- sample(x = pop_basic,size = 250)\npsych::describe(mySample)\n\n   vars   n   mean    sd median trimmed   mad min max range skew kurtosis   se\nX1    1 250 103.17 15.23    103  102.67 13.34  69 154    85 0.34     0.05 0.96\n\nhist(mySample)\n\n\n\n\nIf we run the above code again, we see that we get different measures for mySample:\n\nmySample <- sample(x = pop_basic,size = 250)\npsych::describe(mySample)\n\n   vars   n   mean    sd median trimmed   mad min max range  skew kurtosis   se\nX1    1 250 104.46 14.15    105  104.56 14.83  69 145    76 -0.03    -0.28 0.89\n\nhist(mySample)\n\n\n\n\nand in neither case is the observed sample mean identical to the true population mean.\nThese differences highlight the notion of Variability due to chance-the fact that statistics (e.g., means, SDs) obtained form samples naturally vary from one sample to the next due to the particular observations that are randomly included in the sample.\nThe term sampling error is used to refer to variability due to chance, in that, the numerical value of a sample statistic will probably deviate (be in error) from the parameter it is estimating.\nIn class we mentioned that we can build a theoretical distribution of sampling error, a sampling distribution of means. Essentially our goals are the following steps:\n\npull a sample of a given size from the population\nget the mean of that sample\nsave that sample mean to a vector\nrepeat 1-3\n\nHow many times do we repeat? Theoretically an infinite number of times. Because we can’t wait for infinity, lets just use a really big number like 100,000 (or 1e+05)\nBuilding the sampling distribution of means (using pop_basic from above), note the comments highlight what each line / pipe argument is doing:\n\nsampling_distributions <- \n  # tibble: create a table with a column \"simulation\" that contains values 1:100000\n  tibble(simulation = 1:100000) %>%\n  # group_by: perform every function after this for EACH num:\n  group_by(simulation) %>% \n  # mutate: add a column \"sample_mean\" that contains the mean for simulation num:\n  mutate(sample_mean = mean(sample(pop_basic,size = 250,replace = T)))\n\nLets take a look at our sampling distribution:\n\nhist(sampling_distributions$sample_mean,main = \"Theoretical sampling distribution\")\n\n\n\n\nand the grand mean of the sampling distribution (the mean of means if you will):\n\nmean(sampling_distributions$sample_mean)\n\n[1] 103.3048\n\n\nAgain not quite the TRUE mean of the population, but pretty darn close."
  },
  {
    "objectID": "week04/4_3-sampling_distributions_and_seeing_the_logic_of_nhst.html#standard-error-of-the-mean",
    "href": "week04/4_3-sampling_distributions_and_seeing_the_logic_of_nhst.html#standard-error-of-the-mean",
    "title": "\n17  Sampling distributions and seeing the logic of NHST\n",
    "section": "\n17.2 Standard error of the mean",
    "text": "17.2 Standard error of the mean\nWe’ve also learned that the standard error (SE) of the mean is simply the standard deviation of the sample distribution. That is, how much does the mean vary due to chance provided repeated sampling. In this case it can be found by:\n\nsd(sampling_distributions$sample_mean)\n\n[1] 0.8981243\n\n\nHowever, deriving the SE in this way assumes that we are free to sample and re-sample the population a large number of time (in this case 100 thousand) when in reality we may have neither the time, resources or inclination to do so. Instead, we use the following equation to provide an estimate of SE provided our sample:\n\\[SE=\\frac{SD_{sample}}{\\sqrt{N_{sample}}}\\]\nSo in this case, for a given mySample from the population pop_basic:\n\nset.seed(1)\nmySample <- sample(x = pop_basic,size = 250)\nmySample_se <- sd(mySample)/sqrt(250)\nshow(mySample_se)\n\n[1] 0.951111\n\n\nCompared to the theoretically derived SE above, the estimate is not too far off (0.895 v. 0.884)."
  },
  {
    "objectID": "week04/4_3-sampling_distributions_and_seeing_the_logic_of_nhst.html#the-difference-distribution",
    "href": "week04/4_3-sampling_distributions_and_seeing_the_logic_of_nhst.html#the-difference-distribution",
    "title": "\n17  Sampling distributions and seeing the logic of NHST\n",
    "section": "\n17.3 The difference distribution",
    "text": "17.3 The difference distribution\nImagine for the sake of example, we are interested in IQ. A central debate (as I understand the literature) at present is whether IQ is inherently static or can be improved with education. Indeed the implication of this can be quite controversial (IQ testing is like the third rail of understanding intelligence… making statements about intelligence is like the third rail of psychology… but I digress).\nIn addition to our population of students taking the basic curriculum, we also have an equal number of students taking an advanced curriculum. Assuming that we have already controlled for other factors (e.g., baseline IQs from both groups the same before treatment) we would like to address the following question using data obtained from each group: “Does the enhanced cirriculum result in higher IQ?”\nFirst let’s create our populations:\n\nset.seed(1)\npop_basic <- rnorm(n = 5000,mean = 103,sd = 14)\npop_advanced <- rnorm(n = 5000,mean = 108,sd = 13)\n\nAgain, you’ll note that in this example we’re omnipotent and know the truth of the matter (the true measures)… in the real world no one is all-seeing and knowing. You’ll also notice that I kept the SD for each population roughly equivalent. This homogeniety of variences (i.e., SDs should be approximately equal) is another assumption of our non-parametric tests (like ANOVA). Here I’ve made it so, but if in true the variances are not equal then we may have to adjust how we proceed with our analyses. More on that in a few weeks!\nAnd now to create our difference distribution using the same programming logic as above:\n\ndifference_distribution <- \n  # tibble: create a table with a column \"simulation\" that contains values 1:100000\n  tibble(simulation = 1:100000) %>%\n  # group_by: perform every function after this for EACH num:\n  group_by(simulation) %>% \n  # mutate: add a column \"basic_mean\" that contains the mean for simulation num\n  #         same for \"advanced_mean\"\n  #         get difference between the two samples for that simulation\n  mutate(basic_mean = mean(sample(pop_basic,size = 250,replace = T)),\n         advanced_mean = mean(sample(pop_advanced, size = 250, replace = T)),\n         difference_means = advanced_mean-basic_mean)\n\nAnd now to take a look at the resulting difference distribution:\n\nmean(difference_distribution$difference_means)\n\n[1] 4.915453\n\nhist(difference_distribution$difference_means, main = \"Theoretical difference distribution\")\n\n\n\n\nAs anticipated (because we know all) we end up with a difference distribution with a mean difference of about 5. What we are asked to consider is whether this difference (or any observed difference, really) is enough to say that the likelihood of it occurring by chance is sufficiently small (i.e. is \\(p < .05\\)). For this we need to create a null distribution."
  },
  {
    "objectID": "week04/4_3-sampling_distributions_and_seeing_the_logic_of_nhst.html#the-null-distribution",
    "href": "week04/4_3-sampling_distributions_and_seeing_the_logic_of_nhst.html#the-null-distribution",
    "title": "\n17  Sampling distributions and seeing the logic of NHST\n",
    "section": "\n17.4 The Null distribution",
    "text": "17.4 The Null distribution\nThe logic of constructing a null distribution rests on the claims made by the null hypothesis, that the means of the two (or more) populations in question are identical:\n\\[\\mu_1=\\mu_2\\]\nIn a larger sense, if the null hypothesis is indeed true it suggests that the two populations may indeed be one single population. Here, we know that the null hypothesis is NOT true (again, all seeing and knowing).\n\nmean(pop_advanced)\n\n[1] 107.8715\n\nmean(pop_basic)\n\n[1] 102.9554\n\n# are the 2 means exactly equal?\nmean(pop_advanced)==mean(pop_basic)\n\n[1] FALSE\n\n\nIn the context of our everday experiments, we do not know whether the null is indeed true or not. Based upon the samples that we take we make an inference as to whether we have sufficient evidence to reject the null. We do so if the probability of our observed data, given the null is true, is sufficiently low. In other words, here we are asking: “IF the null is true, THEN what is the likelihood that we would get our observed differences in means.” If the likelihood is low, then we may have reason to believe the null hypothesis to be suspect.\nReturning to the logic of the null distribution, it assumes that our experimental group, pop_advanced, and our control group, pop_basic are from the same population. Thus, any samples obtained betwixt should be equal, and the differences in sample means should approximate 0 (never exactly 0 due to sampling error). However, by virtue of our experimental design we are implicitly assuming that, in fact the two are different from one another. This implicit assumption means that the null distribution cannot be created by creating a difference distribution between the two groups.\nSo how to go about the business of creating a null distribution, then? We create a difference distribution that guarantees that all data is from the same population. In this case we are creating a difference distribution where we take repeated samples from the same population and compare their differences. Based on this distribution, we can make claims regarding the probability of observed differences between scores assuming they are from the same population. Again sampling error guarantees that these differences are not exactly zero.\nTypically the population we choose is our control population (group). In this case the logical control is pop_basic. Modifying the simulation code that we used for the difference distribution, we create a null distribution like so:\n\n# create an empty vector to store the null difference in means\nnull_distribution <- \n  # tibble: create a table with a column \"simulation\" that contains values 1:100000\n  tibble(simulation = 1:100000) %>%\n  # group_by: perform every function after this for EACH num:\n  group_by(simulation) %>% \n  # mutate: add a column \"basic_mean\" that contains the mean for simulation num\n  #         same for \"advanced_mean\"\n  #         get difference between the two samples for that simulation\n  mutate(control_sample1 = mean(sample(pop_basic,size = 250,replace = T)),\n         control_sample2 = mean(sample(pop_basic, size = 250, replace = T)),\n         null_means = control_sample1-control_sample2)\n\nAnd now to take a look at the resulting null distribution:\n\nmean(null_distribution$null_means)\n\n[1] -0.004857817\n\nhist(null_distribution$null_means, main = \"Theoretical null distribution\")\n\n\n\n\nIn discussion we made mention that due to the assumption of equal means and equal variances that the null distribution should approximate the form of the standard distribution were \\(\\mu=0\\) and \\(\\sigma = 1\\). This fact allows us to say something about the probability of an observed difference as a probability of obtaining a Z-score that far removed from 0."
  },
  {
    "objectID": "week04/4_3-sampling_distributions_and_seeing_the_logic_of_nhst.html#probabilty-of-observed-differences",
    "href": "week04/4_3-sampling_distributions_and_seeing_the_logic_of_nhst.html#probabilty-of-observed-differences",
    "title": "\n17  Sampling distributions and seeing the logic of NHST\n",
    "section": "\n17.5 Probabilty of observed differences",
    "text": "17.5 Probabilty of observed differences\nContinuing on from the previous section, we have our null distribution of IQ scores, appropriately named null_distribution. This is the theoretical distribution of differences assuming the two populations are the same. Simply put, this is the distribution of differences we might expect if our advanced curriculum did not have any impact. You’ll note that even assuming that the two groups are identical still yields some differences by chance:\n\nmax(null_distribution$null_means)\n\n[1] 5.514248\n\nmin(null_distribution$null_means)\n\n[1] -5.409361\n\n\nbut that the probability of those extremes is very, very low (but still exists which is why we never prove the alternative hypothesis). Let’s say I take a sample from pop_basic and pop_advanced and find that my means differ by 3. Given the null distribution, what is the probability that I will observe a difference this large? To derive this probability, we take advantage of what we know about the standard distribution, where we have a known probability of obtaining any given Z-score.\nThe process of answering the above is as follows: 1. transform the observed difference to a Z-score using the mean and SD of the null distribution; 2. calculate the probability of that extreme of a score.\nHere is step 1:\n\n# 1. convert the observed difference to a z-score:\nZscore <- (3-mean(null_distribution$null_means))/sd(null_distribution$null_means) \nshow(Zscore)\n\n[1] 2.339173\n\n\nI want to pause here to note that the obtained Z-score is positive. As a result the score is on the right side of the distribution. Our probability function, pnorm() returns the probability of an obtained score or lower (the cumulative probability). However, we want the probability of the observed score or more extreme. In this case more extreme means greater than. Conversely if the observed Z score is less than 0, then more extreme means less than. I bring this up as this determines what value we use to calculate the desired probability. In the latter case, less than we can simple take the output of pnorm() and call it a day. However, in cases like ours, when \\(Z>0\\) we need to use 1-pnorm().\nOK, on to the calculation:\n\nZscore <- (3-mean(null_distribution$null_means))/sd(null_distribution$null_means) \n1-pnorm(Zscore)\n\n[1] 0.009663249\n\n\nBased on a criteria of rejecting the null if \\(p<.05\\) we would reject here. BUT NOTE: not necessarily because it’s less than .05. Remember that if I have a two-tailed test, my criteria requires that I split my \\(\\alpha\\). So in truth I need to obtain a value less than .025 in either direction."
  },
  {
    "objectID": "week04/4_3-sampling_distributions_and_seeing_the_logic_of_nhst.html#a-note-about-sample-size",
    "href": "week04/4_3-sampling_distributions_and_seeing_the_logic_of_nhst.html#a-note-about-sample-size",
    "title": "\n17  Sampling distributions and seeing the logic of NHST\n",
    "section": "\n17.6 A note about sample size",
    "text": "17.6 A note about sample size\nSo I got a pretty low p-value, but you might say it was expected given that my sample size was 250 for each group. Let’s see what happens if I drop the sample size down to 10 students per group, creating a new null distribution, null_distribution_10:\n\n# create an empty vector to store the null difference in means\nnull_distribution_10 <- \n  tibble(simulation = 1:100000) %>%\n  group_by(simulation) %>% \n  mutate(control_sample1 = mean(sample(pop_basic,size = 10,replace = T)),\n         control_sample2 = mean(sample(pop_basic, size = 10, replace = T)),\n         null_means = control_sample1-control_sample2)\n\nWhat does this distribution look like?\n\nhist(null_distribution_10$null_means, main = \"Theoretical null distribution\")\n\n\n\nmean(null_distribution_10$null_means)\n\n[1] 0.01041048\n\n\nAnd let’s take a look at the probability of an observed difference of 3 in this case. Not that I’m using the pnorm() shortcut so I don’t need to calculate my Z by hand:\n\n1-pnorm(q = 3,mean = mean(null_distribution_10$null_means),sd = sd(null_distribution_10$null_means))\n\n[1] 0.3211936\n\n\nAs we can see, size matters. Given that we are omnipotent in this case, we know the truth of the matter is that the two populations are in fact different, and that the mean of pop_basic is not equal to the mean of pop_advanced. Thus what we have here is a failure to reject the null hypothesis when it is indeed false. Hmm, what kind of error is that again? This last example is going to figure prominently in our discussions of power and what power is, properly defined. For now, I think this is a good place to stop this week."
  },
  {
    "objectID": "week04/4_4-more_examples.html",
    "href": "week04/4_4-more_examples.html",
    "title": "\n18  More examples of distributions\n",
    "section": "",
    "text": "19 Distributions\nThis week you’ve encountered the notion of the distribution, which is just a way of describing all of the scores in a given set. We typically describe distributions in terms of tallies. For example if we were looking at the distribution of heights in a given group, the distribution would describe the number of individuals within that group that were a certain height (or more likely within a range of heights). I like to focus on distributions as having three different classifications.\nIn this walkthrough we are going to use R to visualize the relationship between three types of distributions that are going to be particularly relevant for you this week:"
  },
  {
    "objectID": "week04/4_4-more_examples.html#population-distribution",
    "href": "week04/4_4-more_examples.html#population-distribution",
    "title": "\n18  More examples of distributions\n",
    "section": "\n19.1 Population distribution",
    "text": "19.1 Population distribution\nWe’re just extrapolate to play around with some numbers from this website: https://ourworldindata.org/human-height. According to this website the average height of men in North America (from 1996) is about 177 cm, with a standard deviation of 7.49. Height, it turns out is normally distributed.\nLet’s just assume those numbers hold for the US. There are about 151 million men in the US. On my machine it would takes R about 10 secs to generate a distribution of 151 million heights with the above criteria, but it also eats up about 1.2 GB of RAM. Given that not everyone has 32GB of RAM laying around on their laptops, let’s just create a population of 1 million men (depending on your machine this might take a few seconds:\n\npopulation_heights <- rnorm(n = 1000000, mean = 177,sd = 7.49)\n\nCongrats, in a matter of seconds you’ve created a population of 1 million heights! Before doing so, let’s turn this into a data frame / tibble containing columns for person and their corresponding height:\n\npacman::p_load(tidyverse)\nperson_id <- 1:1000000\n\npop_mens_heights <- tibble(person = person_id,\n                      height = population_heights)\n\nLet’s take a look at this distribution.\n\npacman::p_load(cowplot)\nggplot(data = pop_mens_heights,mapping = aes(x=height)) +\n  geom_histogram(bins = sqrt(1000000)) + # ideal # of bins = sqrt of N\n  theme_cowplot()\n\n\n\n\nAnd getting our summary stats for this population\n\npacman::p_load(psych)\npsych::describe(pop_mens_heights)\n\n       vars     n      mean       sd    median   trimmed       mad    min\nperson    1 1e+06 500000.50 288675.3 500000.50 500000.50 370650.00   1.00\nheight    2 1e+06    177.02      7.5    177.01    177.01      7.51 141.86\n              max     range skew kurtosis     se\nperson 1000000.00 999999.00    0     -1.2 288.68\nheight     214.51     72.65    0      0.0   0.01"
  },
  {
    "objectID": "week04/4_4-more_examples.html#sample",
    "href": "week04/4_4-more_examples.html#sample",
    "title": "\n18  More examples of distributions\n",
    "section": "\n19.2 Sample",
    "text": "19.2 Sample\nOf course we don’t have time or money to go around getting the height of 1 million men. Instead we get a sample of 25 from our population, pop_men_heights.\nWe can do this by:\n\n# get a sample of heights\nsample_mens_heights <- sample_n(pop_mens_heights, size = 25)\nsample_mens_heights\n\n# A tibble: 25 × 2\n   person height\n    <int>  <dbl>\n 1 731455   175.\n 2 930817   170.\n 3 658082   175.\n 4 642767   169.\n 5 903667   174.\n 6 631238   172.\n 7 553502   174.\n 8 106059   170.\n 9 857770   167.\n10 407625   171.\n# ℹ 15 more rows\n\n\nNow, let’s take a look at the distribution and the summary stats of our sample:\n\n# histogram\nggplot(data = sample_mens_heights,mapping = aes(x=height)) +\n  geom_histogram(bins = sqrt(25)) + # ideal # of bins = sqrt of N\n  theme_cowplot()\n\n\n\n# summary stats\npsych::describe(sample_mens_heights)\n\n       vars  n      mean        sd    median   trimmed       mad       min\nperson    1 25 660454.32 227428.65 658082.00 684792.10 264102.95 106059.00\nheight    2 25    173.19      5.86    171.65    173.14      4.48    162.55\n          max     range  skew kurtosis       se\nperson 930817 824758.00 -0.79    -0.21 45485.73\nheight    184     21.45  0.35    -0.74     1.17\n\n\nTwo things:\nFirst, related to R, your histogram and summary stats are likely different from mine. This is because sample_n() pulls a random sample. If you resample the data using sample_n() you’re going to get different numbers every time around. FWIW, this is also true when we are getting our samples in research. To ensure that our numbers are identical we need to set the seed—set the manner R “randomly” (nothing in a computer is truly random) selects its sample. For example, if we rerun our sample using this code, our numbers will match:\n\n# setting the seed to 2021\nset.seed(2021)\n\n# get a sample of heights\nsample_mens_heights <- sample_n(pop_mens_heights, size = 25)\n\n# histogram\nggplot(data = sample_mens_heights,mapping = aes(x=height)) +\n  geom_histogram(bins = sqrt(25)) + # ideal # of bins = sqrt of N\n  theme_cowplot()\n\n\n\n# summary stats\npsych::describe(sample_mens_heights)\n\n       vars  n      mean        sd   median   trimmed       mad      min\nperson    1 25 589528.52 282701.81 643236.0 602575.86 364602.47 66630.00\nheight    2 25    176.82      7.17    176.7    176.58      9.58   166.96\n             max     range  skew kurtosis       se\nperson 976624.00 909994.00 -0.30    -1.23 56540.36\nheight    190.11     23.15  0.07    -1.38     1.43\n\n\nSecond, tied to the conceptual material (and related to the discussion above), you’ll note that the distribution of heights in your sample is not identical to the population. This will be true for any sample that you take—the sample statistics are not identical to the population statistics. What we hope is that they are close enough that our sample approximates the population within a certain tolerance. More, no two samples are going to be exactly alike:\n\nset.seed(2021)\nsample1 <- sample_n(pop_mens_heights,size = 25)\nsample2 <- sample_n(pop_mens_heights,size = 25)\n\npsych::describe(sample1)\n\n       vars  n      mean        sd   median   trimmed       mad      min\nperson    1 25 589528.52 282701.81 643236.0 602575.86 364602.47 66630.00\nheight    2 25    176.82      7.17    176.7    176.58      9.58   166.96\n             max     range  skew kurtosis       se\nperson 976624.00 909994.00 -0.30    -1.23 56540.36\nheight    190.11     23.15  0.07    -1.38     1.43\n\npsych::describe(sample2)\n\n       vars  n     mean        sd    median   trimmed       mad      min\nperson    1 25 428057.2 308820.34 353637.00 415141.71 360989.38 14244.00\nheight    2 25    175.6      7.03    176.81    176.05      6.26   156.31\n             max     range  skew kurtosis       se\nperson 967537.00 953293.00  0.38    -1.30 61764.07\nheight    186.36     30.05 -0.69     0.27     1.41\n\n\nThe fact that summary statistics vary from sample to sample, leads us to our next kind of distribution… the sampling distribution."
  },
  {
    "objectID": "week04/4_4-more_examples.html#sampling-distribution",
    "href": "week04/4_4-more_examples.html#sampling-distribution",
    "title": "\n18  More examples of distributions\n",
    "section": "\n19.3 Sampling Distribution",
    "text": "19.3 Sampling Distribution\nThe sampling distribution is a distribution built by the sampling and resampling of a population. A sampling distribution of a statistic shows every possible result that statistic can take in every possible sample from a population and how often each result happens. In practice, the generic characteristics of a sampling distribution are theoretical. That is they are developed by applying some small amount of observation to establish fundamental, and ultimately abstract principles. As an example, the normal distribution is a theoretical distribution that states that for any (approximately) randomly occurring measurement, such as height, a given percentage of scores should fall within specific ranges from the mean. Indeed, we’ve already generated a normal distribution in this walkthrough when we created our population. The function rnorm() generates a normal distribution of n scores, data with a prescribed mean and sd:\n\nrnorm(n = 1000, mean = 100, sd = 15) %>% hist(., main = \"Randomly generated normal distribution\")\n\n\n\n\nReturning to the Sampling Distribution, one claim is that if we were to sample and resample a population to infinity, the Sampling Distribution of Means (i.e. the distribution of those sample means) would be normal. More the the mean of this Sampling Distribution of Means should be identical to the population mean (177.0).\nObviously we can’t get an infinite number of samples, so this assumption / claim is based on underlying mathematical principles and formalisms. BUT… one of the nice things about computers is that while we can’t go to infinity, we can perform a really high number of samples. Let’s generate a sampling distribution of means by taking one-hundred-thousand different samples from our population of men, where each sample is 25 men\n\n# imagine taking a hundred thousand different samples...\nnumber_of_samples <- 100000\n\n# where each sample is 25 men\nN <- 25\n\nsampling_dist_means <- tibble(num = 1:number_of_samples) %>% \n    group_by(num) %>% \n  # using sample for a vector, rather than sample_n for a data frame (tibble)\n    mutate(sample_means = mean(sample(pop_mens_heights$height, size = N, replace = TRUE)),\n           )\n\nLet’s look at our sampling distribution of means:\n\n# histogram\nggplot(data = sampling_dist_means,mapping = aes(x=sample_means)) +\n  geom_histogram(bins = sqrt(10000)) + # ideal # of bins = sqrt of N\n  theme_cowplot()\n\n\n\n# summary stats\npsych::describe(sampling_dist_means)\n\n             vars     n     mean       sd   median  trimmed      mad    min\nnum             1 1e+05 50000.50 28867.66 50000.50 50000.50 37065.00   1.00\nsample_means    2 1e+05   177.02     1.50   177.02   177.02     1.51 170.42\n                   max    range skew kurtosis    se\nnum          100000.00 99999.00    0     -1.2 91.29\nsample_means    183.35    12.93    0      0.0  0.00\n\n\nLook at that!\nOne last bit with the Sampling Distribution of Means. The standard deviation of the Sampling Distribution of Means is known as the standard error. You may have likely encountered this term before as a measure of confidence of the mean of your sample as it relates to the true mean of the population. When working with a given sample, standard error is approximated as the standard deviation of the sample divided-by the square root of the number of scores…\n\\[\\frac{standard\\space deviation}{N}\\] For example the standard error for this single sample is \\(7.54/\\sqrt{25}\\) or 7.54/5 or 1.51. Note that psych::describe() produces the se at the send of its table:\n\nset.seed(2021)\nsample_n(pop_mens_heights,size = 25) %>% psych::describe()\n\n       vars  n      mean        sd   median   trimmed       mad      min\nperson    1 25 589528.52 282701.81 643236.0 602575.86 364602.47 66630.00\nheight    2 25    176.82      7.17    176.7    176.58      9.58   166.96\n             max     range  skew kurtosis       se\nperson 976624.00 909994.00 -0.30    -1.23 56540.36\nheight    190.11     23.15  0.07    -1.38     1.43\n\n\nIn this case the standard error of the sample (1.51) is pretty close to the standard deviation of the sampling distribution (1.49). Given that we are dividing by \\(\\sqrt{N}\\) it should be apparent that the standard error decreases as we increase or sample size. Thus, with increase sample size we are more confident that a given sample accurately represents the whole population.\nTry rerunning the code, but this time with sample sizes of 5, 50, and 500. What happens to your distributions and your measures of central tendency?"
  },
  {
    "objectID": "week04/4_5-bias_variance.html",
    "href": "week04/4_5-bias_variance.html",
    "title": "\n19  Bias in sample variance estimates\n",
    "section": "",
    "text": "One thing to note is that you can perform a sampling distribution of any of the statistical moments / measures of central tendency: mean, median, standard deviation, variance, skew, kurtosis.\nThis following example is taken from Barry Cohen on Biased and Unbiased Sample Variances. It’s very much related to the discussion of biased parameter estimates in Navarro Chapter 10.\n\nThe problem is that the variance of the sample tends to underestimate the variance of the population. Of course, the variance of every sample will be a little different, even if all of the samples are the same size and they are from the same population. Some sample variances will be a little larger than the population variance and some a little smaller, but unfortunately the average of inﬁnitely many sample variances (when calculated by the formula above) will be less than the population variance. This tendency of a sample statistic to consistently underestimate (or overestimate) a population parameter is called bias. The sample variance as deﬁned by the (unnumbered) formula above is therefore called a biased estimator.\n\nWe can demonstrating this fact by building a sampling distribution of variances. Let’s load in our packages\n\npacman::p_load(tidyverse, radiant, cowplot)\n\nFirst let’s create a population of 2000 people using rnorm, with a known mean of 100 and standard deviation of 15. Note that these are the distributions parameters:\n\n# this ensures that your data is like my data\nset.seed(1) \n# generate \"random\" values resulting in a mean ≈ 100; sd ≈ 15\npopulation <- rnorm(n = 2000, mean = 100,sd = 15)\n# check our mean and variance\nmean(population)\n\n[1] 99.79067\n\nradiant.data::sdpop(population)^2\n\n[1] 241.9279\n\n\nSo our variance of this population is about 241.93. Note that radiant.data::sdpop() uses the formula for population variance (actually it’s standard deviation, but you see we squared it). However, if we were to apply this formula to a specific sample, Cohen claims that our estimate of sample variance would be biased. Let’s build up a sampling distribution comparing the biased estimate from radiant.data::sdpop() and our unbiased estimate from sd()^2\nNow to draw R number of samples of N people from this population\n\n# number of resamples\nR <- 1000\n# size of samples\nN = 20\n\nsingle_samp <- sample(population,size = 20)\n\nvariance_dist <- tibble(num = 1:R) %>% \n    group_by(num) %>% \n    mutate(means = mean(sample(population, size = N, replace = TRUE)),\n           bias_variances = radiant.data::sdpop(sample(population, size = N, replace = TRUE))^2,\n           unbias_variances = sd(sample(population, size = N, replace = TRUE))^2\n           )\n\nComparing the population variance to the average biased variance and the average unbiased variance.\n\nsprintf(\"population var: %.2f\", radiant.data::sdpop(population)^2) # population variance\n\n[1] \"population var: 241.93\"\n\nsprintf(\"biased var: %.2f\",mean(variance_dist$bias_variances)) # mean of bias variance\n\n[1] \"biased var: 232.90\"\n\nsprintf(\"unbiased var: %.2f\",mean(variance_dist$unbias_variances)) # mean of unbiased variance\n\n[1] \"unbiased var: 238.89\"\n\n\nWe see that the unbiased variance (239.78) is much closer to the true population variance (241.92) than the biased variance (230.92)."
  },
  {
    "objectID": "week05/5_2_correlations.html#the-relationship-btw-stress-and-health",
    "href": "week05/5_2_correlations.html#the-relationship-btw-stress-and-health",
    "title": "\n20  Correlations\n",
    "section": "\n20.1 The Relationship b/tw Stress and Health",
    "text": "20.1 The Relationship b/tw Stress and Health\n\nWagner, Compas, and Howell (1988) investigated the relationship between stress and mental health in first-year college students. Using a scale they developed to measure the frequency, perceived importance, and desirability of recent life events, they created a measure of negative events weighted by the reported frequency and the respondent’s subjective estimate of the impact of each event. This served as their measure of the subject’s perceived social and environmental stress. They also asked students to complete the Hopkins Symptom Checklist, assessing the presence or absence of 57 psychological symptoms.\n\nThis data can be accessed directly from this companion website for Howell’s stats textbook (a book I used a while back).\n\nstress_data <- read_table(\"https://www.uvm.edu/~statdhtx/methods8/DataFiles/Tab9-2.dat\")\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  ID = col_double(),\n  Stress = col_double(),\n  Symptoms = col_double(),\n  lnSymptoms = col_double()\n)\n\npsych::describe(stress_data)\n\n           vars   n  mean    sd median trimmed   mad   min    max  range skew\nID            1 107 54.00 31.03  54.00   54.00 40.03  1.00 107.00 106.00 0.00\nStress        2 107 21.29 12.49  20.00   20.49 11.86  1.00  58.00  57.00 0.62\nSymptoms      3 107 90.33 18.81  88.00   88.87 17.79 58.00 147.00  89.00 0.74\nlnSymptoms    4 107  4.48  0.20   4.48    4.48  0.19  4.06   4.99   0.93 0.21\n           kurtosis   se\nID            -1.23 3.00\nStress        -0.19 1.21\nSymptoms       0.45 1.82\nlnSymptoms    -0.28 0.02\n\n\n\n20.1.1 Testing our assumptions\nHere we are interested in the relationship between perceived Stress (as measured on a scale factors the number and impact of negative life events) and the presence of psychological Symptoms. As with any other parametric analysis, we first need to see if our data adheres to the assumptions of normality and homogeniety of varience. We can use the tools we’ve been employing for the last few weeks test these assumptions:\nFirst Stress:\n\nhist(stress_data$Stress,breaks = 10)\n\n\n\ncar::qqPlot(stress_data$Stress)\n\n\n\n\n[1] 72 55\n\n\nYou might notice that the plotting functions that I’ve used here are NOT ggplots. In truth, for simple histograms and qqplots I just use the functions above. What’s nice about the car::qqplot() is that it highlights the extreme values, in this case participants 55 and 72. When putting together anything that invoves complex subsetting or presenting and publishing data, ggplot is my preferred method.\nEyeballing the plots, our Stress measures look slightly skewed, but not egregiously so. Let’s take a look at the skew and kurtosis values. Remember that this entails dividing our skew value (from psych::describe of psych::skew by the standard error of the skew. Recall, to get the standard error of the skew, the best way is to bootstrap a sampling distribution.\nHere I’m just going to bootstrap the skew using DescTools::Skew() and get the 95% CI\n\nboot_skew_data <- DescTools::Skew(stress_data$Stress, method = 2, ci.type = \"bca\",conf.level = .95)\n\nThen to get the standard error I can just divide the range of the ci by 3.92 (95% of a normal distribution falls withing ±1.96 SD of the mean; the standard error = 1 SD).\n\nstandard_error_skew <- (boot_skew_data[\"upr.ci\"] - boot_skew_data[\"lwr.ci\"])/3.92\nstandard_error_skew %>% unname()\n\n[1] 0.1887328\n\n\nAlternatively, we can also estimate this value running some simulations (bootstrapping manually):\n\nboot_skew_data <- tibble(simulation = 1:10000) %>%\n  group_by(simulation) %>%\n  mutate(skew_vals = sample(stress_data$Stress,size = 107,replace = T) %>% psych::skew())\nstandard_error_skew <- sd(boot_skew_data$skew_vals)\n\nAt this point I would also like to introduce you to a slightly easier way of bootstrapping (you’ve earned it), using the resample package:\n\npacman::p_load(resample)\n\n\nThe downloaded binary packages are in\n    /var/folders/7k/6v7xmh35305_vjzy660s9_q40000gs/T//RtmpBPdxC2/downloaded_packages\n\n\n\nresample installed\n\nboot_skew_data <- resample::bootstrap(data = stress_data, statistic = mean(Stress),R = 10000)\nboot_skew_data$stats\n\n             Observed       SE     Mean         Bias\nmean(Stress) 21.28972 1.201429 21.28782 -0.001896262\n\n\nYou’ll note that the output of this function (boot_skew_data$stats) gives us the:\n\nempirically Observed sample statistic, in this case the skew of our original sample (0.618)\nthe standard error of the sampling distribution SE = .179\nthe Mean of the sampling distribution\nand the Bias, the difference between the original sample statistic and the Mean of the sampling distribution.\n\nIf so inclined I could get the distribution of skews by invoking $replicates. (See next section)\nRecall that we would then divide our the skew of the actual sample by the SE and evaluate the result against the criteria mentioned in class. (Again if we are being diligent we would repeat this for kurtosis).\nFinally, we might invoke the Shapiro-Wilkes test (\\(W\\)) of normality by comparing our observed distribution against a theoretical normal. Here the null hypothesis is that the \\(observed == theoretical\\), where \\(p < .05\\) indicates that the observed distribution is not normal. Our obtained p-value confirms suggests the possibility that Stress measures deviate from normal.\n\nshapiro.test(stress_data$Stress) # see Field (2014), Sec 5.6.1\n\n\n    Shapiro-Wilk normality test\n\ndata:  stress_data$Stress\nW = 0.96009, p-value = 0.002709\n\n\nHowever, as noted in the Field text (5.6.1) we need to be careful using the Shapiro-Wilkes test on large samples. Ultimately you need to make a judgment on whether or not all of the evidence available leads you to the conclusion of non-normality. In this case I probably would trust the normality of this data due to what I see in the Q-Q plot (very few large deviations from the normal line).\nNow Symptoms\n\nhist(stress_data$Symptoms,breaks = 10)\n\n\n\npsych::describe(stress_data$Symptoms)\n\n   vars   n  mean    sd median trimmed   mad min max range skew kurtosis   se\nX1    1 107 90.33 18.81     88   88.87 17.79  58 147    89 0.74     0.45 1.82\n\ncar::qqPlot(stress_data$Symptoms)\n\n\n\n\n[1] 24 19\n\nshapiro.test(stress_data$Symptoms) # see Field (2014), Sec 5.6.1\n\n\n    Shapiro-Wilk normality test\n\ndata:  stress_data$Symptoms\nW = 0.95918, p-value = 0.00232\n\n\nSymptoms does not pass either the eyeball or the Shapiro-Wilkes tests. These data are slightly positively skewed. Let’s use bootstrapped standard error of the skew to evaluate whether we have a meaningful deviation:\n\nboot_skew_data <- resample::bootstrap(data = stress_data, statistic = psych:: skew(Stress),R = 10000)\n\npsych::skew(stress_data$Symptoms)/boot_skew_data$stats$SE\n\n[1] 4.128424\n\n\nUsing the criteria set forth in Kim (2013), this data is indeed skewed (> 3.29).\nOne way of dealing with non-normal data is by performing a logarithmic transformation (see Field, 5.8). Logarithmic transformations reduce the weight of extreme scores. For example, consider the following vector:\n\nc(2,4,6,8,100)\n\n[1]   2   4   6   8 100\n\n\n100 is setting way out to the extreme of the other scores. Now let’s get the log() of this sequence:\n\nc(2,4,6,8,100) %>% log()\n\n[1] 0.6931472 1.3862944 1.7917595 2.0794415 4.6051702\n\n\nThe values become more manageable (i.e., closer) at this rescaling.\nThis has already been performed for this data, lnSymptoms is a natural logarithmic transform of Symptoms. With your own data, this can be accomplished quite simply in R by using the log() as above:\nQuestion 1. Perform a natural log transform of Symptoms and mutate() it to a new column in your data frame TransformedData. Assess TransformedData using a qqPlot:"
  },
  {
    "objectID": "week05/5_2_correlations.html#plotting-the-data",
    "href": "week05/5_2_correlations.html#plotting-the-data",
    "title": "\n20  Correlations\n",
    "section": "\n20.2 Plotting the data",
    "text": "20.2 Plotting the data\nOne of the first things that you should do is plot your data. Plotting gives you a sense of what is going on with your data. In fact, YOU SHOULD NEVER TAKE A TEST RESULT AT FACE VALUE WITHOUT FIRST LOOKING AT YOUR DATA!! Beware of Anscombe’s quartet!\nTo really drive this home, let’s look at an example from a function that Jan Vanhove developed as a teaching tool. the function plot_r() takes an input correlation coefficient r and number of observations n and plots 16 very distrinct types of patterns that can be produced. For the sake of not slowing down your machine too much, I’d recommend keeping n at less than 200.\n\nsource(\"http://janhove.github.io/RCode/plot_r.R\")\nplot_r(r = 0.5, n = 50)\n\n\n\n\nLet’s plot our data using ggplot. In this case we will be creating a scatterplot of our data. This can be accomplished by adding geom_point() to our base ggplot(). For example:\n\nggplot(stress_data, aes(x = Stress, y = lnSymptoms)) + \n  geom_point() + \n  theme_cowplot() + # turns the plot into something close to APA\n  xlab(\"Stress\") + \n  ylab(\"lnSymptoms\")\n\n\n\n\nQuestion 2: create a ggplot scatterplot comparing lnSymptoms to Symptoms What does the result look like. What about this relationship suggests that performing a logarithmic transform may be kosher"
  },
  {
    "objectID": "week05/5_2_correlations.html#covariance-and-correlation",
    "href": "week05/5_2_correlations.html#covariance-and-correlation",
    "title": "\n20  Correlations\n",
    "section": "\n20.3 Covariance and Correlation",
    "text": "20.3 Covariance and Correlation\nThis week’s readings offer excellent overviews of covariance and correlation so I won’t go into too much depth here. Briefly, let’s make a few connections to ideas that we’ve already encountered.\n\n20.3.1 Covariance\nRecall that variance may be calculated as:   \\(s_{x}^{2}=\\frac{\\sum \\left ( x_{i}-\\bar{X} \\right )^{2}}{n-1}\\)\nWhere the numerator is the sum of squared differences from each score to the sample mean, and the denominator is our degrees of freedom.\nVariance tells us to what degree scores in a particular sample variable deviate from its mean. With this in mind, covarience is a statement about the degree to which two sampled variables deviate from their respective means. Consider we have sampled two measures, X & Y from a population. When addressing the degree to which X and Y co-vary, we are asking the question: “To what degree and in what direction does Y move away from its mean as X moves from its mean?”\nAs such, the formula for covariance is simply an extension of the formula for variance that we already know: \\[s_{xy}=\\frac{\\sum \\left ( x_{i}-\\bar{X} \\right )\\left ( y_{i}-\\bar{Y} \\right )}{n-1}\\]\nSo, in order to calculate the covariance we need the calculate the sum of the cross-product of the sum of squared differences of our two variables (X = Stress; Y = lnSymptoms) and divide that number by of degrees of freedom. We could go about the business of calculating this “by hand”:\n\n# N = number of rows in data frame\nN <- nrow(stress_data)\n\n\n#mean Stress:\nmeanX <- mean(stress_data$Stress)\n\n# and same for lnSymptoms:\nmeanY <- mean(stress_data$lnSymptoms)\n\n# plug these values into our equation:\n\n# sum of cross product\nnumerator <- sum((stress_data$Stress-meanX)*(stress_data$lnSymptoms-meanY))\n\n# degrees of freedom\ndenominator <- (N-1)\n\n# covariance:\ncovXY <- (numerator/denominator) %>% print()\n\n[1] 1.336434\n\n\nThe covariance of Stress and lnSymptoms is 1.336.\nQuestion 3: Alternatively, covariance may be calculated quickly in R using the cov() function: Remember, once you open up a function, you can press the TAB key to get prompts about what to put in each argument.\n\n# use cov() to calculate the covariance between stress_data$Stress and stress_data$lnSymptoms\n\n\n20.3.2 Correlation\nIt may be tempting to calculate our covariance and stop there, but covariance is a limited measure. What I mean by this is that covarience indexes the degree of relationship between two specific variables, but doesn’t allow for more general comparison across situations. For a quick example, lets multiply both Stress and lnSymptoms by two. Let’s use mutate() to create a these by2 columns:\n\nstress_data <- stress_data %>% mutate(\"Stress_by2\" = Stress * 2,\n                                    \"lnSymptoms_by2\" = lnSymptoms * 2\n                                    )\nstress_data\n\n# A tibble: 107 × 6\n      ID Stress Symptoms lnSymptoms Stress_by2 lnSymptoms_by2\n   <dbl>  <dbl>    <dbl>      <dbl>      <dbl>          <dbl>\n 1     1     30       99       4.60         60           9.19\n 2     2     27       94       4.54         54           9.09\n 3     3      9       80       4.38         18           8.76\n 4     4     20       70       4.25         40           8.50\n 5     5      3      100       4.61          6           9.21\n 6     6     15      109       4.69         30           9.38\n 7     7      5       62       4.13         10           8.25\n 8     8     10       81       4.39         20           8.79\n 9     9     23       74       4.30         46           8.61\n10    10     34      121       4.80         68           9.59\n# ℹ 97 more rows\n\n\nWe would like to think that multiplying every value by a constant should have no effect on our general interpretation of the relationships in our data as the overall relationship in our data remain the same. To help make this apparent, let’s imagine that I weigh 200 lbs (yea… imagine) and my wife weighs 100 lbs. So I weigh twice as much as my wife. Over the next year we both go on a binge fest and both double our respective weights—I’m now 400 lbs and my wife is 200 lbs. I still weigh twice as much as my wife. Coincidentally the fact that relationships remain unchanged in spite of these sorts of mathematical transformations is why we could perform the natural logarithmic transform earlier and not feel too guilty.\nQuestion 4: Well what happens when I get the covariance of my new _by2 data?:\n\n# use cov() to calculate the covariance between stress_data$Stress_by2 and stress_data$lnSymptoms_by2\n\nThe covariance changes!! This is a problem, and is why, in order to usefully convey this data we report the correlation. The correlation is a standardized covariance.\nThe unit of measurement we’ll use for standardization is the standard deviation. We can standardize the covariance in one of two ways:\n      1. Standardize our variables (into z-scores) and then calculate the covariance: \\[r_{xy}=\\frac{\\sum z_{x}z_{y}}{n-1}\\]\n\nzX <- scale(stress_data$Stress)\nzY <- scale(stress_data$lnSymptoms)\n\ncorXY <- (sum(zX*zY) / (N-1))\n\ncorXY\n\n[1] 0.5286565\n\n\nor\n      2. Calculate the covariance and standardize it (by dividing by the product of the standard deviation): \\[r_{xy}=\\frac{\\sum \\left ( x_{i}-\\bar{X} \\right )\\left ( y_{i}-\\bar{Y} \\right )}{(n-1)s_{x}s_{y}}\\]\n\n# get SD of X and Y:\nsdX <- sd(stress_data$Stress)\nsdY <- sd(stress_data$lnSymptoms)\n\n# using the covXY calculated above:\ncovXY / (sdX*sdY)\n\n[1] 0.5286565\n\n\nAgain, in R we don’t need to do this by hand, there are in fact several functions. A more comprehensive look can be found in Field 6.5.3. The simplest function is cor(), with outputs the correlation as a single value. However, I prefer to use cor.test() as it tends to provide the most immediately useful data. You can input your data into this function in two ways:\n\ncor.test(stress_data$Stress,stress_data$lnSymptoms)\n\nor\n\ncor.test(~Stress+lnSymptoms,data = stress_data)\n\n\n    Pearson's product-moment correlation\n\ndata:  Stress and lnSymptoms\nt = 6.3818, df = 105, p-value = 4.827e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3765970 0.6529758\nsample estimates:\n      cor \n0.5286565 \n\n\nI prefer the latter as it uses formula notation which is the sort of notation we used for t-tests and will continue to use for regression and ANOVA. Finally, if you don’t want all the additional gobble-dee-gook from cor.test you can simply attach $estimate to the end of either the function or object like so:\n\ncor.test(~Stress+lnSymptoms,data = stress_data)$estimate\n\n      cor \n0.5286565 \n\n\nHow did I know that I could do this you ask? To get a quick list of things that you can pull from the attributes, try the attributes() function. It gives you a list of names that you can call using the $ operator.\n\ncor.test(~Stress+lnSymptoms,data = stress_data) %>% attributes()\n\n$names\n[1] \"statistic\"   \"parameter\"   \"p.value\"     \"estimate\"    \"null.value\" \n[6] \"alternative\" \"method\"      \"data.name\"   \"conf.int\"   \n\n$class\n[1] \"htest\"\n\n\nQuestion 5 save the cor.test() output of the correlation between Stress and lnSymptoms as an object corOut. Submit corOut to the attributes() function. Grab the p.value and estimate:\n\n# using $ grab the p.value and estimate from corOut\n\nOur correlation is expressed in terms of the Pearson’s product-moment correlation coefficient (or \\(r\\), for short). Here \\(r\\) = .529.\nQuestion 6 Rerun the cor.test (no need to get the attributes this time) using the problematic by_2 columns. What do you find relative to the last test?\n\n# calculate the correlation of the \"_by2\" data:"
  },
  {
    "objectID": "week05/5_2_correlations.html#adjusting-pearsonss-r-by-hand",
    "href": "week05/5_2_correlations.html#adjusting-pearsonss-r-by-hand",
    "title": "\n20  Correlations\n",
    "section": "\n20.4 adjusting Pearsons’s \\(r\\) by hand",
    "text": "20.4 adjusting Pearsons’s \\(r\\) by hand\nNote that when our sample size is small (N<30) we may need to adjust \\(r\\). This is because the sampling distribution of \\(r\\) is not normally distributed, most especially when you have strong correlations. Don’t believe me? Let’s run a simulation, this time assuming a sample size of only 20 people.\n\ncor.test(~Symptoms + Stress, data = stress_data)\n\n\n    Pearson's product-moment correlation\n\ndata:  Symptoms and Stress\nt = 6.3165, df = 105, p-value = 6.559e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3719397 0.6498580\nsample estimates:\n      cor \n0.5247429 \n\n\n\nr_distribution <- tibble(simulation = 1:10000) %>%\n  group_by(simulation) %>%\n  mutate(r_value = cor.test(~Symptoms+Stress, data = sample_n(stress_data,size = 20, replace = T))$estimate   )\nhist(r_distribution$r_value)\n\n\n\n\nQuestion 7 Try re-running the above code, this time with sample sizes of 100 instead of 20. What do you see?\n\n# rerun the simulation\n\nThe formula for this adjustment is:     \\(r_{adj}=\\sqrt{1-\\frac{\\sum \\left (1-r^2 \\right )\\left ( n-1 \\right )}{n-2}}\\)\nFor now, we can calculate this by hand, but later we will see that it will be provided by another function (or at least its squared value will be).\n\n# Pearson's r:\nrXY <- cor.test(~Stress+lnSymptoms,data = stress_data)$estimate\n\n# adjusted r:\nrXYadj <- sqrt(1-((1-rXY^2)*(N-1)/(N-2))) %>% print()\n\n     cor \n0.522126"
  },
  {
    "objectID": "week05/5_2_correlations.html#significance-testing-r",
    "href": "week05/5_2_correlations.html#significance-testing-r",
    "title": "\n20  Correlations\n",
    "section": "\n20.5 Significance testing \\(r\\)\n",
    "text": "20.5 Significance testing \\(r\\)\n\nIt may be useful to perform tests of significance on \\(r\\). Typically, there are two types of tests that we perform:\n\na test of the observed \\(r\\) to \\(r=0\\), and\na test of the difference between two \\(r\\) values.\n\nRegarding the first case, to test that the observed correlation is different from 0 we can use the aforementioned cor.test():\n\ncor.test(~Stress+lnSymptoms,data = stress_data)\n\n\n    Pearson's product-moment correlation\n\ndata:  Stress and lnSymptoms\nt = 6.3818, df = 105, p-value = 4.827e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3765970 0.6529758\nsample estimates:\n      cor \n0.5286565 \n\n\nIn the second case, the difference between \\(r\\) values, we may invoke the paired.r() function from the psych package. For example lets assume that we know that the correlation between GRE scores and GPA for one group of 100 students is 0.50 and for another group of 80 students it’s 0.61. We ask is the correlation for the second group significantly higher than the first. To test the difference between these two independent \\(r\\)s” we\n\npsych::paired.r(xy = .50, n = 100, xz = .61, n2 = 80,twotailed = T)\n\nCall: psych::paired.r(xy = 0.5, xz = 0.61, n = 100, n2 = 80, twotailed = T)\n[1] \"test of difference between two independent correlations\"\nz = 1.05  With probability =  0.3\n\n\nwhere:\n\n\nxy: the correlation in the first data set\n\nxz: the correlation in the second data set\n\nn: the number of samples in the first data set\n\nn2: the number of samples in the second data set -twotailed: run a two-tailed test, TRUE or FALSE\n\n\nThis output tells us the resulting Fisher \\(z\\) score and corresponding probability.\nQuestion 8: For example assume that you sample 35 men and find a correlation of .42 between Symptoms and Stress. In another sample of 42 women you find a correlation of .51. Perform a test to see if those two correlations are different from one another.\n\n# use psych::paired.r to answer Question #8"
  },
  {
    "objectID": "week05/5_3_regressions.html#fitting-the-data-to-a-model",
    "href": "week05/5_3_regressions.html#fitting-the-data-to-a-model",
    "title": "\n21  Regression, pt. 1: Running the model\n",
    "section": "\n21.1 Fitting the data to a model",
    "text": "21.1 Fitting the data to a model\nAs social scientists, we are not only concerned with observation, but also with explanation. Measures of correlation provide us with the former; we use regression methods to build models in service of the latter. Again both the assigned texts provide excellent overviews of regression, so I won’t repeat much of what they say here.\nFor the purposes of this vignette (and this course) we will limit ourselves to linear regression, that is describing a line that best fits our data. By “fit” we mean a line than when compared to our observed data minimizes our squared residuals."
  },
  {
    "objectID": "week05/5_3_regressions.html#the-relationship-btw-stress-and-health",
    "href": "week05/5_3_regressions.html#the-relationship-btw-stress-and-health",
    "title": "\n21  Regression, pt. 1: Running the model\n",
    "section": "\n21.2 The Relationship b/tw Stress and Health",
    "text": "21.2 The Relationship b/tw Stress and Health\nLet’s revisit our data from the first walk-through:\n\nWagner, Compas, and Howell (1988) investigated the relationship between stress and mental health in first-year college students. Using a scale they developed to measure the frequency, perceived importance, and desirability of recent life events, they created a measure of negative events weighted by the reported frequency and the respondent’s subjective estimate of the impact of each event. This served as their measure of the subject’s perceived social and environmental stress. They also asked students to complete the Hopkins Symptom Checklist, assessing the presence or absence of 57 psychological symptoms.\n\nThis data can be accessed directly from this companion website for Howell’s stats textbook.\n\nstress_data <- read_table(\"https://www.uvm.edu/~statdhtx/methods8/DataFiles/Tab9-2.dat\")\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  ID = col_double(),\n  Stress = col_double(),\n  Symptoms = col_double(),\n  lnSymptoms = col_double()\n)\n\npsych::describe(stress_data)\n\n           vars   n  mean    sd median trimmed   mad   min    max  range skew\nID            1 107 54.00 31.03  54.00   54.00 40.03  1.00 107.00 106.00 0.00\nStress        2 107 21.29 12.49  20.00   20.49 11.86  1.00  58.00  57.00 0.62\nSymptoms      3 107 90.33 18.81  88.00   88.87 17.79 58.00 147.00  89.00 0.74\nlnSymptoms    4 107  4.48  0.20   4.48    4.48  0.19  4.06   4.99   0.93 0.21\n           kurtosis   se\nID            -1.23 3.00\nStress        -0.19 1.21\nSymptoms       0.45 1.82\nlnSymptoms    -0.28 0.02\n\n\nTo perform a linear regression we may call upon the lm() function. This function uses formula notation outcome variable ~ predictor variable(s). A simple regression has a single predictor. More often in our analyses we are concerned with the relative effects of multiple predictors, but this is multiple regression and saved for the Spring course. Here, we may be interested in the degree to which perceived Stress contributes to the number of lnSymptons, or using formula terminology: “How do lnSymptoms (outcome) vary as a function of Stress (predictor)”. This is represented in R as stress_data$lnSymptoms~stress_data$Stress"
  },
  {
    "objectID": "week05/5_3_regressions.html#linear-models-in-r",
    "href": "week05/5_3_regressions.html#linear-models-in-r",
    "title": "\n21  Regression, pt. 1: Running the model\n",
    "section": "\n21.3 Linear models in R",
    "text": "21.3 Linear models in R\nLet’s run the model we introduced in the last section. Note that instead of specifying the data frame for each variable, I can place it in the data= argument.\n\nlm(lnSymptoms~Stress,data = stress_data)\n\n\nCall:\nlm(formula = lnSymptoms ~ Stress, data = stress_data)\n\nCoefficients:\n(Intercept)       Stress  \n   4.300537     0.008565  \n\n\nThis output gives us our intercept and slope coefficients. That said, theirs more lurking behind this output. The resulting output of the lm() function is an object with the class lm. This simply means that R understands that this object is storing a linear model. Hiding behind this spartan output is a multitude of info that may be accessed via its attributes or it may be thrown into other functions for additional info and analysis. My tip… GET IN THE HABIT OF SAVING (ASSIGNING) YOUR MODELS. In this case let’s assign the model we just ran to stress_symptoms_model\n\nstress_symptoms_model <- lm(lnSymptoms~Stress,data = stress_data)\n\n\n21.3.1 Attributes of class lm:\nAs I mentioned above, lm objects contain various attributes that may be accessed using the names convention, $nameOfAttribute.\nFor example, returning to the stress_symptoms_model object we created above, we may get a glimpse of its attributes by:\n\nattributes(stress_symptoms_model)\n\n$names\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n$class\n[1] \"lm\"\n\nstress_symptoms_model$residuals\n\n           1            2            3            4            5            6 \n 0.037639323  0.011508729  0.004407164 -0.223337657  0.278938976  0.262339353 \n           7            8            9           10           11           12 \n-0.216226628  0.008264362 -0.193462063  0.204051115  0.133337343 -0.155679251 \n          13           14           15           16           17           18 \n-0.045884469  0.028334551 -0.155679251 -0.254621647 -0.137363093  0.016773551 \n          19           20           21           22           23           24 \n 0.357607907  0.243820353 -0.282917628  0.033932947 -0.064990034  0.407257917 \n          25           26           27           28           29           30 \n 0.003404758  0.021897739  0.035071937  0.189244294  0.108281966 -0.013123706 \n          31           32           33           34           35           36 \n-0.234484836  0.219322294 -0.176332459 -0.030854449 -0.183458251  0.262268521 \n          37           38           39           40           41           42 \n-0.239353647 -0.046311261  0.101345155 -0.107814044 -0.140097291 -0.370393063 \n          43           44           45           46           47           48 \n 0.021897739  0.166111353  0.126967897  0.032561729  0.073479541  0.006251511 \n          49           50           51           52           53           54 \n-0.160091093 -0.033866242 -0.089593420  0.110316333  0.082044343 -0.038040440 \n          55           56           57           58           59           60 \n-0.009426726 -0.070215895  0.026507135 -0.206792222 -0.215357024 -0.055546449 \n          61           62           63           64           65           66 \n-0.291503706 -0.136643865  0.003404758  0.126759115 -0.043850103  0.088591947 \n          67           68           69           70           71           72 \n 0.140416947 -0.274251885  0.356359125 -0.034559647  0.004783570 -0.172322132 \n          73           74           75           76           77           78 \n-0.163087291  0.006961956  0.306215758 -0.100953836 -0.017485657  0.066205125 \n          79           80           81           82           83           84 \n-0.029852044  0.100660511 -0.239370083 -0.066710063 -0.070121261 -0.134714420 \n          85           86           87           88           89           90 \n 0.278938976  0.092780353  0.251559956 -0.428892905  0.105060560  0.039027343 \n          91           92           93           94           95           96 \n 0.054768927  0.350241155 -0.170835232 -0.025250034  0.053197164  0.031945917 \n          97           98           99          100          101          102 \n-0.175912826 -0.011625261  0.058350768  0.233516155  0.338095135 -0.055170044 \n         103          104          105          106          107 \n-0.308985677  0.228080145  0.052651115  0.076727164  0.042924729 \n\n\nThis command tells us the names of several attributes (values) that are stored in stress_symptoms_model. Typically we are interested in the $coefficients, fitted.values (aka predicted values), and $residuals. Recall that in our model fitted.values are the predicted values of lnSymptoms for each value of Stress, that is those values that fall along the line of best fit. residuals are the difference between our observed values of lnSymptoms and those predicted by the model. Mathematically stress_symptoms_model$residuals is equivalent to:\n\nstress_data$lnSymptoms - stress_symptoms_model$fitted.values\n\n\n21.3.2 Testing the residuals\nAn important assumption test for our model is that the residuals are normally distributed. In fact some argue that this is more important than than having normal distributions in the raw data itself. Quickly, we can test this using the regular methods:\n\nhist(stress_symptoms_model$residuals,breaks = 10)\ncar::qqPlot(stress_symptoms_model$residuals) %>% show()\npsych::describe(stress_symptoms_model$residuals)\n\nQuestion: Run a Shapiro-Wilkes test on our stress_symptoms_model residuals. How do we feel about the residuals here?\n\n# test the stress_symptoms_model residulas\n\nWe’ll return to diagnosing your regression model for violations of your assumptions (normality & heteroscadicity) in an an upcoming walkthrough. For now, I just wanted to use this as an example of extracting values from your model.\n\n21.3.3 Using your lm object with other functions\nAs I mentioned above you may also use lm objects with other functions. For example, for our model stress_symptoms_model:\n\nsummary(stress_symptoms_model)\n\n\nCall:\nlm(formula = lnSymptoms ~ Stress, data = stress_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.42889 -0.13568  0.00478  0.09672  0.40726 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.300537   0.033088 129.974  < 2e-16 ***\nStress      0.008565   0.001342   6.382 4.83e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1726 on 105 degrees of freedom\nMultiple R-squared:  0.2795,    Adjusted R-squared:  0.2726 \nF-statistic: 40.73 on 1 and 105 DF,  p-value: 4.827e-09\n\n\nprovides us with info about the coefficients, their standard error, t-scores, and corresponding p-values (Pr>|t|). It also provides us with our \\(r^2\\) and adjusted \\(r_{adj}^2\\) (which corresponds to our \\(r\\) and \\(r_{adj}\\) from above).\nCoincidentally we can also call the individual attributes of this summary as well:\n\nsummary(stress_symptoms_model) %>% attributes()\n\n$names\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\n$class\n[1] \"summary.lm\"\n\nsummary(stress_symptoms_model)$coefficients\n\n               Estimate  Std. Error    t value      Pr(>|t|)\n(Intercept) 4.300536618 0.033087547 129.974478 8.085268e-118\nStress      0.008564802 0.001342063   6.381819  4.827486e-09\n\n\nFor what it’s worth, Bodo Winter (optional text) is a fan of using the tidy function from the broom package to clean up the output. I prefer summary but can understand the desire for clean and to the point output.\n\npacman::p_load(broom)\nbroom::tidy(stress_symptoms_model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)  4.30      0.0331     130.   8.09e-118\n2 Stress       0.00856   0.00134      6.38 4.83e-  9"
  },
  {
    "objectID": "week05/5_3_regressions.html#a-note-on-degrees-of-freedom-of-our-model",
    "href": "week05/5_3_regressions.html#a-note-on-degrees-of-freedom-of-our-model",
    "title": "\n21  Regression, pt. 1: Running the model\n",
    "section": "\n21.4 A note on degrees of freedom of our model",
    "text": "21.4 A note on degrees of freedom of our model\nThe general equation for calculating the requisite degrees of freedom of a model is \\(n-k\\) where \\(n\\) is the number of observations (or later conditions) and \\(k\\) is the number of parameters that are required to be known for the calculation.\nRecall how we calculate variance / standard deviation. We take the sum of the squared deviations from the mean \\(\\sum(X-\\bar{X})\\) and divide by \\((n-1)\\). Remember that we are subtracting that 1 because in order to calculate the variance we must keep one thing constant, the mean. In this case the mean is our parameter that must be known in order to calculate the variance.\nWe can now ask the question “what must be known in order to derive the line of best fit?”, or more simply what must one know in order to create a particular line. We must know both the line’s slope and its intercept. Knowing slope alone leads to the indeterminism presented in Fig A (identical slopes, infinitely-many intercepts), intercept alone leads to indeterminism presented in Fig B (identical intercept, infinitely-many slopes). We need both to define a particular line. Just like the mean above, these are the two parameters that must be locked-in to derive our model.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nA simple way of restating this is that your model degrees of freedom are the: \\[number \\space of \\space observations -  number \\space of \\space predictors\\]\nwhere at least one of your predictors is used as a constant (the intercept) and the remainder provide your \\(\\beta\\) coefficients.\nFor simple regression we have 1 predictor and 1 constant so our degrees of freedom are \\(n-k\\); where \\(k=2\\). However, in your future there be be dragons… models that may have multiple predictors and multiple intercepts (multiple regression, mixed effects / multi-level models, etc). Indeed, using df may not even make much sense in such cases, and there are arguments about how to even calculate them in the first place."
  },
  {
    "objectID": "week05/5_3_regressions.html#interpreting-the-output",
    "href": "week05/5_3_regressions.html#interpreting-the-output",
    "title": "\n21  Regression, pt. 1: Running the model\n",
    "section": "\n21.5 Interpreting the output",
    "text": "21.5 Interpreting the output\nWith our plot in hand, we can return to our summary(stress_symptoms_model) output:\n\nsummary(stress_symptoms_model)\n\n\nCall:\nlm(formula = lnSymptoms ~ Stress, data = stress_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.42889 -0.13568  0.00478  0.09672  0.40726 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.300537   0.033088 129.974  < 2e-16 ***\nStress      0.008565   0.001342   6.382 4.83e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1726 on 105 degrees of freedom\nMultiple R-squared:  0.2795,    Adjusted R-squared:  0.2726 \nF-statistic: 40.73 on 1 and 105 DF,  p-value: 4.827e-09\n\n\n\nCall: tells us the formula we originally input for this model\nResiduals: descriptive stats of the residuals\nCoefficients: provides the estimated values, standard error of the estimates, and NHST of the intercept and beta coefficient. In both cases the null hypothesis is Estimate = 0. Again for the intercept this may or may not be useful. For example you might use it to test whether people who haven’t had a single drink of alcohol have no depression at all. The test of beta tests against the null hypothesis of zero correlation (relationship). All tests are conducted provided the t-value on \\(n-k\\) degrees of freedom (see next bullet point)\nResidual standard error: This is calculated by first getting the sum of the squared residuals and dividing that number by the degrees of freedom of the model. In this case the df equals the number of total observations (107) minus the number of parameters (\\(k\\)) that were calculated by the model in order to generate the predicted values (2). This may be interpreted as a measure of goodness of fit of the model, and perhaps more importantly its predictive value. This value is closely tied to Cohen Chapter 10 “The Variance of the Estimate”, but rather than the denominator being \\(N\\), it is the degrees of freedom for the model. In this way it speaks more directly to the model itself.\nMultiple R-squared and Adjusted R-squared: The coefficient of determination calculated from \\(r\\) or \\(r_{adj}\\). \\(r^2\\) tells us the degree to which our model accounts for observed variance in the observed outcomes. On was to think about this is, to what degree does variation in our predictor explain the observed variation in our outcomes. More on this below.\nFinally, this output gives is the resulting \\(F\\)-ratio and corresponding \\(df\\) for an \\(F\\)-test. This is a significance test for our \\(r^2\\). Does our model explain a significant amount of the variance?"
  },
  {
    "objectID": "week05/5_3_regressions.html#the-coeff.-of-determination-as-index-of-explanatory-value",
    "href": "week05/5_3_regressions.html#the-coeff.-of-determination-as-index-of-explanatory-value",
    "title": "\n21  Regression, pt. 1: Running the model\n",
    "section": "\n21.6 The coeff. of determination as index of explanatory value",
    "text": "21.6 The coeff. of determination as index of explanatory value\nHere we may turn to a (a little bit longer than expected) digression involving J.S. Mill and exact and inexact sciences. One philosophical conceit is that we can never really empirically observe causes, but only effects. That is, I can never truly observe that one billiard ball caused another to move, I can only say with a high degree of certainty that I observed the first billiard ball striking the second and subsequently the second moved at a particular velocity. In order to deal with this epistemological crisis (how can you really ever know anything!!!) we build explanatory models. The degree to which our model is able to account for the range of our previous observations tells us how useful our model parameters (predictors) are in explaining a phenomenon in question.\nFor example, to explain what I observe about the contact and subsequent motion of billiard balls I might build a model that takes into account the mass and velocity (speed and direction) of the first ball, the mass and velocity of the second ball and the point and angle of contact. Presuming I use these as predictors, I can account with varying degrees of success the heading and direction of the balls, or the outcome. In fact, in this example, I’m likely to be very successful in describing most, but not all observed outcomes. That is, there are some variations in the outcomes that just using these parameters does not result in a successful account. I can refine my billiard contact model to include things like the friction caused from the felt, the force of gravity, the angle of the table top, etc., and it’s likely the case that with each new predictor my model becomes better. However at some point I begin to reach diminishing returns (see over-fitting next semester).\nThis is why physics appears to be an exact science (well, mechanical physics at least). They’ve had centuries to develop and refine their models about the mechanical workings of our very local corner of the universe to the point that these models are able to account for a very high degree of variation in observed phenomena. These models typically have a very high \\(r^2\\) and as such tend to have high predictive value. Which is why with some understanding of gravitational force, mass, and a few simple calculations, high school students all over country are able to do this in their physics lab.\nIndeed within some disciplines having an \\(r^2\\) < .9 means you model isn’t good enough. Typically this is the case when the phenomenon under observation is not complex (hope that doesn’t offend). As you move away from physics and chemistry (or at least the low complexity phenomena under each discipline), the explanatory value of models tends to decrease. Us folks in the social sciences, we are moving into inexact-science-land. Weather patterns, economies, ecosystems, and human behaviors are highly complex; in many cases in inordinate number of contributing factors may be present in an observed outcome. As a result degree of variation that these models may account for is typically diminished, especially in the social sciences. For example depending on the phenomena and sub-discipline, an \\(r^2\\) of .3 might mean you’re cooking with gas! In fact, for some phenomena, a reported \\(r^2\\) that is too high might be a cause for suspicion (“they weren’t so much cooking with gas so much as they were cooking the books”).\nAs a final note, its typically poor practice to get a significant \\(r^2\\) and just stop there. Remember, this is just a claim that your model accounts for significantly more variation than 0%. If you were to have a model that accounted for 6% (\\(r^2 = .06\\)) of the variance but was significant (\\(p<.05\\)), you need to ask yourself whether that model is useful at all. It certainly has very little predictive value (I wouldn’t bet my life… or even a few bills on it predicting a future outcome), and its true explanatory value is quite questionable.\n\nFor what it’s worth, this isn’t just a joke. I can point to a paper from my field that was published in a very prominent journal (**cough Psych Science) that reported just this… a \\(r^2\\) of about .08."
  },
  {
    "objectID": "week05/5_3_regressions.html#write-up-and-presentation",
    "href": "week05/5_3_regressions.html#write-up-and-presentation",
    "title": "\n21  Regression, pt. 1: Running the model\n",
    "section": "\n21.7 Write up and presentation",
    "text": "21.7 Write up and presentation\nOur obtained results can be reported in several different ways. However, typically it’s best practice to report both the \\(beta\\) (\\(b\\)) coefficient with corresponding \\(t-test\\); as well as the general statistics about the model including the \\(r^2\\) and corresponding \\(F-test\\). Prior to any reporting of the stats, you also need to articulate the structure of the model.\nI think a good template to start from is:\n\n“[Clearly restate the alternative hypothesis identifying operationalized variables]. To test this hypothesis the data were [what type of analysis?] with [what is your dv] as the outcome and [what is your predictor] as our predictor. The resulting model was significant, \\(r^2\\)=0.XX; \\(F\\)(df1,df2)=F-ratio, \\(p\\)=p-value and revealed a significant relationship between the [covariables], \\(b\\)=XX; \\(t\\)(df)=XX, \\(p\\)=XX$.”\n\nReturning to our stress_symptoms_model:\n\nsummary(stress_symptoms_model)\n\n\nCall:\nlm(formula = lnSymptoms ~ Stress, data = stress_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.42889 -0.13568  0.00478  0.09672  0.40726 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.300537   0.033088 129.974  < 2e-16 ***\nStress      0.008565   0.001342   6.382 4.83e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1726 on 105 degrees of freedom\nMultiple R-squared:  0.2795,    Adjusted R-squared:  0.2726 \nF-statistic: 40.73 on 1 and 105 DF,  p-value: 4.827e-09\n\n\nFor example, with this data you may report it as:\n\n“We hypothesized that increases in self-reported measures of Stress would correspond to increases in the number of self-reported symptoms. Given that our Symptoms data showed a significant violation of the normality assumption, we transformed these scores using a log-transformation (lnSymptoms). To test this hypothesis the data were submitted to a simple linear regression with lnSymptoms as the outcome and Stress as our predictor. The resulting model was significant, \\(r^2\\)=.28; \\(F\\)(1,105)=40.73, \\(p\\)<.001, and revealed a significant positive relationship between the Stress and Symptoms, \\(b\\)=.008; \\(t\\)(105)=6.38, \\(p\\)<.001.”\n\n\n21.7.1 Reporting standardized \\(\\beta\\)\n\nThat said, in many circles it’s typical practice to report the standardized \\(beta\\) coefficients (\\(\\beta\\)). These are the \\(beta\\) coefficients we would have obtained if we had initially converted both our Stress and lnSymptom values to \\(z\\)-scores, and then ran the model using those \\(z\\)-scores. For example, let’s add z-transformed values for both to our original dataframe\n\nstress_data <- stress_data %>% mutate(z_Stress = scale(Stress)[,1],\n                                      z_lnSymptoms = scale(lnSymptoms)[,1])\n\nAnd now run the model (piping to a tidy output)\n\nz_model <- lm(z_lnSymptoms ~ z_Stress, data = stress_data)\nbroom::tidy(z_model)\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic       p.value\n  <chr>           <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept) -7.07e-16    0.0824 -8.58e-15 1.00         \n2 z_Stress     5.29e- 1    0.0828  6.38e+ 0 0.00000000483\n\n\nThat said, you don’t NEED to do the z-transform by hand. We can retroactively obtain these coefficients from raw values by using the lm.beta() function from the lm.beta package (duplicate names, confusing I know). We can then input our model into the lm.beta() function and get the summary of those results. The new column std_estimate is reported in addition to the estimate:\n\npacman::p_load(lm.beta)\n\n# this is the raw data model you ran earlier\nstress_symptoms_model <- lm(lnSymptoms ~ Stress, data = stress_data)\nlm.beta(stress_symptoms_model) %>% broom::tidy()\n\n# A tibble: 2 × 6\n  term        estimate std_estimate std.error statistic   p.value\n  <chr>          <dbl>        <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)  4.30          NA       0.0331     130.   8.09e-118\n2 Stress       0.00856        0.529   0.00134      6.38 4.83e-  9\n\n\nNote that if you are using summary rather than broom::tidy() then the lm.beta output looks like this:\n\nlm.beta(stress_symptoms_model) %>% summary()\n\n\nCall:\nlm(formula = lnSymptoms ~ Stress, data = stress_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.42889 -0.13568  0.00478  0.09672  0.40726 \n\nCoefficients:\n            Estimate Standardized Std. Error t value Pr(>|t|)    \n(Intercept) 4.300537           NA   0.033088 129.974  < 2e-16 ***\nStress      0.008565     0.528656   0.001342   6.382 4.83e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1726 on 105 degrees of freedom\nMultiple R-squared:  0.2795,    Adjusted R-squared:  0.2726 \nF-statistic: 40.73 on 1 and 105 DF,  p-value: 4.827e-09\n\n\nSo amending the example reporting paragraph above:\n\nThe resulting model was significant, \\(r^2\\)=.28; \\(F\\)(1,105)=40.73, \\(p\\)<.001, and revealed a significant positive relationship between the Stress and Symptoms, \\(\\beta =.528\\); \\(t(105)=6.38\\), \\(p<.001\\).”\n\nCoincidentally you may have noticed that this \\(\\beta\\) value is the correlation that we established in the previous walkthrough! Quickly:\n\ncor(stress_data$Stress, stress_data$lnSymptoms)\n\n[1] 0.5286565\n\n\nAs explained by Barry Cohen (in his useful textbook that I didn’t have you buy…):\n\nif you have already tested the Pearson’s \\(r\\) against zero, there is no need to test the corresponding \\(b\\), as the \\(t\\) values will be exactly the same (except for rounding error) for both tests”. More, a test on \\(b\\) is equivalent to a test on \\(r\\) in the one-predictor case. If it is true that \\(X\\) and \\(Y\\) are related, then it must also be true that \\(Y\\) varies with \\(X\\)—that is, that the slope is nonzero. This suggests that a test on \\(b\\) (beta coefficient) will produce the same answer as a test on \\(r\\), and we could dispense with a test for \\(b\\) altogether.\n\nThe reverse is also true. In the single predictor case, if one has a test of \\(b\\) this obviates the need to run an independent test of \\(r\\). The output of lm provides us with a simple test that gives us the significance of the beta coefficient, \\(b\\)."
  },
  {
    "objectID": "week05/5_4_plotting.html#load-in-the-data",
    "href": "week05/5_4_plotting.html#load-in-the-data",
    "title": "\n22  Plotting regression data\n",
    "section": "\n22.1 Load in the data",
    "text": "22.1 Load in the data\nFirst let’s reload our stress_data in case you’ve reset your environment:\n\nstress_data <- read_table(\"https://www.uvm.edu/~statdhtx/methods8/DataFiles/Tab9-2.dat\")\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  ID = col_double(),\n  Stress = col_double(),\n  Symptoms = col_double(),\n  lnSymptoms = col_double()\n)"
  },
  {
    "objectID": "week05/5_4_plotting.html#create-a-simple-scatter-plot",
    "href": "week05/5_4_plotting.html#create-a-simple-scatter-plot",
    "title": "\n22  Plotting regression data\n",
    "section": "\n22.2 Create a simple scatter plot",
    "text": "22.2 Create a simple scatter plot\nAnd now to create a simple scatter plot of lnSymptoms as a function of Stress\n\n# creating a new plot with our original scatter plot as a template:\nscatter_plot <- ggplot(stress_data, aes(x = Stress, y = lnSymptoms)) +\n  geom_point() + # creates scatter plot\n  theme_cowplot() + \n  xlab(\"Stress\") + ylab(\"lnSymptoms\") + \n  theme_cowplot()\n\nshow(scatter_plot)"
  },
  {
    "objectID": "week05/5_4_plotting.html#add-a-regression-line",
    "href": "week05/5_4_plotting.html#add-a-regression-line",
    "title": "\n22  Plotting regression data\n",
    "section": "\n22.3 Add a regression line",
    "text": "22.3 Add a regression line\nFrom this scatterplot we can overlay a regression line as so\n\nregression_plot <- scatter_plot +\n# adding a regression line\n  geom_smooth(method=lm, level = 0.95, color=\"black\", se = TRUE)\n\nshow(regression_plot)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nTeasing apart that last line: geom_smooth(method=lm, level = 0.95, color=\"black\", se = TRUE)\nTo add a simple regression line to a scatter plot we can use the geom_smooth() function. geom_smooth() takes the following arguments:\n-method: what kind of regression line do you want to create? in this case we are performing a linear regression, so lm.\n-mapping: if you did not specify your aes() in the original call you need to specify them here (whats on the x-axis, y-axis, etc). Additionally if you are performing custom mappings this is the place.\n-level: create a ribbon specifying a confidence interval of each predicted value (here I’m using 95% CI).\n-se: do you want a shaded ribbon to be shown on the plot (TRUE of FALSE)\n-color: what color do you want the line.\n-linetype: solid line? dotted line? dashed line? default is solid.\nNote that geom_smooth() is useful if one is doing very simple regression plots. If one has a more complicated model structure, then you may need to use the attributes of the model to create a data frame that works for you. I’ll coordinate with my analog for the Spring semester and see how they want to approach this, though I’m thinking at the very least I’ll have a walkthrough at the tail end of the semester that shows how one might accomplish this.\nThe shaded ribbon around the line of regression represents the 95% CI of the estimate at each value of the predictor. The size of the CI may be adjusted by using the level parameter in geom_smooth().\n\n22.3.1 Final code example\nPerforming the previous two steps in a single call. Also, personally I like to lighten-up the scatter plot point to really emphasize the regression line. This can be accomplished by modifying the color of the point (line 2 below)\n\nregression_plot <- ggplot(stress_data, aes(x = Stress, y = lnSymptoms)) +\n  geom_point(color = \"gray\") + # creates scatter plot\n  # add regression line\n  geom_smooth(method=lm, level = 0.95, color=\"black\", se = TRUE) +\n  theme_cowplot()\n\nshow(regression_plot)\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "week05/5_4_plotting.html#using-ggpmisc-to-display-additional-info",
    "href": "week05/5_4_plotting.html#using-ggpmisc-to-display-additional-info",
    "title": "\n22  Plotting regression data\n",
    "section": "\n22.4 Using ggpmisc to display additional info",
    "text": "22.4 Using ggpmisc to display additional info\nIn theory, the regression_plot created above should suffice, and the example above only relies upon ggplot making it fairly robust. Sometimes, however, it’s good to convey additional info about the model in the plot. This usually is the \\(R^2\\) and the equation for the model. Using ggplot() alone, this is a little more complex. But has been made easier by someone in the community “building a package for that”. See this exchange for details.\nBorrowing the final example from the site above, we can use the ggpmisc package to make life easier. In fact ggpmisc() allows us to create the same plot above using a slightly different code:\n\n# load in ggpmisc\npacman::p_load(ggpmisc)\n\n\nregression_plot <- ggplot(stress_data, aes(x = Stress, y = lnSymptoms)) +\n  geom_point(color = \"gray\", size = .5) + \n  stat_poly_line(color=\"black\") + #ggpmisc replacement for geom_smooth()\n  theme_cowplot()\n  \nshow(regression_plot)\n\n\n\n\nAnd now to add \\(R^2\\) and the equation of the line to our previously created regression_plot\n\nregression_plot + # our regression plot from above\n  stat_poly_eq(aes(label = paste(after_stat(eq.label),\n                                 after_stat(rr.label), sep = \"*\\\", \\\"*\")))\n\n\n\n\nComparing this plot to our stress_symptoms_model output:\n\nstress_symptoms_model <- lm(Stress~lnSymptoms, data = stress_data)\ntidy(stress_symptoms_model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic       p.value\n  <chr>          <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)   -125.      22.9      -5.45 0.000000339  \n2 lnSymptoms      32.6      5.11      6.38 0.00000000483\n\n\nFrom this plot it becomes apparent that the model coefficients obtained above represent the values of the slope and intercept of the regression line.\n\nThe beta coefficient conveys the slope of the line—unit change in outcome per unit change in predictor. Typically this is expressed as b in text or B in SPSS. In R output, the beta estimate is tied to the name of the corresponding predictor variable. In our case that’s Stress.\nThe intercept tells us what the value of the outcome would be if the predictor was 0. In most instances the intercept is not terribly useful. For example if you were looking at the relationship between height and weight it would make no sense to concern yourself with instances where height is absolutely 0. Obviously, a caveat here is if the value 0 is meaningful for your analysis. For example, perhaps you are looking at depression as a function of alcohol consumption and want to explicitly say something about people that have never had a single drink in their life.\n\nOne way of making a meaningless intercept meaningful is by centering your data (subtracting every value of your predictor from a constant). For example, if you center around the mean, the intercept will tell you the predicted value of the outcome variable at the mean value of the predictor.\nJust for giggles, lets plot our mean-centered data. We mean center Stress by subtracting each value of Stress from its mean:\n\nggplot(data = stress_data, aes(x=(lnSymptoms-mean(lnSymptoms)), y=Stress)) +\n  geom_point(color = \"black\", size = 1, shape = 19) +\n  geom_smooth(method=lm, level = .95, color=\"black\") +\n  theme_cowplot() + \n  xlab(\"Stress\") + \n  ylab(\"lnSymptoms\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nas you can see slope doesn’t change, but the values become centered around the intercept 0. In this case 0 represents the means Stress score."
  },
  {
    "objectID": "week05/5_5_diagnostics.html#testing-the-residuals",
    "href": "week05/5_5_diagnostics.html#testing-the-residuals",
    "title": "\n23  Regression model diagnostics\n",
    "section": "\n23.1 Testing the residuals",
    "text": "23.1 Testing the residuals\nAn important assumption test for our model is that the residuals are normally distributed. In fact some argue that this is more important than than having normal distributions in the raw data itself. Quickly, we can test this using the regular methods from the normal distribution walkthrough (see earlier this week).\nAssuming we have loaded in our data and created our model:\n\n# load in data\nstress_data <- read_table(\"https://www.uvm.edu/~statdhtx/methods8/DataFiles/Tab9-2.dat\")\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  ID = col_double(),\n  Stress = col_double(),\n  Symptoms = col_double(),\n  lnSymptoms = col_double()\n)\n\n# linear model\nstress_symptoms_model <- lm(lnSymptoms ~ Stress, data = stress_data)\n\n\nhist(stress_symptoms_model$residuals,breaks = 10)\ncar::qqPlot(stress_symptoms_model$residuals) %>% show()\npsych::describe(stress_symptoms_model$residuals)\n\nQuestion 9: Run a Shapiro-Wilkes test (see earlier walkthrough) on our stress_symptoms_model residuals. How do we feel about the residuals here?\n\n# run a S-W on the residuals"
  },
  {
    "objectID": "week05/5_5_diagnostics.html#diagnostic-plots---base-method",
    "href": "week05/5_5_diagnostics.html#diagnostic-plots---base-method",
    "title": "\n23  Regression model diagnostics\n",
    "section": "\n23.2 Diagnostic plots - base method",
    "text": "23.2 Diagnostic plots - base method\nWe can also submit the stress_symptoms_model to a battery of diagnostic plots. See the Flora Chapter 1 section on Basic Regression Diagnostic Concepts for an ex-plainer on each of these plots.\n\nplot(stress_symptoms_model)\n\nproduces a host of successive plots, including a plot of “Residuals v. Fitted”, A Q-Q plot that includes possible outliers, and a leverage plot."
  },
  {
    "objectID": "week05/5_5_diagnostics.html#diagnostic-plots---ggplot-method-ggfortify",
    "href": "week05/5_5_diagnostics.html#diagnostic-plots---ggplot-method-ggfortify",
    "title": "\n23  Regression model diagnostics\n",
    "section": "\n23.3 Diagnostic plots - ggplot method (ggfortify)",
    "text": "23.3 Diagnostic plots - ggplot method (ggfortify)\nThe above function produces the plots in succession using base R graphics. There is a ggplot based alternative that uses the ggfortify package, that produces all 4 diagnostic plots in a single figure. See this link for more details.\n\npacman::p_load(ggfortify)\n\n# you mush have ggfortify loaded to call autoplot\nautoplot(stress_symptoms_model)"
  },
  {
    "objectID": "week05/5_5_diagnostics.html#diagnostic-plots---performance-method",
    "href": "week05/5_5_diagnostics.html#diagnostic-plots---performance-method",
    "title": "\n23  Regression model diagnostics\n",
    "section": "\n23.4 Diagnostic plots - performance method",
    "text": "23.4 Diagnostic plots - performance method\nWe can get a similar battery of diagnostics using the performance library. We can just submit our model to performance::check_model. The nice thing about this method is the plot reminds you of what to look for. Because this function generates so many plots, it may be useful to execute this in your Console (as well as your qmd) to see the individual plots better.\n\nperformance::check_model(stress_symptoms_model)"
  },
  {
    "objectID": "week05/5_5_diagnostics.html#testing-heteroscedasticity",
    "href": "week05/5_5_diagnostics.html#testing-heteroscedasticity",
    "title": "\n23  Regression model diagnostics\n",
    "section": "\n23.5 Testing Heteroscedasticity",
    "text": "23.5 Testing Heteroscedasticity\nThe assumption of Heteroscedasticity and the assumption of homogeneity of variance are one in the same. We typically refer to homogeneity of variance when we have categorical predictors. For example, next week this will involve comparing the variance of scores (their spread) in Group A against Group B.\n\ntwo_groups <- tibble(groupA = rnorm(n = 30,mean = 150,sd = 10),\n                     groupB = rnorm(n = 30, mean = 150, sd = 45)) %>% \n  pivot_longer(cols = c(\"groupA\", \"groupB\"), names_to = \"group\",values_to = \"scores\")\n  \nggplot(two_groups, aes(x=group, y=scores)) + \n  stat_summary(fun = \"mean\", size=1, fill=\"lightgray\", geom = \"bar\") +\n  geom_point() +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nIn this case, it’s apparent that the spread of scores for groupB is wider than groupA. Note that for categorical predictors we can test this using the car::leveneTest (we’ll revisit next week)\n\ncar::leveneTest(scores~group,two_groups)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value    Pr(>F)    \ngroup  1  26.002 3.919e-06 ***\n      58                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHowever, with continuous predictors our data doesn’t lump down neatly into groups. Consider this example:\n\nset.seed(1)\n\nheteroscedastic_data <- tibble(predictor = runif(500, 0, 1),\n                          outcome = 5 * rnorm(500, predictor, predictor)\n)\n\nggplot(heteroscedastic_data, aes(x=predictor, y=outcome)) + \n  geom_point(color=\"gray\") + \n  geom_smooth(method=lm, level = 0.95, color=\"black\", se = FALSE) + \n  theme_cowplot()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWe see here that the outcome becomes more variable as the predictor grows larger. This plot is demonstrating a violation of Heteroscedasticity The black line represents the line of best fit derived from running lm. Together, this highlights the central problem… in this case the error (residuals) increase as a function of the predictor. Among other things this would this may result in non-normal residuals.\nA comprehensive visual inspection may be performed by calling the autoplot function on the linear model:\n\nlm(outcome~predictor, heteroscedastic_data) %>% autoplot()\n\n\n\n\nIn this case, the important plots are the first three. If Heteroscedasticity were not present, plots (1) and (3) would have a flat horizontal red line indicating the average residuals of this regression. As the residuals are clumped in areas, we can say that Heteroscedasticity is evident in this data set. Plot (2), our qqplot, also is influenced by this Heteroscedasticity, as the distribution of errors is systematic rather than random (i.e., another confounding relationship is affecting error).\nSeveral tests for Heteroscedasticity can be accessed from the olsrr package. For example, a simple test for Heteroscedasticity is the Breusch Pagan Test. Testing our stress_symptoms_model remember that the null hypothesis for this test is that the data is NOT heteroscedastic, therefore a p-value (here evaluated along the \\(\\chi^2\\) distribution (Chi2) that is less than .05 means that the model has violated this assumption. In this case our stress_symptoms_model check out fine:\n\npacman::p_load(olsrr)\n\nalso installing the dependency 'goftest'\n\n\n\nThe downloaded binary packages are in\n    /var/folders/7k/6v7xmh35305_vjzy660s9_q40000gs/T//RtmpEbFMZZ/downloaded_packages\n\n\n\nolsrr installed\n\nols_test_breusch_pagan(stress_symptoms_model)\n\n\n Breusch Pagan Test for Heteroskedasticity\n -----------------------------------------\n Ho: the variance is constant            \n Ha: the variance is not constant        \n\n                 Data                  \n --------------------------------------\n Response : lnSymptoms \n Variables: fitted values of lnSymptoms \n\n        Test Summary         \n ----------------------------\n DF            =    1 \n Chi2          =    1.434746 \n Prob > Chi2   =    0.2309914 \n\n\nAnd now taking a look at our heteroscedastic_data, we see a violation.\n\nheteroscedastic_data_model <- lm(outcome~predictor, heteroscedastic_data)\nols_test_breusch_pagan(heteroscedastic_data_model)\n\n\n Breusch Pagan Test for Heteroskedasticity\n -----------------------------------------\n Ho: the variance is constant            \n Ha: the variance is not constant        \n\n               Data                 \n -----------------------------------\n Response : outcome \n Variables: fitted values of outcome \n\n         Test Summary           \n -------------------------------\n DF            =    1 \n Chi2          =    193.0889 \n Prob > Chi2   =    6.731713e-44"
  },
  {
    "objectID": "week05/5_6_example_report.html#step-1-load-in-packages",
    "href": "week05/5_6_example_report.html#step-1-load-in-packages",
    "title": "\n24  Example Report\n",
    "section": "\n24.1 Step 1: Load in packages",
    "text": "24.1 Step 1: Load in packages\n\npacman::p_load(tidyverse, psych, ggpmisc, ggfortify, cowplot)"
  },
  {
    "objectID": "week05/5_6_example_report.html#step-2-loading-in-the-data",
    "href": "week05/5_6_example_report.html#step-2-loading-in-the-data",
    "title": "\n24  Example Report\n",
    "section": "\n24.2 Step 2: loading in the data",
    "text": "24.2 Step 2: loading in the data\nNote: I’m just using the Stress and Health data from our previous walkthrough examples\n\nstress_data <- read_table(\"https://www.uvm.edu/~statdhtx/methods8/DataFiles/Tab9-2.dat\")\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  ID = col_double(),\n  Stress = col_double(),\n  Symptoms = col_double(),\n  lnSymptoms = col_double()\n)"
  },
  {
    "objectID": "week05/5_6_example_report.html#step-3-plotting-the-data",
    "href": "week05/5_6_example_report.html#step-3-plotting-the-data",
    "title": "\n24  Example Report\n",
    "section": "\n24.3 Step 3: plotting the data",
    "text": "24.3 Step 3: plotting the data\nNote: I often start with just a raw plot to get a feel for the data. I worry about adding all the other features and formatting once I’m ready for the write-up\n\nstress_plot <- ggplot(data = stress_data, aes(x = Stress, y = lnSymptoms)) +\n  geom_point()\n\nstress_plot\n\n\n\n\nIf there was anything strange or notable in the plot, I would note it here. For example I might note that at first glance there appears to be a positive relationship between Stress and lnSymptoms, and that there aren’t any data points that appear to be outstanding or potential outliers."
  },
  {
    "objectID": "week05/5_6_example_report.html#step-4-descriptive-stats",
    "href": "week05/5_6_example_report.html#step-4-descriptive-stats",
    "title": "\n24  Example Report\n",
    "section": "\n24.4 Step 4: Descriptive stats",
    "text": "24.4 Step 4: Descriptive stats\nNote: even though means are not especially important in this analysis (i.e., I’m not worried about comparing the mean value of lnSymptoms to the mean of Stress), it may be worth it to look at other measures related to our scores that convey information about variability (sd), skew, and kurtosis. Primarily here, I’m just looking for red flags. FWIW noting jumps out here, so we are good to move on.\n\npsych::describe(stress_data)\n\n           vars   n  mean    sd median trimmed   mad   min    max  range skew\nID            1 107 54.00 31.03  54.00   54.00 40.03  1.00 107.00 106.00 0.00\nStress        2 107 21.29 12.49  20.00   20.49 11.86  1.00  58.00  57.00 0.62\nSymptoms      3 107 90.33 18.81  88.00   88.87 17.79 58.00 147.00  89.00 0.74\nlnSymptoms    4 107  4.48  0.20   4.48    4.48  0.19  4.06   4.99   0.93 0.21\n           kurtosis   se\nID            -1.23 3.00\nStress        -0.19 1.21\nSymptoms       0.45 1.82\nlnSymptoms    -0.28 0.02"
  },
  {
    "objectID": "week05/5_6_example_report.html#step-5-building-the-model",
    "href": "week05/5_6_example_report.html#step-5-building-the-model",
    "title": "\n24  Example Report\n",
    "section": "\n24.5 Step 5: Building the model",
    "text": "24.5 Step 5: Building the model\nNote: you build the model here, BUT I wouldn’t evaluate or interpret it’s output just yet. Save that until after your diagnostics!!!\n\nstress_symptoms_model <- lm(lnSymptoms~Stress, data = stress_data)"
  },
  {
    "objectID": "week05/5_6_example_report.html#step-6-model-diagnostics",
    "href": "week05/5_6_example_report.html#step-6-model-diagnostics",
    "title": "\n24  Example Report\n",
    "section": "\n24.6 Step 6: Model diagnostics",
    "text": "24.6 Step 6: Model diagnostics\nI will now do a visual diagnostic of the model residuals\n\n# remember you need ggfortify to do this\nautoplot(stress_symptoms_model)\n\n\n\n\nThe visual inpection here doesn’t raise any red flags."
  },
  {
    "objectID": "week05/5_6_example_report.html#step-7-evaluate-the-model",
    "href": "week05/5_6_example_report.html#step-7-evaluate-the-model",
    "title": "\n24  Example Report\n",
    "section": "\n24.7 Step 7: Evaluate the model",
    "text": "24.7 Step 7: Evaluate the model\nNow it’s time to take a look at our model:\n\nsummary(stress_symptoms_model)\n\n\nCall:\nlm(formula = lnSymptoms ~ Stress, data = stress_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.42889 -0.13568  0.00478  0.09672  0.40726 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.300537   0.033088 129.974  < 2e-16 ***\nStress      0.008565   0.001342   6.382 4.83e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1726 on 105 degrees of freedom\nMultiple R-squared:  0.2795,    Adjusted R-squared:  0.2726 \nF-statistic: 40.73 on 1 and 105 DF,  p-value: 4.827e-09"
  },
  {
    "objectID": "week05/5_6_example_report.html#step-8-apa-camera-ready-write-up",
    "href": "week05/5_6_example_report.html#step-8-apa-camera-ready-write-up",
    "title": "\n24  Example Report\n",
    "section": "\n24.8 Step 8: APA camera ready write-up",
    "text": "24.8 Step 8: APA camera ready write-up\nThis has 2 parts\n\ncamera ready figure with caption\ndata-analysis write-up\n\n(Also, FWIW, in usually hide the code in this section using the #| echo: false argument in the chunk options. This way you just see the plot and written parts with no interruption. See here for examples.)\nFirst the camera-ready plot, including the regression line and equation. Also be sure to type in a caption below your plot (e.g., Figure 1):\n\n# remember you need ggpmisc loaded to do the line and equation\n# and cowplot to APAify it\n\nggplot(data = stress_data, aes(x = Stress, y = lnSymptoms)) + \n  geom_point(color=\"gray\", size = 0.5) + \n  stat_poly_line(level = .95, color = \"black\") +\n  stat_poly_eq(aes(label = paste(after_stat(eq.label),\n                                 after_stat(rr.label), sep = \"*\\\", \\\"*\"))) +\n  theme_cowplot()\n\n\n\n\nFigure 1. Natural-log of the number of symptoms as predicted by stress scores\nNext the write-up:\nWe hypothesized that increases in self-reported measures of Stress would correspond to increases in the number of self-reported symptoms. Given that our Symptoms data showed a significant violation of the normality assumption, we transformed these scores using a log-transformation (lnSymptoms). To test this hypothesis the data were submitted to a simple linear regression with lnSymptoms as the outcome and Stress as our predictor. The resulting model was significant, \\(r^2\\)=.28; \\(F\\)(1,105)=40.73, \\(p\\)<.001, and revealed a significant positive relationship between the Stress and Symptoms, \\(b\\)=.008; \\(t\\)(105)=6.38, \\(p\\)<.001."
  },
  {
    "objectID": "week05/5_6_example_report.html#step-9-turn-it-in",
    "href": "week05/5_6_example_report.html#step-9-turn-it-in",
    "title": "\n24  Example Report\n",
    "section": "\n24.9 Step 9: Turn it in!!",
    "text": "24.9 Step 9: Turn it in!!"
  },
  {
    "objectID": "week06/6_1_t-test.html#things-to-consider-before-running-the-t-test",
    "href": "week06/6_1_t-test.html#things-to-consider-before-running-the-t-test",
    "title": "\n25  Testing differences in means / t-test\n",
    "section": "\n25.1 Things to consider before running the t-test",
    "text": "25.1 Things to consider before running the t-test\nBefore running a t.test there are a few practical and statistical considerations that must be taken. In fact, these considerations extend to every type of analysis that we will encounter for the remainder of the semester (and indeed the rest of your career) so it would be good to get in the habit of running through your checks. In what proceeds here I will walk step by step with how I condunct a t.test (while also highlighting certain decision points as they come up).\n\n25.1.1 What is the nature of your sample data?\nIn other words where is the data coming from? Is it coming from a single sample of participants? Is it coming from multiple samples of the SAME participants? Is it coming from multiple groups of participants. This will not only determine what analysis you choose to run, but in also how you go about the business of preparing to run this analysis. Of course, truth be told this information should already be known before you even start collecting your data, which reinforces an important point, your central analyses should already be selected BEFORE you start collecting data! As you design your experiments you should do so in a way in which the statistics that you run are built into the design, not settled upon afterwards. This enables you to give the tests you perform the most power, as you are making predictions about the outcomes of your test a priori. This will become a major theme on the back half of the class, but best to introduce it now.\nFor this week, it will determine what test we will elect to perform. Let’s grab some sample data from an experiment by Hand, et al. (1994)\n\nHand, et al., 1994, reported on family therapy as a treatment for anorexia. There were 17 girls in this experiment, and they were weighed before and after treatment. The weights of the girls, in pounds, is provided in the data below:\n\n\nanorexia_data <- read_delim(\"https://www.uvm.edu/~statdhtx/methods8/DataFiles/Tab7-3.dat\", \n                     \"\\t\", escape_double = FALSE, trim_ws = TRUE)\n\nRows: 17 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): ID\ndbl (2): Before, After\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nSo what is known: we have 17 total participants from (hypothetically) the same population that are measured twice (once Before treatment, and once After treatment). Based upon the experimental question we need to run a paired-sample (matched-sample) test. (Although I’ll use this data to provide an example of a one-sample test later on).\n\n25.1.2 What is the structure of your data file?\nBefore doing anything you should always take a look at your data:\n\nshow(anorexia_data)\n\n# A tibble: 17 × 3\n   ID    Before After\n   <chr>  <dbl> <dbl>\n 1 01      83.8  95.2\n 2 02      83.3  94.3\n 3 03      86    91.5\n 4 04      82.5  91.9\n 5 05      86.7 100. \n 6 06      79.6  76.7\n 7 07      76.9  76.7\n 8 08      94.2 102. \n 9 09      73.4  94.9\n10 10      80.5  75.2\n11 11      81.6  77.8\n12 12      82.1  95.5\n13 13      77.6  90.7\n14 14      83.5  92.6\n15 15      89.9  93.8\n16 16      86    91.7\n17 17      87.3  98  \n\n\nSo what do we have here, three columns:\n\n\nID: the participant number\n\nBefore: participants’ weights before treatment\n\nAfter: participants’ weights after treatment\n\nMost important for present purposes this data is in WIDE format—each line represents a participant. While this might be intuitive for tabluar visualization, many statistical softwares prefer when LONG format, where each line represents a single observation (or some mixed of WIDE and LONG like SPSS).\nI spoke a little bit about this issue in Week 2, Walkthrough 0.\n\n25.1.2.1 Getting data from WIDE to LONG\nSo the data are in WIDE format, each line has multiple observations of data that are being compared. Here both Before scores and After scores are on the same line. In order to make life easier for analysis and plotting in ggplot, we need to get the data into LONG format (Before scores and After scores are on different lines). This can be done using the pivot_longer() function from the tidyr package.\nBefore gathering, one thing to consider is whether or not you have a column that defines each subject. In this case we have ID. This tells R that these data are coming from the same subject and will allow R to connect these data when performing analysis. That said, for t.test() this is not crucially important—t.test() assumes that the order of lines represents the order of subjects, e.g., the first Before line is matched to the first After line. Later on when we are doing ANOVA, however, this participant column will be important an we will need to add if it is missing.\nUsing pivot_longer(): This function takes a number of arguments, but for us right now, the most important are data: your dataframe; cols: which columns to gather; names_to: what do you want the header of the collaped nonminal variables to be? Here, we might ask what title would encapsulate both Before and After. I’ll choose treatment ; values_to: what do the values represent, here I choose weight. I’m just going to overwrite the original data frame:\n\nanorexia_data <- pivot_longer(anorexia_data,cols = c(\"Before\",\"After\"),names_to = \"treatment\", values_to = \"weight\")\nanorexia_data\n\n# A tibble: 34 × 3\n   ID    treatment weight\n   <chr> <chr>      <dbl>\n 1 01    Before      83.8\n 2 01    After       95.2\n 3 02    Before      83.3\n 4 02    After       94.3\n 5 03    Before      86  \n 6 03    After       91.5\n 7 04    Before      82.5\n 8 04    After       91.9\n 9 05    Before      86.7\n10 05    After      100. \n# ℹ 24 more rows\n\n\nOk data is structured correctly, on to the next step.\n\n25.1.3 Testing assumptions\nRemember that you should always test to see if the data fit the assumptions of the test you intend to perform. In this case, we need to assess two things:\n\n25.1.3.1 Is the data normally distributed?\nKnowing the design of your experiment also has implications for testing your assumptions. For example, whether you have a paired (matched) sample design (e.g., two samples from the same participants) or an independent sample design (e.g., two groups) determines how you go about the business of testing the normality assumption. If you have an independent samples test, you test each sample separately, noting measures of skew, kurtosis, inspecting the qqPlot, and Shapiro-Wilkes test (though acknowledging that SW is very sensitive). However, if you are running a paired (matched) samples test, you need to be concerned with the distribution of the difference scores. In the present example we are comparing participants’ weights Before treatment to their weight After. This is a paired design, so I need to test the differences between each participant’s Before and After for normality.\nFirst, let me filter() my data accordingly for Before and After (essentially creating separate vectors for each condition):\n\nbeforeTreatment <- filter(anorexia_data, treatment==\"Before\") \nafterTreatment <- filter(anorexia_data, treatment==\"After\") \n\nAnd now compute the difference scores, and run my assumption tests:\n\ndiffWeights <- beforeTreatment$weight - afterTreatment$weight\n\npsych::describe(diffWeights)\n\n   vars  n  mean   sd median trimmed  mad   min max range skew kurtosis   se\nX1    1 17 -7.26 7.17   -9.1   -7.15 5.93 -21.5 5.3  26.8 0.18    -0.77 1.74\n\ncar::qqPlot(diffWeights)\n\n\n\n\n[1]  9 10\n\nshapiro.test(diffWeights)\n\n\n    Shapiro-Wilk normality test\n\ndata:  diffWeights\nW = 0.9528, p-value = 0.5023\n\n\nWhat conclusions might we draw about normality?\n\n25.1.4 Getting the descriptive stats and plotting the means.\nFinally, as we will be performing a test of difference in means, it would be a good idea to get descriptive measures of means and variability for each group. Indeed, these data were already obatined when we used psych::describe() to assess the normality of each sample. Here I’ll just do it again to get these values:\n\npsych::describeBy(anorexia_data$weight,group = anorexia_data$treatment)\n\n\n Descriptive statistics by group \ngroup: After\n   vars  n  mean   sd median trimmed  mad  min   max range  skew kurtosis   se\nX1    1 17 90.49 8.49   92.6   90.77 3.85 75.2 101.6  26.4 -0.74    -0.93 2.06\n------------------------------------------------------------ \ngroup: Before\n   vars  n  mean   sd median trimmed  mad  min  max range skew kurtosis   se\nX1    1 17 83.23 5.02   83.3   83.15 4.15 73.4 94.2  20.8 0.15    -0.29 1.22\n\n\nTypically along with the mean, you need to report a measure of variability of your sample. This can be either the SD, SEM, or if you choose the 95% CI, although this is more rare in the actual report. See the supplied HW example and APA examples for conventions on how to report these in your results section.\n\n25.1.4.1 Plotting in ggplot\nI’ve mentioned the limits and issues with plotting bar plots, but they remain a standard, so we will simply proceed using these plots. But I’ll note that boxplots, violin plots, bean plots, and pirate plots are all modern alternatives to bar plots and are easy to execute in ggplot(). Try a Google search.\nIn the meantime, to produce a bar plot in R we simply modify a few of the arguments that we are familiar width.\nHere is the code for plotting these two groups:\n\nggplot(data = anorexia_data, aes(x=treatment, y=weight)) +\n  stat_summary(fun = \"mean\", geom = \"col\") + \n  stat_summary(fun.data = \"mean_se\", geom = \"errorbar\", width = .1) +\n  scale_y_continuous(expand = c(0,0)) + \n  theme_cowplot()\n\n\n\n\nBreaking this down line-by-line:\n\nggplot(data = anorexia_data, aes(x=treatment, y=weight)): standard fare for starting a ggplot.\nstat_summary(fun = \"mean\", geom = \"col\"): stat_summary() gets summary statistics and projects them onto the geom of your choice. In this case we are getting the mean values, fun = \"mean\" and using them to create a column plot geom = \"col\" .\nstat_summary(fun.data = \"mean_se\", geom = \"errorbar\", width = .1) : here we are creating error bars, geom = \"errorbar\". Important to note here is that error bars require knowing three values: mean, upper limit, and lower limit. Whenever you are asking for a single value, like a mean, you use fun. When multiple values are needed you use fun.data. Here fun.data = \"mean_se\" requests Standard error bars. Other alternatives include 95% CI \"mean_cl_normal\" and Standard deviation \"mean_sdl\". The width argument adjusts the width of the error bars.\nscale_y_continuous(expand = c(0,0)): Typically R will do this strange thing where it places a gap bewteen the data and the x-axis. This line is a hack to remove this default. It says along the y-axis add 0 expansion (or gap).\ntheme_cowplot(): quick APA aesthetics.\n\nYou may also feel that the zooming factor is off. This may especially be true in cases where there is little visual discrepency between the bars. To “zoom in” on the data you can use coord_cartesian(). For example, you might want to only show the range between 70 lbs and 100 lbs. When doing this, be careful not to truncate the upper limits of your bars and importantly your error bars.\n\nggplot(data = anorexia_data, aes(x=treatment, y=weight)) +\n  stat_summary(fun = \"mean\", geom = \"col\") + \n  stat_summary(fun.data = \"mean_se\", geom = \"errorbar\", width = .1) +\n  scale_y_continuous(expand = c(0,0)) + \n  theme_cowplot() +\n  coord_cartesian(ylim = c(70,100))\n\n\n\n\nAdditionally, to get this into true APA format I would need to adjust my axis labels. Here capitalization is needed. Also, because the weight has a unit measure, I need to be specific about that:\n\nggplot(data = anorexia_data, aes(x=treatment, y=weight)) +\n  stat_summary(fun = \"mean\", geom = \"col\") + \n  stat_summary(fun.data = \"mean_se\", geom = \"errorbar\", width = .1) +\n  scale_y_continuous(expand = c(0,0)) + \n  theme_cowplot() +\n  coord_cartesian(ylim = c(70,100)) +\n  xlab(\"Treatment\") + \n  ylab(\"Weight (lbs)\")\n\n\n\n\nFinally, you may have notice that the order of Treatment on the plot is opposite of what we might like to logically present. In this case the “After” data comes prior to the “Before” data on the x-axis. This is because R defaults to alphabetical order when loading in data. To correct this I can use scale_x_discrete() and specify the order that I want in limits:\n\nggplot(data = anorexia_data, aes(x=treatment, y=weight)) +\n  stat_summary(fun = \"mean\", geom = \"col\") + \n  stat_summary(fun.data = \"mean_se\", geom = \"errorbar\", width = .1) +\n  scale_y_continuous(expand = c(0,0)) + \n  theme_cowplot() +\n  coord_cartesian(ylim = c(70,100)) +\n  xlab(\"Treatment\") + \n  ylab(\"Weight (lbs)\") + \n  scale_x_discrete(limits=c(\"Before\",\"After\"))\n\n\n\n\nAlternatively I can correct the order of the levels of a factor within the dataframe itself using fct_relevel() (this gets loaded with tidyverse). Note that here I am overwriting the original treatment column. If you do this proceed at your own risk! You could also just mutate a new column if you would rather not overwrite.\n\nanorexia_data$treatment <- fct_relevel(anorexia_data$treatment, \"Before\", \"After\")\n\nggplot(data = anorexia_data, aes(x=treatment, y=weight)) +\n  stat_summary(fun = \"mean\", geom = \"col\", \n               fill = \"lightgray\", color = \"black\") + # adding some color mods here.\n  stat_summary(fun.data = \"mean_se\", geom = \"errorbar\", width = .1) +\n  scale_y_continuous(expand = c(0,0)) + \n  theme_cowplot() +\n  coord_cartesian(ylim = c(70,100)) +\n  xlab(\"Treatment\") + \n  ylab(\"Weight (lbs)\")\n\n\n\n# note that I don't have to do the `scale_x_discrete(limits=c(\"Before\",\"After\"))` correction\n\nAll good (well maybe check with Sierra first)! One other thing to consider (although please do not worry about it here) is the recent argument that when dealing with repeated measures data you need to adjust you error bars. See this pdf by Richard Morey (2005) for more information on this issue. We’ll revisit this issue when running Repeated Measures ANOVA.\n\n25.1.4.2 An aside… other types of plots\nAs I mentioned barplots (especially those that use standard error bars) have more recently come under criticism for “hiding” the true name of the data. There is currently a movement to make data more transparent using other kinds of plots that convey more information about your sample. For example let’s contrast out barplot from above with a combination “pointrange” and violin plot. The pointrange simply provides the mean as a point with error bars extending as specified (here I choose standard error). The violin plot is essentially a histogram of the data turned on its side, centered on the mean, and then mirrored… it gives us info about the TRUE distribution of scores.\nLet’s take a look side by side\n\nbarplot <- ggplot(data = anorexia_data, aes(x=treatment, y=weight)) +\n  stat_summary(fun = \"mean\", geom = \"col\", \n               fill = \"lightgray\", color = \"black\") + # adding some color mods here.\n  stat_summary(fun.data = \"mean_se\", geom = \"errorbar\", width = .1) +\n  scale_y_continuous(expand = c(0,0)) + \n  theme_cowplot() +\n  coord_cartesian(ylim = c(70,110)) +\n  xlab(\"Treatment\") + \n  ylab(\"Weight (lbs)\")\n\n# note that order matters. I need to do the violin before the pointrange or else the violin will \"paint over\" the pointrange.\n\npoint_violin_plot <- ggplot(data = anorexia_data, aes(x=treatment, y=weight)) +\n  geom_violin() +\n  stat_summary(fun.data = \"mean_se\", geom = \"pointrange\", color = \"black\") + # adding some color mods here.\n  scale_y_continuous(expand = c(0,0)) + \n  theme_cowplot() +\n  coord_cartesian(ylim = c(70,110)) +\n  xlab(\"Treatment\") + \n  ylab(\"Weight (lbs)\")\n\ncowplot::plot_grid(barplot, point_violin_plot)\n\n\n\n\nAs you can see the second plot gives me more information about what’s truly going on with my data."
  },
  {
    "objectID": "week06/6_1_t-test.html#performing-the-t-test-paired-sample-t-test",
    "href": "week06/6_1_t-test.html#performing-the-t-test-paired-sample-t-test",
    "title": "\n25  Testing differences in means / t-test\n",
    "section": "\n25.2 Performing the t-test (Paired sample t-test)",
    "text": "25.2 Performing the t-test (Paired sample t-test)\nOkay, now that we’ve done all of our preparation, we’re now ready to perform the test. We can do so using the t.test() function. In this case, the experimental question warrants a paired samples t-test. Given that our Levene’s test failed to reject the null, we will assume that our variances are equal.\nSince we’ve got long-format data we will use the formula syntax. This reads “predicting changes in weight as a function of treatment.\n\nt.test(weight~treatment,data=anorexia_data,paired=T, var.equal=T)\n\n\n    Paired t-test\n\ndata:  weight by treatment\nt = -4.1802, df = 16, p-value = 0.0007072\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -10.948840  -3.580571\nsample estimates:\nmean difference \n      -7.264706 \n\n\nThe output provides us with our \\(t\\) value, the \\(df\\) and the \\(p\\) value. It also includes a measure of the 95% CI, and the mean difference. Remember that the null hypothesis is that there is no difference between our two samples. In the case of repeated measures especially, it makes sense to think of this in terms of a difference score of change, where the null is 0. The resulting interpretation is that on average participants’ weight increased 7.26 pounds due to the treatment, with a 95% likelihood that the true mean change is between 3.58 lbs and 10.95 lbs. Important for us is that 0 is not in the 95% CI, reinforcing that there was indeed a non-zero change (rejecting the null)."
  },
  {
    "objectID": "week06/6_1_t-test.html#other-t-tests",
    "href": "week06/6_1_t-test.html#other-t-tests",
    "title": "\n25  Testing differences in means / t-test\n",
    "section": "\n25.3 Other \\(t\\) tests:",
    "text": "25.3 Other \\(t\\) tests:\n\n25.3.1 One sample:\nThe data in our example warranted running a paired t-test. However, as noted we can run a t.test() to compare a single sample to a single value. For example it might be reasonable to ask whether or not the 17 adolescent girls that Hand, et al., 1994 treated were different from what would be considered the average weight of a teenaged girl. A quick Google search suggests that the average weight of girls 12-17 in 2002 was 130 lbs. How does this compare to Hand et al.’s participants Before treatment? We can run a one sample t-test to answer this question:\n\nbeforeTreatment <- filter(anorexia_data,treatment==\"Before\")\nt.test(beforeTreatment$weight, mu = 130)\n\n\n    One Sample t-test\n\ndata:  beforeTreatment$weight\nt = -38.44, df = 16, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 130\n95 percent confidence interval:\n 80.65007 85.80876\nsample estimates:\nmean of x \n 83.22941 \n\n\nYes, this group of girls was significantly underweight compared to the national average.\n\n25.3.2 Independent samples example\nWe run an independent samples t-test when we have reason to believe that the data in the two samples is NOT meaningfully related in any fashion. Consider this example regarding Joshua Aronson’s work on stereotype threat:\n\nJoshua Aronson has done extensive work on what he refers to as “stereotype threat,” which refers to the fact that “members of stereotyped groups often feel extra pressure in situations where their behavior can confirm the negative reputation that their group lacks a valued ability” (Aronson, Lustina, Good, Keough, Steele, & Brown, 1998). This feeling of stereo- type threat is then hypothesized to affect performance, generally by lowering it from what it would have been had the individual not felt threatened. Considerable work has been done with ethnic groups who are stereotypically reputed to do poorly in some area, but Aronson et al. went a step further to ask if stereotype threat could actually lower the performance of white males—a group that is not normally associated with stereotype threat.\n\n\nAronson et al. (1998) used two independent groups of college students who were known to excel in mathematics, and for whom doing well in math was considered important. They assigned 11 students to a control group that was simply asked to complete a difficult mathematics exam. They assigned 12 students to a threat condition, in which they were told that Asian students typically did better than other students in math tests, and that the purpose of the exam was to help the experimenter to understand why this difference exists. Aronson reasoned that simply telling white students that Asians did better on math tests would arousal feelings of stereotype threat and diminish the students’ performance.\n\nHere we have two mutually exclusive groups of white men, those that are controls and those under induced threat. Importantly we have no reason to believe that any one control man’s score is more closely tied to any individual experimental group counterpart than any others (we’ll return to this idea in a bit).\nHere is the data:\n\nstereotype_data <- read_delim(\"https://www.uvm.edu/~statdhtx/methods8/DataFiles/Tab7-7.dat\", delim = \"\\t\")\n\nRows: 23 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): ID\ndbl (2): Score, Group\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAs before, let’s take a look at the file structure:\n\nshow(stereotype_data)\n\n# A tibble: 23 × 3\n   ID    Score Group\n   <chr> <dbl> <dbl>\n 1 01        4     1\n 2 02        9     1\n 3 03       12     1\n 4 04        8     1\n 5 05        9     1\n 6 06       13     1\n 7 07       12     1\n 8 08       13     1\n 9 09       13     1\n10 10        7     1\n# ℹ 13 more rows\n\n\nI want to look at this example as it give is an opportunity to deal with another common issue in data cleaning. If you take a look at Group you see it’s either 1 or 2. In this case Group 1 are the control subjects and Group 2 are the threat subjects. Using numbers instead of names to identify levels of a factor is a convention from older methods and software. In more modern software you don’t need to do this sort of number coding (the software works this out in the background).\nIf you want to change this, you can use the recode_factor() function from dplyr package in the tidyverse (https://dplyr.tidyverse.org/reference/recode.html). For what it’s worth there are several other ways to do this including a recode() function in car. See http://rprogramming.net/recode-data-in-r/ for examples.\nHere I’m just going to mutate a new column, namedGroup column with the recoded names:\n\nstereotype_data <- stereotype_data %>% \n  mutate(\"namedGroup\" = dplyr::recode_factor(Group,\n                                            \"1\"=\"Control\", \n                                            \"2\"=\"Threat\")\n                            )\nstereotype_data\n\n# A tibble: 23 × 4\n   ID    Score Group namedGroup\n   <chr> <dbl> <dbl> <fct>     \n 1 01        4     1 Control   \n 2 02        9     1 Control   \n 3 03       12     1 Control   \n 4 04        8     1 Control   \n 5 05        9     1 Control   \n 6 06       13     1 Control   \n 7 07       12     1 Control   \n 8 08       13     1 Control   \n 9 09       13     1 Control   \n10 10        7     1 Control   \n# ℹ 13 more rows\n\n\nAn now to run the requisite assumption tests. Note that in this case I am running an Indepednent samples test, so I need to test the assumptions on each sample separately. Here I’m going be a little more critical about how I test for normality.\nUsing psych::describeBy:\n\npsych::describeBy(stereotype_data,group = stereotype_data$namedGroup)\n\n\n Descriptive statistics by group \ngroup: Control\n            vars  n mean   sd median trimmed  mad min max range  skew kurtosis\nID*            1 11 6.00 3.32      6    6.00 4.45   1  11    10  0.00    -1.53\nScore          2 11 9.64 3.17      9    9.89 4.45   4  13     9 -0.31    -1.48\nGroup          3 11 1.00 0.00      1    1.00 0.00   1   1     0   NaN      NaN\nnamedGroup*    4 11 1.00 0.00      1    1.00 0.00   1   1     0   NaN      NaN\n              se\nID*         1.00\nScore       0.96\nGroup       0.00\nnamedGroup* 0.00\n------------------------------------------------------------ \ngroup: Threat\n            vars  n mean   sd median trimmed  mad min max range  skew kurtosis\nID*            1 12 6.50 3.61    6.5     6.5 4.45   1  12    11  0.00    -1.50\nScore          2 12 6.58 3.03    7.0     6.9 2.22   0  10    10 -0.86    -0.39\nGroup          3 12 2.00 0.00    2.0     2.0 0.00   2   2     0   NaN      NaN\nnamedGroup*    4 12 2.00 0.00    2.0     2.0 0.00   2   2     0   NaN      NaN\n              se\nID*         1.04\nScore       0.87\nGroup       0.00\nnamedGroup* 0.00\n\n\nUsing DescTools:\ncontrol group\n\ncontrol_group <- stereotype_data %>% filter(namedGroup==\"Control\") \n\ncontrol_skew <- DescTools::Skew(x=control_group$Score,method = 2,conf.level = .95,ci.type = \"bca\",R = 1000)\ncontrol_skew_ses <- (control_skew[3] - control_skew[2])/3.92\ncontrol_skew[1]/control_skew_ses\n\n      skew \n-0.6529139 \n\ncontrol_kurt <- DescTools::Kurt(x=control_group$Score,method = 2,conf.level = .95,ci.type = \"bca\",R = 1000)\ncontrol_kurt_ses <- (control_kurt[3] - control_kurt[2])/3.92\ncontrol_kurt[1]/control_kurt_ses\n\n     kurt \n-1.105614 \n\n\nthreat group\n\nthreat_group <- stereotype_data %>% filter(namedGroup==\"Threat\") \n\nthreat_skew <- DescTools::Skew(x=threat_group$Score,method = 2,conf.level = .95,ci.type = \"bca\",R = 1000)\nthreat_skew_ses <- (threat_skew[3] - threat_skew[2])/3.92\nthreat_skew[1]/threat_skew_ses\n\n     skew \n-1.746705 \n\nthreat_kurt <- DescTools::Kurt(x=threat_group$Score,method = 2,conf.level = .95,ci.type = \"bca\",R = 1000)\nthreat_kurt_ses <- (threat_kurt[3] - threat_kurt[2])/3.92\nthreat_kurt[1]/threat_kurt_ses\n\n     kurt \n0.4284613 \n\n\nThis is a pain… can we make a function for this!!!!\nQQ-Plot\n\npacman::p_load(qqplotr)\n\nggplot(stereotype_data, aes(sample=Score, group=namedGroup, color=namedGroup)) + \n  geom_qq() + geom_qq_line()\n\n\n\n\nHomogeniety of Variance:\nWhile the paired samples test doesn’t make this assumption, the Independence samples test assumes the variability of scores for the two groups is roughly homogeneous.\nFor a t-test this can be tested by using the leveneTest() from the car package:\n\n# using long-format enter as a formula:\ncar::leveneTest(Score~namedGroup, data=stereotype_data, center=\"mean\")\n\nLevene's Test for Homogeneity of Variance (center = \"mean\")\n      Df F value Pr(>F)\ngroup  1  0.4306 0.5188\n      21               \n\n\nYou’ll note above I elected to mean center my samples. This is consistent with typical practice although “median” centering may be more robust.\n\ncar::leveneTest(data=stereotype_data, Score~namedGroup)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  1  0.4639 0.5033\n      21               \n\n\nT-test Given that my obtained Pr(>F), or p-value of Levene’s F-test, is greater than .05, I may elect to assume that my variances are equal. However, if you remained skeptical, there are adjustments that you may make. This includes adjusting the degrees of freedom according to Welch-Satterthwaite recommendation (see below). Recall later on that we are looking at our obtained \\(t\\) value with respect to the number of \\(df\\). This adjustment effectively reduces the \\(df\\) in turn making your test more conservative.\nThe Levene’s test failed to reject the null so I may proceed with my t.test assuming variances are equal. Note that paired=FALSE for independent sample tests:\n\nt.test(data=stereotype_data, Score~namedGroup, paired=FALSE, var.equal=T)\n\n\n    Two Sample t-test\n\ndata:  Score by namedGroup\nt = 2.3614, df = 21, p-value = 0.02795\nalternative hypothesis: true difference in means between group Control and group Threat is not equal to 0\n95 percent confidence interval:\n 0.3643033 5.7417573\nsample estimates:\nmean in group Control  mean in group Threat \n             9.636364              6.583333 \n\n\nThis output gives us the \\(t\\)-value, \\(df\\) and \\(p\\)-value. Based on this output I may conclude that the mean score in the Control group is significantly greater than the Threat group.\nJust as an example, let’s set var.equal to FALSE:\n\nt.test(data=stereotype_data, Score~namedGroup, paired=FALSE, var.equal=F)\n\n\n    Welch Two Sample t-test\n\ndata:  Score by namedGroup\nt = 2.3565, df = 20.614, p-value = 0.02843\nalternative hypothesis: true difference in means between group Control and group Threat is not equal to 0\n95 percent confidence interval:\n 0.3556143 5.7504463\nsample estimates:\nmean in group Control  mean in group Threat \n             9.636364              6.583333 \n\n\nComparing the outputs you see that in this case R has indicated that it has run the test with the Welsh correction. Note that this changes the \\(df\\) and consequently the resulting \\(p\\) value. That this change was negligible reinforces that the variances were very similar to one another. However in cases where they are not close to one another you may see dramatic changes in \\(df\\).\nIn R, the t.test() function sets var.equal=FALSE by default. Why you ask? Well, you can make the argument that the variances are ALWAYS unequal, its only a matter of degree. Assuming variances are unequal makes your test more conservative, meaning that if the test suggests that you should reject the null, you can be slightly more confident that you are not committing Type I error. At the same time, it could be argued that setting your var.equal=TRUE in this case (where the Levene test failed to reject the null) makes your test more powerful, and you should take advantage of that power to avoid Type II error."
  },
  {
    "objectID": "week06/6_1_t-test.html#independent-or-paired-sample",
    "href": "week06/6_1_t-test.html#independent-or-paired-sample",
    "title": "\n25  Testing differences in means / t-test\n",
    "section": "\n25.4 Independent or Paired Sample?",
    "text": "25.4 Independent or Paired Sample?\nIt is safe to assume that anytime that you are collecting data samples from the same person at two different points in time that you need to run a paired-samples test. However, it would not be safe to assume that if the samples are coming from different groups of people that you always run an independent samples test. Remember the important qualifier mentioned above: That there no reason to believe that any one participant in the first group is is more closely related to any single counterpart in the second group than the remaining of others. In our Independent test example we have no reason to assume this is the case, we assume that members of the Control and Threat groups were randomly selected. But what if we instead recruited brothers or twins? In this case, it may make sense to treat members of the two groups as paired; brothers have a shared history (education, socio-economic level, family dynamic, etc) that would make their scores more likely to be related to one another than by random chance."
  },
  {
    "objectID": "week06/6_3_t-test-lm.html#group-means-grand-means-and-their-models",
    "href": "week06/6_3_t-test-lm.html#group-means-grand-means-and-their-models",
    "title": "\n27  t-test? It’s just a linear model\n",
    "section": "\n27.1 group means, grand means and their models",
    "text": "27.1 group means, grand means and their models\nWe left off the last walkthrough distinguishing between group means and grand means; and how there relationship holds if the null hypothesis is true. To simply reiterate, if the null hypothesis is true that there is no mean difference between two independent samples, then not only should the two means be equal to one another, but they should also be equal to the grand mean. We also hinted that the grand mean figured prominently in the null_hypothesis_models that we first introduced last week in regression. In that wall through we talked about how the null_model was built using (random) variation about the mean of out outcome variable as the only predictor. The same holds here, but can be restated that the null_model is built using random variation around the grand mean as the only predictor (mathematically this is the same thing). In the case of our stereotype_data:\n\nnull_model <- lm(Score ~ 1, data = stereotype_data)\n\nwhere ~1 is the intercept indicating the grand mean of both the Control group and Threat group combined.\nLooking at our stereotype_data scores, the null_model can be conveyed graphically. First, in order to do so we need to add the predicted scores from our model to our stereotype_data data frame:\n\nstereotype_data <- stereotype_data %>%\n  mutate(means_fitted = null_model$fitted.values)\n\nBelow the red points represent control group scores with their residuals (lines), the green are the threat group scores, and the horizontal line is the grand mean. The sum of the residuals indicates the amount of error in this model (termed random error).\n\nlibrary(ungeviz)\nnull_plot <- ggplot(data = stereotype_data, aes( x = namedGroup, \n                                                 y = Score, \n                                                 col = namedGroup)) +\n  geom_hline(yintercept = mean(stereotype_data$Score)) +\n  geom_point(position = position_jitter(width = .4, height = 0, seed = 1)) + \n  geom_linerange(aes(ymin = means_fitted, ymax = Score),\n                 position = position_jitter(width = .4, height = 0, seed = 1)) +\n  theme_cowplot() +\n  theme(legend.position=\"none\")\n\nshow(null_plot)\n\n\n\n\nIn contrast, our group_model includes out different groups as a predictor, namely saying that we expect scores to differ between our two groups.\n\ngroup_model <- lm(Score ~ namedGroup, data = stereotype_data)\n\nVisually, the same data is plotted below, except this time the residual error considers the mean of each group. That is rather than any scores error being determined with respect to the grand mean, its now determined with respect the the mean of its group (the colored horiztonal lines)\n\nlibrary(ungeviz)\n\n# adding the predicted scores from the group model to the data frame (see above)\n\nstereotype_data <- stereotype_data %>%\n  mutate(group_fitted = group_model$fitted.values)\n\ngroup_plot <- ggplot(data = stereotype_data, aes(x = namedGroup, \n                                                 y = Score, \n                                                 col = namedGroup)) +\n  stat_summary(geom = \"hpline\", width = 1, size = 1.5) +\n  geom_point(position = position_jitter(width = .4, height = 0, seed = 1)) + \n  geom_linerange(aes(ymin = group_fitted, ymax = Score),\n                 position = position_jitter(width = .4, height = 0, seed = 1)) +\n  theme_cowplot() +\n  theme(legend.position=\"none\")\n\nshow(group_plot)\n\nNo summary function supplied, defaulting to `mean_se()`\n\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\nLooking at the two plots side-by-side, it’s pretty apparent the one of these fits much better than the other.\n\nplot_grid(null_plot, group_plot,labels = c(\"null_model\", \"group_model\"), label_x = .4)\n\nNo summary function supplied, defaulting to `mean_se()`"
  },
  {
    "objectID": "week06/6_3_t-test-lm.html#an-analysis-of-variance-aka-variability-accounted-for",
    "href": "week06/6_3_t-test-lm.html#an-analysis-of-variance-aka-variability-accounted-for",
    "title": "\n27  t-test? It’s just a linear model\n",
    "section": "\n27.2 an analysis of variance (aka variability accounted for)",
    "text": "27.2 an analysis of variance (aka variability accounted for)\nLast week, we acknowledged that regression was a test for the amount of variance that our model including the predictor accounts for above and beyond the null_model. This value, our coefficient of determination, was measured/expressed as \\(r^2\\) and tested for significance using an \\(F\\)-test. The same holds true here. Performing an analysis of variance to compare the two models we see that the model that includes Group as a predictor lowers the residual sum of squares (RSS) enough to be deemed significant. Rather the difference between the null_model RSS and the residual_model RSS is enough to generate an \\(F\\)-ratio of 5.76 which is significant at the \\(p<.05\\) level. We’ll get into the details of how this ratio is calculated next week.\n\nanova(null_model, group_model)\n\nAnalysis of Variance Table\n\nModel 1: Score ~ 1\nModel 2: Score ~ namedGroup\n  Res.Df    RSS Df Sum of Sq      F  Pr(>F)  \n1     22 254.96                              \n2     21 201.46  1    53.494 5.5761 0.02795 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "week06/6_3_t-test-lm.html#looking-at-the-group_model-output",
    "href": "week06/6_3_t-test-lm.html#looking-at-the-group_model-output",
    "title": "\n27  t-test? It’s just a linear model\n",
    "section": "\n27.3 looking at the group_model output",
    "text": "27.3 looking at the group_model output\nSo, the group model is significantly better. What does that mean? Simply adding Group as a predictor to our model provides us with enough information about the variation in our data to suggest that that variation (e.g., differences in scores) migh be systematically driven by group differences (in this case related to threat and non-threat). Before diving into the group_model I want to take a moment and look at those group means again:\n\ngroup_means <- stereotype_data %>% \n  group_by(namedGroup) %>%\n  summarise(mean = mean(Score),\n            sd = sd(Score))\ngroup_means\n\n# A tibble: 2 × 3\n  namedGroup   mean    sd\n  <fct>       <dbl> <dbl>\n1 Control (0)  9.64  3.17\n2 Threat (1)   6.58  3.03\n\n\nOk. Now to look at the outcome of our group_model\n\nsummary(group_model)\n\n\nCall:\nlm(formula = Score ~ namedGroup, data = stereotype_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5833 -1.6098  0.4167  2.3902  3.4167 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            9.6364     0.9339  10.319 1.12e-09 ***\nnamedGroupThreat (1)  -3.0530     1.2929  -2.361   0.0279 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.097 on 21 degrees of freedom\nMultiple R-squared:  0.2098,    Adjusted R-squared:  0.1722 \nF-statistic: 5.576 on 1 and 21 DF,  p-value: 0.02795\n\n\nStarting from the top, recall that our group_model had namedGroup as a predictor with two levels—Control and Threat.\nThe Coefficients section conveys important information about both of our Groups. We see Threat listed (namedGroupThreat) but what of Control? It’s there too, captured in the (Intercept). Thinking back to our discussion on the linear regression equation:\n\\[\n\\hat{Y} = \\beta_0 + \\beta_1*x + \\epsilon\n\\]\nthe intercept, \\(\\beta_0\\), is the value of our predicted outcome, \\(\\hat{Y}\\), when the value of our predictor, \\(x\\) is zero. In this case, R takes the first level in alphabetical order and sets it as “0” (which is why I added the numeric coding) and each subsequent level goes up an integer. So the (Intercept) is the value of our predicted outcome variable when namedGroup = Control, or 9.6364. The coefficient for namedGroupThreat, -3.0530 represents the slope of the regression line. So for every value of 1 we go increase for our predictor, the predicted outcome goes down 3.0530. Plotting the data, including the regression line, and the group means (larger points)\n\nggplot(data = stereotype_data, aes(x = namedGroup, \n                                                 y = Score, \n                                                 col = namedGroup)) +\n  geom_point(position = position_jitter(width = .2, height = 0, seed = 1)) + \n  stat_summary(fun = mean, \n               size = 1, \n               color = \"black\", \n               mapping=aes(group=1), \n               geom=\"line\") +\n  stat_summary(size=1, geom = \"pointrange\") +\n  theme_cowplot() +\n  theme(legend.position=\"none\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nNo summary function supplied, defaulting to `mean_se()`\n\n\n\n\n\nThe regression line connects the two series at their means. It starts at the intercept in the Control group 9.6364, and travels down 3.0530 to pass through the Threat. So… 9.6364 - 3.0530 = 6.5834.\nReminding ourselves of the group means:\n\nstereotype_data %>% \n  group_by(namedGroup) %>%\n  summarise(mean = mean(Score)) %>%\n  pander()\n\n\n\n\n\n\n\nnamedGroup\nmean\n\n\n\nControl (0)\n9.636\n\n\nThreat (1)\n6.583\n\n\n\n\n\nTurning out attention to the \\(t\\) value associated for namedGroupThreat, this is the exact same \\(t\\) and \\(p\\) that we get when we use t.test()\n\nt.test(Score~namedGroup, data = stereotype_data, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  Score by namedGroup\nt = 2.3614, df = 21, p-value = 0.02795\nalternative hypothesis: true difference in means between group Control (0) and group Threat (1) is not equal to 0\n95 percent confidence interval:\n 0.3643033 5.7417573\nsample estimates:\nmean in group Control (0)  mean in group Threat (1) \n                 9.636364                  6.583333 \n\n\nThis \\(t\\) value is also the square-root of the \\(F\\)-value obtained in the model analysis of variance.\nLooking forward, everything we’ve said in the last two walkthroughs apply to when we get to ANOVA as well.\n\nstereotype_data %>% group_by(namedGroup) %>% summarize(mean = mean(Score),\n                                                       sumsq = sum((Score-mean(Score))^2)\n                                                      )\n\n# A tibble: 2 × 3\n  namedGroup   mean sumsq\n  <fct>       <dbl> <dbl>\n1 Control (0)  9.64  101.\n2 Threat (1)   6.58  101."
  },
  {
    "objectID": "week07/7_1-effect_size.html",
    "href": "week07/7_1-effect_size.html",
    "title": "\n28  Effect Size\n",
    "section": "",
    "text": "29 Variation due to chance, sampling error, the standard error, and the 95% CI\nWe’ve talked at length about one critical fact regarding your data… that the parameters / descriptive stats of your empirically observed sample are very unlikely (i.e., practically impossible) to match the population parameters. More, not only is it unlikely that the mean from your sample equals the population mean, BUT the mean of a sample that you take today is not likely to match a sample from the same population taken tomorrow (variation due to chance). We demonstrated this fact by generating an example population and then simulated taking samples from it. First imagine running 5K simulations taking 50 people per sample.\nFrom this sampling distribution of means we know several things:\nVisually this looks like:\nWe also took a look at how this distribution changes as a function of our sample size. Based on the last two weeks discussions, \\(N=30\\) seems like a magic number. Instead of 50, let’s look at what happens if we run our simulations assuming we collected only 10 participants:\nSame population, same number of simulations, drastically different SEM and 95% CI. In the second case we are not as effectively estimating our population mean.\nTogether these two examples highlight something very important. Assuming all other things equal, our ability to effectively say something about our populations of interest is influenced by the number of people in our sample.\nWhile in this class we have often run simulations with our data to estimate parameters, the truth of the matter is that the simulation method is a fairly recent practice given the computational demands. Two or three decades ago, running 5000, 10000, 100000 simulations provided a given data set would have been not only inefficient, but practically untenable for most consumer computers. Algorithmic methods for parameter estimates (i.e., estimating a parameter given an equation based on the sample) not only go back more than a half century, but are still quite common practice today. For example, we made the claim that one could estimate the SEM by:\n\\[\nSEM = \\frac{SD_X}{\\sqrt{N_X}}\n\\]\nwhere \\(SD_X\\) is the standard deviation of our sample scores and \\(N_X\\) is the number in our sample. Again we arrive at a value that provides us with a statement about how well we are estimating the mean parameter. However, I’d like to point out one very important consequence of this equation for SEM. Imagine that from a population I take two samples—the first of size 30 and the second of size 100. We assume that the differences in standard error that I report from those two populations is largely going to due to the differences in \\(N\\) (30 v. 100). To a much lesser degree they are going to be due to fluctuations in the \\(SD\\) from sample to sample.\nVariation due to chance suggests that any observation that I make via random sampling is always going to have some inherent degree of error. In a simple example, there is always going to be the true population mean and there is going to be the value that I obtain in any given sample. The latter jumps around stochastically; randomly from sample to sample.\nKicking this up to two samples drawn from theoretically distinct populations, this guarantees that any observed differences in means is also going to vary stochastically. For example, lets imagine two populations who’s mean Stats Proficiency Scores differ by 50; for the sake of comparison I’m going to keep their SDs identical. For any given experiment the observed difference between these groups is going to vary. We can note this in the difference distribution where despite a true difference of 50, some observed differences might be as little as 30 and as high as 70.\nThis possibility for random variation presents a problem given all we have are our individual sample(s) to try to figure out whats going on with our population. In the larger sense of the above difference distribution it suggests that no single sample is likely to tell us the true differences between mean for our two groups. In the practical sense, we are unlikely to run 5000 separate experiments to obtain the difference distribution above. We’re likely only to run 1 or 2, and from those attempt to extrapolate our population parameters. Moreover in any single sample, scores are likely to vary around the sample mean.\nGiven we must estimate our SEM from our sample, we want to be able to get an impression about how big our population differences might be. But, as noted before scores in any individual sample are likely to vary randomly. In most scientific literature, random variation of this sort is termed “noise”. However, in our experiments we are typically looking for systematic, non-random variation… for example scores from one group are systematically higher than another. Thus, when comparing two samples, like an experimental group and a control, we need to uncover how much of our variation is systematic compared to how much of it is due to random fluctuations.\nEnter Jacob Cohen who borrowed a useful idea at the time, signal detection theory which was created during the invention of the radio and radar. Radio is an apt example (if sightly outdated… does anyone listen to the radio anymore?) When one tunes their radio dial, they are attempting to pick up their desired channel or signal (WRKP in Cincinnati) amidst all of the other radio broadcasts and background noise (FWIW, similar principles apply for fiber optics, Wi-Fi, really anything where information is sent over a signal). The simple idea was that you want to know how strong a given signal needed to be for you to detect it over the noise. In our case the noise is the random variation in our samples and the signal is the effect of our experimental treatment or manipulation. From this we can ask ourselves two inter-related questions:\nThe answers to these two questions are our effect size and power.\nAs stated above, Effect Size is a measure of the strength of effect of our treatment in the face of random variation in our current experiment. While a simple example of effect size can be made using the comparison of mean differences, we should note our definition does not limit to just this simple case. Any measure that describes the relationship between systematic v. random variation is in effect… well, an Effect Size. For example, \\(R^2\\) (from last week) is a measure of effect size.\nIn the simple comparison of two sample means, we often invoke Cohen’s d (conceptually tied to d-prime from signal detection theory). Keeping with this metaphor, the control group is our noise and the signal is the effect of our treatment, ideally capture in the experimental group. Taking a look at the distribution of scores between our two groups might look something like this:\nYou’ll notice that even though there is separation between the group means, there is still a bit overlap between our groups. Conceptually this is akin to our experimental group containing some of the same inherent noise as the control group in addition to the signal or effect."
  },
  {
    "objectID": "week07/7_1-effect_size.html#cohens-d",
    "href": "week07/7_1-effect_size.html#cohens-d",
    "title": "\n28  Effect Size\n",
    "section": "\n32.1 Cohen’s \\(d\\)\n",
    "text": "32.1 Cohen’s \\(d\\)\n\nCohen surmised that we could apply the same logic to experiments to determine how big the effect would be (in other words how easy it is to distinguish the signal from the noise).\n\\[ d = \\frac{M_{H1}-\\mu_{H0}}{S_{H1}} \\]\nYou might notice you have seen this formula before!\n\\[ Z = \\frac{M-\\mu}{S} \\]\nSo Cohen’s \\(d\\) is in standard deviation units:\n\n\nSize\n\\(d\\)\n\n\n\nSmall\n.2\n\n\nMedium\n.5\n\n\nLarge\n.8\n\n\n\n\n32.1.1 Small Visualized\n\n\n\n\n\n\n32.1.2 Medium Visualized\n\n\n\n\n\n\n32.1.3 Large Visualized\n\n\n\n\n\nYou notice that even in a large effect there is a lot of overlap between the curves."
  },
  {
    "objectID": "week07/7_1-effect_size.html#calculating-effect-size",
    "href": "week07/7_1-effect_size.html#calculating-effect-size",
    "title": "\n28  Effect Size\n",
    "section": "\n32.2 Calculating effect size",
    "text": "32.2 Calculating effect size\nLet’s return to our example data from our t-test walkthrough. Recall that we used this data from Hand et al. (1994):\n\nHand, et al., 1994, reported on family therapy as a treatment for anorexia. There were 17 girls in this experiment, and they were weighed before and after treatment. The weights of the girls, in pounds, is provided in the data below:\n\n\nanorexia_data <- read_delim(\"https://www.uvm.edu/~statdhtx/methods8/DataFiles/Tab7-3.dat\", \n                     \"\\t\", escape_double = FALSE, trim_ws = TRUE)\n\nRows: 17 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): ID\ndbl (2): Before, After\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# putting in long format\nanorexia_data <- pivot_longer(data = anorexia_data,cols = c(Before,After),names_to = \"Treatment\",values_to = \"Weight\")\n\nSo what is known: we have 17 total participants from (hypothetically) the same population that are measured twice (once Before treatment, and once After treatment). Based upon the experimental question we need to run a paired-sample (matched-sample) test.\n\nt.test(Weight~Treatment, data=anorexia_data, paired=T)\n\n\n    Paired t-test\n\ndata:  Weight by Treatment\nt = 4.1802, df = 16, p-value = 0.0007072\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n  3.580571 10.948840\nsample estimates:\nmean difference \n       7.264706 \n\n\nWe can estimate the effect size using the effectsize package and pasting in the exact same arguments that we used for the t-test.\nIn this case we want to calculate Cohen’s d. In this case I’m adding a correct since I have a small sample size (see ?cohens_d)\n\npacman::p_load(effectsize)\n\neffectsize::cohens_d(Weight~Treatment, data=anorexia_data, paired = T,correction = T)\n\nCohen's d |       95% CI\n------------------------\n1.01      | [0.41, 1.59]\n\n\nPretty strong effect we’ve got here.\n\\(t(16) = 4.18, p < .001, d = 1.01\\)"
  },
  {
    "objectID": "week07/7_2-power.html#truth-and-no-truth-to-power",
    "href": "week07/7_2-power.html#truth-and-no-truth-to-power",
    "title": "\n29  Power\n",
    "section": "\n29.1 Truth and no truth to power",
    "text": "29.1 Truth and no truth to power\nIn your travels you are likely to encounter two kinds of power estimates, a priori power and a posteriori power. A priori power fits with our description above, and is calculated before one conducts their experiment. A posteriori power is calculated after the experiment is completed, and asks provided the observed effect size what is the power that you achieved. One of these two is better than the other… in fact just say no to a posteriori, or post-hoc / observed power (the latter term shows up in SPSS). As a measure, it is so directly tied to you obtained p-value as to be rendered essentially meaningless, and does not really answer the question that power analysis intends to address."
  },
  {
    "objectID": "week07/7_2-power.html#finding-power",
    "href": "week07/7_2-power.html#finding-power",
    "title": "\n29  Power\n",
    "section": "\n29.2 Finding power",
    "text": "29.2 Finding power\nThe calculation of power takes into consideration 3 factors:\n\nwhat is the size of the effect of your treatment / manipulation\nhow large of a sample do you have to amplify your effect\nwhat is the criterion by which you will measure if the effect is detected.\n\nTo squeeze every last drop out of the radio metaphor we can think of this as:\n\nwhat is the characteristic of the original signal relative to noise\nhow much of a boost can you give that signal (how many antennae is it emitted from)\nhow sensitive is the receiving antenna that tunes in the signal and tunes out the noise.\n\nIn statistical terms these are:\n\nyour effect size\nyour sample size\nyour \\(\\alpha\\) (alpha) value (e.g, \\(p<.05\\))\n\nAs experimenters, we can manipulate any of these 3 components to increase the power of our design, though due to practical or theoretical constraints tend to focus on sample size and alpha value. Assuming that we allow effect size to remain constant (that is we don’t try to enhance or exaggerate the effect, see the Cohen textbook for examples) we can calculate power as such:\nCalculating a priori Power Involves setting a specific likelihood (power = .80) at given \\(\\alpha\\) level (.05), given a known effect size. This is typically used to estimate the sample size needed to achieve that power (i.e., how much of a signal boost do we need). A key problem here is determining the proper effect size? One suggestion is to survey the literature for independent experiments and meta-analyses and use their results as a guide. BUT, one important thing to consider is that due to publication bias, those results are likely to be overestimates (only effects that are significant are reported, see Kicinski et al., 2015). Some other rules of thumb, assume your effect is small-to-medium (d = .3 or .4).\nPost-hoc Power is estimated given the observed effect size you found from your study, the sample size and \\(\\alpha\\) you set. As I mentioned post-hoc power doesn’t really have much value. See here for a more detailed discussion."
  },
  {
    "objectID": "week07/7_2-power.html#estimating-a-priori-power",
    "href": "week07/7_2-power.html#estimating-a-priori-power",
    "title": "\n29  Power\n",
    "section": "\n29.3 Estimating A priori Power",
    "text": "29.3 Estimating A priori Power\nHere we aim to ensure a specific likelihood that we will find a significant effect (\\(1-\\beta\\)). There are two likelihood values that are often chosen for power: .8 and .95. 80% power is the value suggested by Cohen because it’s economical. 95% is called high power, and it has been favored of late (but its very expensive and you will see why). To calculate power, we need 3 pieces of information. 1) Effect size (in Cohen’s \\(d\\)), 2) your \\(\\alpha\\) level, and 3) a number of tails you want to set (usually 2-tailed).\nTo calculate power in R we can use the pwr package. It should already be installed on your computer (it comes with R), but if for some reason it is not, you can re-install it. In our explanation of power thus fair we have discussed it in the context of comparing two means, as we would in a t-test. Let’s start by performing a power analysis using this example.\nFor example let’s assume that we are attempting to achieve a power of .80 given an effect size (d) of .4 and an \\(\\alpha\\) of .05 when comparing two means in a independent samples t-test. Here we would use the pwr.t.test function in the pwr package.\npwr.t.test works by solving what you leave as NULL.\n\nlibrary(pwr)\npwr.t.test(n = NULL, d = .4, power = .80, sig.level = 0.05,\n               type = c(\"two.sample\"), alternative = c(\"two.sided\"))\n\n\n     Two-sample t test power calculation \n\n              n = 99.08032\n              d = 0.4\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nHere it is telling me that in order to achieve my desired power I would need to recruit 99 participants for each group. Conversely, if this was a paired samples t-test:\n\npwr.t.test(n = NULL, d = .4, power = .80, sig.level = 0.05,\n               type = c(\"paired\"), alternative = c(\"two.sided\"))\n\n\n     Paired t test power calculation \n\n              n = 51.00945\n              d = 0.4\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\nI would need to recruit 51 participants (again showing you repeated measures designs are more powerful).\nWhat happens if I bump my desired power up to .95?\n\npwr.t.test(n = NULL, d = .4, power = .95, sig.level = 0.05,\n               type = c(\"paired\"), alternative = c(\"two.sided\"))\n\n\n     Paired t test power calculation \n\n              n = 83.16425\n              d = 0.4\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\npwr.t.test(n = NULL, d = .4, power = .95, sig.level = 0.05,\n               type = c(\"two.sample\"), alternative = c(\"two.sided\"))\n\n\n     Two-sample t test power calculation \n\n              n = 163.4006\n              d = 0.4\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nIn this case I need to add about 30 more participants in the paired-samples case and about 64 participants to EACH group in the independent samples case."
  },
  {
    "objectID": "week07/7_2-power.html#sample-size-and-power-diminishing-returns",
    "href": "week07/7_2-power.html#sample-size-and-power-diminishing-returns",
    "title": "\n29  Power\n",
    "section": "\n29.4 Sample size and Power (diminishing returns)",
    "text": "29.4 Sample size and Power (diminishing returns)\nSample size and power at a given effect size have a non-linear relationship, such that eventually you hit a wall in how much of an increase in power you get with each increase in sample size. Keeping in mind that pwr.t.test works by solving what you leave as NULL, we can calculate power given our estimated effect size, our alpha, and our likely sample size. Say for example we have a treatment with an estimated effect of \\(d=.40\\). Given our budget, we are limited to collecting no more than 50 participants total (25 in each group). We can then ask ourselves how powerful is our design (what is the likelihood of detecting an effect given it is there).\n\npwr.t.test(n = 50, d = .4, power = NULL, sig.level = 0.05,\n               type = c(\"two.sample\"), alternative = c(\"two.sided\"))\n\n\n     Two-sample t test power calculation \n\n              n = 50\n              d = 0.4\n      sig.level = 0.05\n          power = 0.5081857\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nHere our power is .51, indicating that we have a 51% chance of detecting an existing effect. This study may not be worth the time, money, and effort. What if we could run this as a paired test:\n\npwr.t.test(n = 50, d = .4, power = NULL, sig.level = 0.05,\n               type = c(\"paired\"), alternative = c(\"two.sided\"))\n\n\n     Paired t test power calculation \n\n              n = 50\n              d = 0.4\n      sig.level = 0.05\n          power = 0.7917872\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\nNow our power approaches 80%. This might be a good study to run.\nLets take a look at some power curves, with a priori power as a function of sample size:\n\n\n\n\n\nAs you can see, the larger the effect the fewer subjects you need. At the same time both curve plateau suggesting that about a certain sample size your gains in power are inconsequential."
  },
  {
    "objectID": "week07/7_2-power.html#power-and-effect-size",
    "href": "week07/7_2-power.html#power-and-effect-size",
    "title": "\n29  Power\n",
    "section": "\n29.5 Power and effect size",
    "text": "29.5 Power and effect size\nIn a similar vein we address this question: what is the smallest effect size that we can hope to detect given our design. Lets assume we can collect no more than 25 participants and desire a power level of .80.\nAgain using pwr.t.test, setting our desired power at .80 and leaving d NULL:\n\npwr.t.test(n = 100, d = NULL, power = .8, sig.level = 0.05,\n               type = c(\"paired\"), alternative = c(\"two.sided\"))\n\n\n     Paired t test power calculation \n\n              n = 100\n              d = 0.2829005\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*"
  },
  {
    "objectID": "week07/7_2-power.html#regression-example",
    "href": "week07/7_2-power.html#regression-example",
    "title": "\n29  Power\n",
    "section": "\n29.6 Regression example",
    "text": "29.6 Regression example\nWe can also estimate power of simple regression analyses using this package and the pwr.f2.test. In this case we need to know our\n\ndesired power\n\nnumerator degrees of freedom, u\n\ndenominator degrees of freedom, v\n\nthe effect size, \\(R^2\\), f2\n\nand our significance level\n\nAs above, whatever is left NULL is what’s calculated. For example, how many participants would we need to detect an R^2 of .39, at a desired power of .80 assuming a simple regression (where our numerator df = 1)\n\npwr.f2.test(u = 1,v = NULL,f2 = .39,power = .80)\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 20.22032\n             f2 = 0.39\n      sig.level = 0.05\n          power = 0.8\n\n\nThis output tells us that our denominator df would need to be about 20. Knowing the relationship between \\(N\\) and the denominator df in simple regression suggests I would need 22 participants.\nYou can check out this link for more detail about the power package"
  },
  {
    "objectID": "week07/7_3-power_es_sig_tests.html#significance-test-when-n-12",
    "href": "week07/7_3-power_es_sig_tests.html#significance-test-when-n-12",
    "title": "\n30  Power, Effect sizes, and Significance testing\n",
    "section": "\n30.1 Significance Test when N = 12",
    "text": "30.1 Significance Test when N = 12"
  },
  {
    "objectID": "week07/7_3-power_es_sig_tests.html#significance-test-when-n-24",
    "href": "week07/7_3-power_es_sig_tests.html#significance-test-when-n-24",
    "title": "\n30  Power, Effect sizes, and Significance testing\n",
    "section": "\n30.2 Significance Test when N = 24",
    "text": "30.2 Significance Test when N = 24"
  },
  {
    "objectID": "week07/7_3-power_es_sig_tests.html#significance-test-when-n-48",
    "href": "week07/7_3-power_es_sig_tests.html#significance-test-when-n-48",
    "title": "\n30  Power, Effect sizes, and Significance testing\n",
    "section": "\n30.3 Significance Test when N = 48",
    "text": "30.3 Significance Test when N = 48\n\n\n\n\n\nYou will notice the area of \\(1-\\beta\\) increases because as we add to the sample size, the curve (noise) gets thinner (SEM), but the distance between them does not change! This effect size is theoretically independent of significance testing as its simply based on the mean difference / standard deviation. If you know the true standard deviation (\\(\\sigma\\)) then this is a true statement. However, we never know the true standard deviation so we approximate it based on sample. So our observed estimate of effect size from experimental data is another guess based on the variation due to chance. Because of this we cannot tie effect size to p-value.\nTo demonstrate this, let’s revisit the Hand et al data this time reducing the size of the effect. Again regarding Hand:\n\nHand, et al., 1994, reported on family therapy as a treatment for anorexia. There were 17 girls in this experiment, and they were weighed before and after treatment. The weights of the girls, in pounds, is provided in the data below:\n\n\nanorexia_data <- read_delim(\"https://www.uvm.edu/~statdhtx/methods8/DataFiles/Tab7-3.dat\", \n                     \"\\t\", escape_double = FALSE, trim_ws = TRUE)\n\nRows: 17 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): ID\ndbl (2): Before, After\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# reducing the effect (minimizing difference)\nanorexia_data <- anorexia_data %>% \n  mutate(After = After - 6) \n\n# putting in long format\nanorexia_data <- pivot_longer(data = anorexia_data,cols = c(Before,After),names_to = \"Treatment\",values_to = \"Weight\")\n\nSo what is known: we have 17 total participants from (hypothetically) the same population that are measured twice (once Before treatment, and once After treatment). Based upon the experimental question we need to run a paired-sample (matched-sample) test.\n\nt.test(Weight~Treatment, data=anorexia_data, paired=T)\n\n\n    Paired t-test\n\ndata:  Weight by Treatment\nt = 0.72773, df = 16, p-value = 0.4773\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.419429  4.948840\nsample estimates:\nmean difference \n       1.264706 \n\npacman::p_load(effectsize)\neffectsize::cohens_d(Weight~Treatment, data=anorexia_data, paired = T,correction = T)\n\nCohen's d |        95% CI\n-------------------------\n0.18      | [-0.31, 0.65]\n\n\nSo on first pass we’ve got a null effect (\\(p>.05\\)) and our effect size is small (0.16). Let’s image that instead of 17 participants we had 170:\n\nanorexia_data_170 <- do.call(\"rbind\", replicate(10, anorexia_data, simplify = FALSE))\n\n\nt.test(Weight~Treatment, data=anorexia_data_170, paired=T)\n\n\n    Paired t-test\n\ndata:  Weight by Treatment\nt = 2.3651, df = 169, p-value = 0.01916\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.2090925 2.3203193\nsample estimates:\nmean difference \n       1.264706 \n\npacman::p_load(effectsize)\neffectsize::cohens_d(Weight~Treatment, data=anorexia_data, paired = T,correction = T)\n\nCohen's d |        95% CI\n-------------------------\n0.18      | [-0.31, 0.65]\n\n\nWe see our effect size remains unchanged, but now our test is significant!"
  },
  {
    "objectID": "week07/7_4-simulating_power.html",
    "href": "week07/7_4-simulating_power.html",
    "title": "\n31  Simulating Power!!!\n",
    "section": "",
    "text": "32 No one man should have all that Superpower\nThe two examples above were to provide a gist of how to conduct a simple power analysis using simulation methods. Moving forward, we’ll ramp up the complexity of our designs by including ANOVA. One thing that is apparent is that creating the simulated datasets as above may become a little cumbersome. Fortunately for us, there are packages in R that do the heavy lifting in this process for us.\nA relatively new, but powerful (haha) package is Superpower (haha) developed by Daniel Lakens and Aaron Caldwell. A paper outlining this package can be found here. Let’s load in the package:\nThe first step is to consider the design and predicted effects of your anticipated data set. For example in our general example above we have a between-groups design with 2 factors. We had an estimated effect size of Cohen’s d = 0.4. We elected to run a t-test given this design, although FWIW we could have also run an ANOVA.\nWe can input this information into Superpower using ANOVA_design():\nFrom here you would take your design_result and plug it into the ANOVA_power, including (but not limited to) the following arguments:\nThe output provides estimates of power based upon the provided parameters. For our purposes, the pairwise t-test, we would refer to the outputs labelled pairwise\nFor now, this is where I leave you. The goals here were to provide you with a conceptual basis for how we can run power analyses using simulations as well as an introductory example. In future walkthroughs, I will show an example of how to conduct a power analysis given each of the designs that we will encounter. For those that want to get a head start I would check out these two wonderful resources from the authors of Superpower"
  },
  {
    "objectID": "week07/7_4-simulating_power.html#the-general-procedure",
    "href": "week07/7_4-simulating_power.html#the-general-procedure",
    "title": "\n31  Simulating Power!!!\n",
    "section": "\n31.1 The general procedure",
    "text": "31.1 The general procedure\nEach of the examples in this walkthrough are going to follow a common procedure:\n\nidentify your hypothesized result.\nrun a series of simulations, randomizing data given parameters consistent with (1).\ntest each simulation given your criterion of significance (typically \\(\\alpha\\))."
  },
  {
    "objectID": "week07/7_4-simulating_power.html#revisiting-an-example-using-a-t-test",
    "href": "week07/7_4-simulating_power.html#revisiting-an-example-using-a-t-test",
    "title": "\n31  Simulating Power!!!\n",
    "section": "\n31.2 Revisiting an example using a t-test",
    "text": "31.2 Revisiting an example using a t-test\nI think a good place to start is revisiting our examples from a few weeks back. The the walkthrough on Power we ran an a priori power analysis provided some combination of:\n\nan effect size\na given sample size\na desired power\nand a performed test including significance level.\n\nWe noted that to perform a power analysis we needed to know values for 3 out of 4, where the goal of the analysis is to calculate the 4th. For example, tin the typical power analysis we want to know the number of participants that we will need for our study given a known effect size, d=.4 and a desired power-level, typically .8. In the case that we are running a independent (two) sample t-test with an \\(\\alpha\\) of .05, we can run this analysis algorithmically by calling the following in R:\n\nlibrary(pwr)\npwr.t.test(n = NULL, d = .4, power = .8, sig.level = 0.05,\n               type = c(\"two\"), alternative = c(\"two.sided\"))\n\n\n     Two-sample t test power calculation \n\n              n = 99.08032\n              d = 0.4\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nIn this case, our output indicates that we will need approximately 100 participants per group."
  },
  {
    "objectID": "week07/7_4-simulating_power.html#simulated-power-analysis-an-example-part-1",
    "href": "week07/7_4-simulating_power.html#simulated-power-analysis-an-example-part-1",
    "title": "\n31  Simulating Power!!!\n",
    "section": "\n31.3 Simulated power analysis, an example, part 1",
    "text": "31.3 Simulated power analysis, an example, part 1\nAs an alternative, we might also elect to run our power analysis using Monte Carlo simulation methods. In this simple case we start by creating simulated samples given our effect size parameter. For example, starting with an effect size of d=.40 we can create 2 groups, a control group with a standard distribution (M = 0, SD = 1) and an experimental group with a mean of 0.4, and SD = 1\n\ncontrol_group <- rnorm(n=100, mean = 0, sd = 1)\nexperimental_group <- rnorm(n = 100, mean = 0.4, sd = 1)\n\nWe then run a independent samples t-test comparing the two groups\n\nt.test(x = control_group, y = experimental_group, paired = F, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  control_group and experimental_group\nt = -2.4205, df = 198, p-value = 0.0164\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.6387013 -0.0652081\nsample estimates:\n  mean of x   mean of y \n-0.01475916  0.33719553 \n\n\nNow lets simulate this process 1000 times. First, we build a tibble that simulates 1000 control and experimental samples of size n = 100. Each row in the tibble sample_simulations contains a list containing the control_sample, a list containing the experimental_sample and their respective means.\n\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(broom)\n\nsample_simulations <- tibble(simulation = 1:1000) %>% \n  mutate(control_sample = map(simulation,~rnorm(n = 100,mean = 0,sd = 1)),\n         experimental_sample = map(simulation,~rnorm(n = 100,mean = 0.4,sd = 1)),\n         mean_control = map_dbl(control_sample, ~mean(.)),\n         mean_experimental = map_dbl(experimental_sample, ~mean(.))\n         )  \n\nWe then follow this up by running a simulation t.test() for each row in the sample_simulations. The tibble t_test_simulations, contains the tidy() output for each t.test()\n\nlibrary(broom)\nt_test_simulations <- tibble()\n\nfor (test_sim in 1:1000){\n  control <- sample_simulations$control_sample[test_sim] %>% unlist()\n  experimental <- sample_simulations$experimental_sample[test_sim] %>% unlist()\n  t_test_results <- t.test(control,experimental,paired=F,var.equal=T)\n  t_test_simulations <- bind_rows(t_test_simulations,tidy(t_test_results))\n}\n\nLet’s take a look at the distribution of p.values from our simulations:\n\nlibrary(cowplot)\n\nt_statistic_plot <- ggplot(t_test_simulations, aes(x=statistic)) +\n  geom_density() + theme_cowplot()\n\np_value_plot <- ggplot(t_test_simulations, aes(x=p.value)) +\n  geom_density() + theme_cowplot()\n\ncowplot::plot_grid(t_statistic_plot, p_value_plot,nrow = 2)\n\n\n\n\nFinally, we can ask ourselves what percentage of our obtained p-values are p < .05.\n\nmean(t_test_simulations$p.value<.05)\n\n[1] 0.781\n\n\nThis is the simulated power, and matches the power obtained using pwr.t.test."
  },
  {
    "objectID": "week07/7_4-simulating_power.html#simulated-power-analysis-an-example-part-2",
    "href": "week07/7_4-simulating_power.html#simulated-power-analysis-an-example-part-2",
    "title": "\n31  Simulating Power!!!\n",
    "section": "\n31.4 Simulated power analysis, an example, part 2",
    "text": "31.4 Simulated power analysis, an example, part 2\nWe can reconfigure our simulations to ask a more practical question: how many participants do we need to reach our desired power. In this case, I’m going to start with a realistic minimum number of participants, 10 per group, and work my way up by 5s to a realistic maximum of 200.\n\nparticipants <- seq(10,200,5)\n\nsimulations <- 1:1000\n\nNow let’s redo the simulation above, but this time running 1000 simulations per n_participants. First we create an empty tibble crossing n_participants and n_simulations.\n\nsample_simulations <- tibble(n_participants = rep(participants, \n                                                  each = length(simulations)\n                                                  ),\n                             n_simulations = rep(simulations, \n                                                times = length(participants)\n                                                  ),\n                             )\n\nWe then run the sample simulations, running 1000 simulations for each sample size of n_participants\n\nsample_simulations <- sample_simulations %>% \n  mutate(control_sample = map(n_participants,~rnorm(.,mean = 0,sd = 1)),\n         experimental_sample = map(n_participants,~rnorm(.,mean = 0.4,sd = 1)),\n         mean_control = map_dbl(control_sample, ~mean(.)),\n         mean_experimental = map_dbl(experimental_sample, ~mean(.))\n         ) \n\nWe then run a t.test for each simulation:\n\nt_test_simulations <- tibble()\n\nfor (test_sim in 1:nrow(sample_simulations)){\n  control <- sample_simulations$control_sample[test_sim] %>% unlist()\n  experimental <- sample_simulations$experimental_sample[test_sim] %>% unlist()\n  t_test_results <- t.test(control,experimental,paired=F,var.equal=T)\n  t_test_simulations <- bind_rows(t_test_simulations,tidy(t_test_results))\n}\n\nFinally we combine our two tables…\n\nsimulation_outcomes <- dplyr::bind_cols(sample_simulations, t_test_simulations)\n\ngenerate a summary table containing the proportion of tests that yielded a significant result…\n\npower_table <- simulation_outcomes %>% \n  dplyr::group_by(n_participants) %>%\n  dplyr::summarize(power = mean(p.value<.05))\n\nand plot, here the horizontal line represents desired power of .80. Essentially we a looking for the first point that is above this line, which is at about 100 participants.\n\nggplot(power_table, aes(x = n_participants, y = power)) +\n  geom_point() +\n  geom_line() + \n  geom_hline(yintercept = .8, lty = \"dashed\")"
  },
  {
    "objectID": "week07/7_4-simulating_power.html#additional-resources",
    "href": "week07/7_4-simulating_power.html#additional-resources",
    "title": "\n31  Simulating Power!!!\n",
    "section": "\n33.1 Additional resources",
    "text": "33.1 Additional resources\n\nThe Superpower Github repository (https://github.com/arcaldwell49/Superpower) provides a more comprehensive walkthrough of Superpower using slightly more complex examples than provided here.\nPower Analysis with Superpower (https://aaroncaldwell.us/SuperpowerBook/): is a fully comprehensive web-based text that goes into great detail with how to conduct power analyses for complex ANOVA designs."
  },
  {
    "objectID": "week08/8_1-one-way-anova.html#anovas-for-comparing-means",
    "href": "week08/8_1-one-way-anova.html#anovas-for-comparing-means",
    "title": "\n32  The One-way ANOVA\n",
    "section": "\n32.1 ANOVAs for comparing means",
    "text": "32.1 ANOVAs for comparing means\nThis week we covered ANOVA. A major emphasis in the material (especially Winter’s optional text) is that ANOVA (much like many of the other tests that we cover in this course) is an extension of the general linear model. In many respects, ANOVA and regression are conceptually identical—whereas in linear regression our predictor variables are typically continuous (or, in some cases ordinal) we usually reserve the term ANOVA for instances when our predictors are discrete or nominal (although it really need not be). This would be the difference say in predicting weight as a function of height (linear regression) in contrast to weight as function of hometown (Dayton, Youngstown, Cleveland, Cincinnati). Given that the assigned readings (and the optional Winter text) do a wonderful job of explaining the underlying principles of ANOVA, I won’t spend too much time here rehashing what is already available there. Both the Flora text and the Winter text, especially does an excellent job of demonstrating how even though regression and ANOVA are often treated differently in terms of research focus (e.g., observation v. experimentation) and data focus (correlation/goodness of fit v. comparing means) they are indeed one and the same. Here, my goal is to reinforce this idea using examples in R, as well as providing a practical tutorial that will serve as our entry point into ANOVA.\nWe typically understand ANOVA as a method for allowing us to compare means from more than two samples. To see how this connects with what we have learned from regression lets use an example dataset. Before proceeding, this walk-through assumes that you have the following packages installed and loaded in R:\n\npacman::p_load(car,\n               cowplot, \n               tidyverse,\n               psych)"
  },
  {
    "objectID": "week08/8_1-one-way-anova.html#the-data",
    "href": "week08/8_1-one-way-anova.html#the-data",
    "title": "\n32  The One-way ANOVA\n",
    "section": "\n32.2 The data",
    "text": "32.2 The data\nTo start, lets download Eysenck’s (1974) study on verbal recall and levels of processing. As described in Howell (2012):\n\nCraik and Lockhart (1972) proposed as a model of memory that the degree to which verbal material is remembered by the subject is a function of the degree to which it was processed when it was initially presented. Thus, for example, if you were trying to memorize a list of words, repeating a word to yourself (a low level of processing) would not lead to as good recall as thinking about the word and trying to form associations between that word and some other word. Eysenck (1974) was interested in testing this model and, more important, in looking to see whether it could help to explain reported differences between young and old subjects in their ability to recall verbal material…\nEysenck randomly assigned 50 subjects between the ages of 55 and 65 years to one of five groups—four incidental-learning groups and one intentional-learning group. (Incidental learning is learning in the absence of the expectation that the material will later need to be recalled.) The Counting group was asked to read through a list of words and simply count the number of letters in each word. This involved the lowest level of processing, because subjects did not need to deal with each word as anything more than a collection of letters. The Rhyming group was asked to read each word and think of a word that rhymed with it. This task involved considering the sound of each word, but not its meaning. The Adjective group had to process the words to the extent of giving an adjective that could reasonably be used to modify each word on the list. The Imagery group was instructed to try to form vivid images of each word. This was assumed to require the deepest level of processing of the four incidental conditions. None of these four groups were told that they would later be asked for recall of the items. Finally, the Intentional group was told to read through the list and to memorize the words for later recall. After subjects had gone through the list of 27 items three times, they were given a sheet of paper and asked to write down all of the words they could remember. If learning involves nothing more than being exposed to the material (the way most of us read a newspaper or, heaven forbid, a class assignment), then the five groups should have shown equal recall—after all, they all saw all of the words. If the level of processing of the material is important, then there should have been noticeable differences among the group means.\n\nWe can download this dataset using the following code:\n\ndataset <- read_delim(\"https://www.uvm.edu/~statdhtx/methods8/DataFiles/Ex11-1.dat\", \n                      delim = \"\\t\")\n\nRows: 50 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\ndbl (2): GROUP, RECALL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nshow(dataset)\n\n# A tibble: 50 × 2\n   GROUP RECALL\n   <dbl>  <dbl>\n 1     1      9\n 2     1      8\n 3     1      6\n 4     1      8\n 5     1     10\n 6     1      4\n 7     1      6\n 8     1      5\n 9     1      7\n10     1      7\n# ℹ 40 more rows"
  },
  {
    "objectID": "week08/8_1-one-way-anova.html#getting-the-data-in-order",
    "href": "week08/8_1-one-way-anova.html#getting-the-data-in-order",
    "title": "\n32  The One-way ANOVA\n",
    "section": "\n32.3 Getting the data in order",
    "text": "32.3 Getting the data in order\nAlthough the focus for this course is conceptual and applied knowledge of statistics, I again want to be mindful of the practice of data analysis. That is, in the real world, you’ll be asked to do some data wrangling, or getting your data in the right format and containing the right information to properly proceed with analysis. When looking at this data, two important things jump out that we might want to address: (1) there is no column indicating participant ID. and (2) the data is numerically coded. A little bit about each of these points.\n\n32.3.1 The importance of a participant ID column\nDepending on what statistical software you use, including a Participant ID column varies in its importance. For building statistical models in R that involve categorical data, including designs that that test within and across subjects making sure that you have Participant IDs coded is vitally important. Ideally this will already be done before you import your data, but on the case that it’s not you need to add a column specifying participant ID. How you do this will depend on what format your data is in (Wide v. Long) and whether you have a repeated-measures design (multiple measures from each participant).\n** I grant that it might be useful to use you Excel ninja skills here and get everything in its right place before importing, but will assume that you want to stay in an R environment.\nIn this case, our data is in long format and we have a between subjects design. That is every line represents a single measure of a participant and every participant is represented only once. In this case we can just assign a unique number to each participant / row. In most cases this is a simple as assigning participant numbers {1,2,3…n}. An easy way to do this is to use the seq_along() function. The seq_along() function simply says to create numbers in sequence that match along a vector. In this case we can seq_along the GROUP column (vector).\n\ndataset <- dataset %>% mutate(PartID = seq_along(dataset$GROUP))\ndataset\n\n# A tibble: 50 × 3\n   GROUP RECALL PartID\n   <dbl>  <dbl>  <int>\n 1     1      9      1\n 2     1      8      2\n 3     1      6      3\n 4     1      8      4\n 5     1     10      5\n 6     1      4      6\n 7     1      6      7\n 8     1      5      8\n 9     1      7      9\n10     1      7     10\n# ℹ 40 more rows\n\n\nFWIW, if you have the same compulsion that I do and insist that your PartID column is first in your dataset this can be solved using select. Here, I’m saying put PartID first and then follow it with everything() else.\n\ndataset <- dataset %>% select(PartID, everything())\n\nOK. That’s addressed. Now onto issue #2.\n\n32.3.2 Recoding the factors (if number coded)\nAs we can see the data is number coded. In this case,\n\n1 = ‘Counting’,\n2 = ‘Rhyming’,\n3 = ‘Adjective’,\n4 = ‘Imagery’ and\n5 = ‘Intentional’\n\nMy advice for what to do if you get dummy coded data is to create a corresponding column in your data set that contains the factors in nominal (name) format.\nRecall from previous weeks that we can use the recode() and recode_factor()functions to reassign the number variables. Here, we are recoding the levels of a factor, so the preferred function is recode_factor(). When we recode_factor() we have the added benefit of automatically factorizing, telling R to treat the IV as a factor. Let’s create a new column dataset$GROUP_FACTOR that contains this data:\n\n# assigning the appropriate names for the dummy codes\ndataset <- dataset %>% \n  mutate(GROUP_FACTOR = dplyr::recode_factor(GROUP, \n                                             \"1\" = \"Counting\",\n                                             \"2\"=\"Rhyming\",\n                                             \"3\"=\"Adjective\",\n                                             \"4\"=\"Imagery\",\n                                             \"5\"=\"Intentional\"\n                                             )\n  )\n\n\n32.3.3 Renaming column headers (optional)\nAnd now, just to be clear, let’s rename the original GROUP to GROUP_NUM. This can be accomplished by using the dplyr::rename() function.\n\n# The template is rename(NEW_NAME = OLD_NAME)\ndataset <- dataset %>% dplyr::rename(GROUP_NUM = GROUP)\n\nCheck out this link for info on how to rename multiple columns at once using names() or dplyr::rename() from the tidyverse.\n\n32.3.4 Reordering your levels\nOne important question you should ask before you look at your data is “what is your control (group, condition)”. Proper experimentation requires a proper control in order to properly isolate the influence of the manipulation (that’s a lot of propers). Here, the best candidates for our control group might either be “Counting” or “Intentional”, depending on how the original problem was approached. If the larger comparison involved “Intentional v. incidental” learning for recall, then the “Intentional” group serves best as your control. If the original question involved levels of processing, then “Counting”” (theoretically the lowest level of incidental processing) is best. Here I am assuming the latter (although I believe theoretically Eysenck originally was interested in the former).\nI bring this up, as its typically best to ensure that your control is entered first into the ANOVA model. To check the order of your levels, you may simply:\n\nlevels(dataset$GROUP_FACTOR)\n\n[1] \"Counting\"    \"Rhyming\"     \"Adjective\"   \"Imagery\"     \"Intentional\"\n\n\nHere we see that “Counting” is first and will therefore be entered first into the model.\nAssuming that we wanted to reorder the sequence, say to have Intentional as the control, then we might simply use fct_relevel():\n\ndataset <- dataset %>% \n  mutate(GROUP_FACTOR = fct_relevel(GROUP_FACTOR, \"Intentional\"))\n                              \nlevels(dataset$GROUP_FACTOR)\n\n[1] \"Intentional\" \"Counting\"    \"Rhyming\"     \"Adjective\"   \"Imagery\"    \n\n\nNote that with fct_relevel() whatever level(s) I list get moved to the front of the line. In this case I just bumped Intentional up while keeping the remaining levels ordered the same.\nHowever, I liked the original order, so let’s change it back:\n\ndataset <- dataset %>% \n  mutate(GROUP_FACTOR = fct_relevel(GROUP_FACTOR, \n                                    \"Counting\", \n                                    \"Rhyming\", \n                                    \"Adjective\", \n                                    \"Imagery\", \n                                    \"Intentional\"))\n\nlevels(dataset$GROUP_FACTOR)\n\n[1] \"Counting\"    \"Rhyming\"     \"Adjective\"   \"Imagery\"     \"Intentional\"\n\n\nThat’s better. Why the order is important will be made clear later in this write up. For now, think back to our example in class of running a t-test using lm(). You may recall that the group level that was first entered into the model served as the model intercept where the second group level was expressed in terms of the slope of the line (beta). A similar output will be happening here.\nSo the data structure looks good.\nOne last little bit of ninja-ing. If you wanted to save a particular R object to be used later on you can use the write_rds() function (from tidyverse). I’m going to save dataset as I’ve cleaned it up so that I can use it in the next walkthrough.\n\nwrite_rds(dataset,'clean_dataset.rds')\n\nOnward to the analysis!!!!"
  },
  {
    "objectID": "week08/8_1-one-way-anova.html#assumptions-for-anova",
    "href": "week08/8_1-one-way-anova.html#assumptions-for-anova",
    "title": "\n32  The One-way ANOVA\n",
    "section": "\n32.4 Assumptions for ANOVA",
    "text": "32.4 Assumptions for ANOVA\n\n32.4.1 Checking the normality assumption, OPTION 1 raw data by group\nTo check the distribution of outcomes in ANOVA, you have two options. The first would be to check the distribution of outcomes for EACH group/condition independently. In the case of the example dataset we could get info related to the skew and kurtosis of RECALL for each GROUP_FACTOR using describeBy():\n\npsych::describeBy(dataset,group = \"GROUP_FACTOR\")\n\n\n Descriptive statistics by group \nGROUP_FACTOR: Counting\n              vars  n mean   sd median trimmed  mad min max range skew kurtosis\nPartID           1 10  5.5 3.03    5.5     5.5 3.71   1  10     9    0    -1.56\nGROUP_NUM        2 10  1.0 0.00    1.0     1.0 0.00   1   1     0  NaN      NaN\nRECALL           3 10  7.0 1.83    7.0     7.0 1.48   4  10     6    0    -1.22\nGROUP_FACTOR*    4 10  1.0 0.00    1.0     1.0 0.00   1   1     0  NaN      NaN\n                se\nPartID        0.96\nGROUP_NUM     0.00\nRECALL        0.58\nGROUP_FACTOR* 0.00\n------------------------------------------------------------ \nGROUP_FACTOR: Rhyming\n              vars  n mean   sd median trimmed  mad min max range skew kurtosis\nPartID           1 10 15.5 3.03   15.5   15.50 3.71  11  20     9 0.00    -1.56\nGROUP_NUM        2 10  2.0 0.00    2.0    2.00 0.00   2   2     0  NaN      NaN\nRECALL           3 10  6.9 2.13    6.5    6.88 0.74   3  11     8 0.18    -0.40\nGROUP_FACTOR*    4 10  2.0 0.00    2.0    2.00 0.00   2   2     0  NaN      NaN\n                se\nPartID        0.96\nGROUP_NUM     0.00\nRECALL        0.67\nGROUP_FACTOR* 0.00\n------------------------------------------------------------ \nGROUP_FACTOR: Adjective\n              vars  n mean   sd median trimmed  mad min max range  skew\nPartID           1 10 25.5 3.03   25.5   25.50 3.71  21  30     9  0.00\nGROUP_NUM        2 10  3.0 0.00    3.0    3.00 0.00   3   3     0   NaN\nRECALL           3 10 11.0 2.49   11.0   11.25 2.97   6  14     8 -0.66\nGROUP_FACTOR*    4 10  3.0 0.00    3.0    3.00 0.00   3   3     0   NaN\n              kurtosis   se\nPartID           -1.56 0.96\nGROUP_NUM          NaN 0.00\nRECALL           -0.84 0.79\nGROUP_FACTOR*      NaN 0.00\n------------------------------------------------------------ \nGROUP_FACTOR: Imagery\n              vars  n mean   sd median trimmed  mad min max range skew kurtosis\nPartID           1 10 35.5 3.03   35.5   35.50 3.71  31  40     9 0.00    -1.56\nGROUP_NUM        2 10  4.0 0.00    4.0    4.00 0.00   4   4     0  NaN      NaN\nRECALL           3 10 13.4 4.50   11.5   12.75 1.48   9  23    14 0.99    -0.53\nGROUP_FACTOR*    4 10  4.0 0.00    4.0    4.00 0.00   4   4     0  NaN      NaN\n                se\nPartID        0.96\nGROUP_NUM     0.00\nRECALL        1.42\nGROUP_FACTOR* 0.00\n------------------------------------------------------------ \nGROUP_FACTOR: Intentional\n              vars  n mean   sd median trimmed  mad min max range skew kurtosis\nPartID           1 10 45.5 3.03   45.5    45.5 3.71  41  50     9 0.00    -1.56\nGROUP_NUM        2 10  5.0 0.00    5.0     5.0 0.00   5   5     0  NaN      NaN\nRECALL           3 10 12.0 3.74   11.0    12.0 2.97   5  19    14 0.05    -0.47\nGROUP_FACTOR*    4 10  5.0 0.00    5.0     5.0 0.00   5   5     0  NaN      NaN\n                se\nPartID        0.96\nGROUP_NUM     0.00\nRECALL        1.18\nGROUP_FACTOR* 0.00\n\n\nIf we wanted to perform for extensive methods like hist(), qqPlot(), and shapiro.test(), in the past I had you filter be each level (group) and proceed. So for example for Counting:\n\ncountingGroup <- filter(dataset,GROUP_FACTOR==\"Counting\")\n\nhist(countingGroup$RECALL)\n\n\n\nqqPlot(countingGroup$RECALL)\n\n\n\n\n[1] 6 5\n\nshapiro.test(countingGroup$RECALL)\n\n\n    Shapiro-Wilk normality test\n\ndata:  countingGroup$RECALL\nW = 0.98372, p-value = 0.9819\n\n\nIn the past you would have to repeat this for each other level. An alternative to generate an output by each level of a factor is to use the by() function. For example to generate a sequence of qqPlots (for the sake of space I’m not going to execute this code here, but try on your computer)\n\n# by(dependent variable, grouping factor, name of function)\nby(dataset$RECALL,INDICES = dataset$GROUP_FACTOR,qqPlot)\n\nYou can do the same with hist() and shapiro.test()\n\nby(dataset$RECALL,INDICES = dataset$GROUP_FACTOR,hist)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndataset$GROUP_FACTOR: Counting\n$breaks\n[1]  4  5  6  7  8  9 10\n\n$counts\n[1] 2 2 2 2 1 1\n\n$density\n[1] 0.2 0.2 0.2 0.2 0.1 0.1\n\n$mids\n[1] 4.5 5.5 6.5 7.5 8.5 9.5\n\n$xname\n[1] \"dd[x, ]\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n------------------------------------------------------------ \ndataset$GROUP_FACTOR: Rhyming\n$breaks\n[1]  2  4  6  8 10 12\n\n$counts\n[1] 1 4 3 1 1\n\n$density\n[1] 0.05 0.20 0.15 0.05 0.05\n\n$mids\n[1]  3  5  7  9 11\n\n$xname\n[1] \"dd[x, ]\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n------------------------------------------------------------ \ndataset$GROUP_FACTOR: Adjective\n$breaks\n[1]  6  8 10 12 14\n\n$counts\n[1] 2 1 3 4\n\n$density\n[1] 0.10 0.05 0.15 0.20\n\n$mids\n[1]  7  9 11 13\n\n$xname\n[1] \"dd[x, ]\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n------------------------------------------------------------ \ndataset$GROUP_FACTOR: Imagery\n$breaks\n[1]  8 10 12 14 16 18 20 22 24\n\n$counts\n[1] 2 5 0 1 0 1 0 1\n\n$density\n[1] 0.10 0.25 0.00 0.05 0.00 0.05 0.00 0.05\n\n$mids\n[1]  9 11 13 15 17 19 21 23\n\n$xname\n[1] \"dd[x, ]\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n------------------------------------------------------------ \ndataset$GROUP_FACTOR: Intentional\n$breaks\n[1]  4  6  8 10 12 14 16 18 20\n\n$counts\n[1] 1 0 2 3 2 1 0 1\n\n$density\n[1] 0.05 0.00 0.10 0.15 0.10 0.05 0.00 0.05\n\n$mids\n[1]  5  7  9 11 13 15 17 19\n\n$xname\n[1] \"dd[x, ]\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n\nby(dataset$RECALL,INDICES = dataset$GROUP_FACTOR,shapiro.test)\n\ndataset$GROUP_FACTOR: Counting\n\n    Shapiro-Wilk normality test\n\ndata:  dd[x, ]\nW = 0.98372, p-value = 0.9819\n\n------------------------------------------------------------ \ndataset$GROUP_FACTOR: Rhyming\n\n    Shapiro-Wilk normality test\n\ndata:  dd[x, ]\nW = 0.93024, p-value = 0.4502\n\n------------------------------------------------------------ \ndataset$GROUP_FACTOR: Adjective\n\n    Shapiro-Wilk normality test\n\ndata:  dd[x, ]\nW = 0.90607, p-value = 0.2551\n\n------------------------------------------------------------ \ndataset$GROUP_FACTOR: Imagery\n\n    Shapiro-Wilk normality test\n\ndata:  dd[x, ]\nW = 0.81967, p-value = 0.02511\n\n------------------------------------------------------------ \ndataset$GROUP_FACTOR: Intentional\n\n    Shapiro-Wilk normality test\n\ndata:  dd[x, ]\nW = 0.94339, p-value = 0.5913\n\n\nand even our custom standardized skew and kurtosis functions.\n\n32.4.2 Checking the normality assumption, OPTION 2, the better way: Look at the residuals.\nAlthough by() may or may not make life easier for you in this test case, things rapidly become more complicated when attempting to check normality by condition. For example if you’re running a 2×3×3 mixed effect ANOVA, you would need to run through 18 conditions!!! So what to do?!?!?!\nA simpler alternative is to run you model and analyze your residuals. This web link does a nice and quick job of explaining the logic.\nIn this case we would run our ANOVA model using lm (see told you its all the same)\n\ndataset_aov <- lm(RECALL~GROUP_FACTOR, data=dataset)\n\nCongrats, you’ve just run an ANOVA, but for now we aren’t interested in the results from the model. Remember from a few weeks back that many outputs have attributes that may be accessed. In this case, we want the models residuals. These can be accessed by\n\nresid(dataset_aov)\n\n            1             2             3             4             5 \n 2.000000e+00  1.000000e+00 -1.000000e+00  1.000000e+00  3.000000e+00 \n            6             7             8             9            10 \n-3.000000e+00 -1.000000e+00 -2.000000e+00  5.551115e-17  5.551115e-17 \n           11            12            13            14            15 \n 1.000000e-01  2.100000e+00 -9.000000e-01 -9.000000e-01 -9.000000e-01 \n           16            17            18            19            20 \n 4.100000e+00 -9.000000e-01 -3.900000e+00  1.100000e+00  1.000000e-01 \n           21            22            23            24            25 \n 5.551115e-17  2.000000e+00 -3.000000e+00 -5.000000e+00  3.000000e+00 \n           26            27            28            29            30 \n 5.551115e-17  2.000000e+00  2.000000e+00 -1.000000e+00  5.551115e-17 \n           31            32            33            34            35 \n-1.400000e+00 -2.400000e+00  2.600000e+00 -2.400000e+00 -4.400000e+00 \n           36            37            38            39            40 \n 9.600000e+00 -1.400000e+00 -3.400000e+00  5.600000e+00 -2.400000e+00 \n           41            42            43            44            45 \n-2.000000e+00  7.000000e+00  2.000000e+00 -7.000000e+00 -2.000000e+00 \n           46            47            48            49            50 \n-1.000000e+00  2.000000e+00  3.000000e+00 -1.000000e+00 -1.000000e+00 \n\n\nFrom here, we can simply take the residuals (as we did with regression) and submit them to our standard tests for normality. For example, testing skew, and kurtosis:\n\nresid(dataset_aov) %>% skew()\n\n[1] 0.6388343\n\nresid(dataset_aov) %>% kurtosi()\n\n[1] 1.221947\n\n\nor submit the entire model diagnostic plots:\n\npacman::p_load(performance)\n\nperformance::check_model(dataset_aov)\n\n\n\n\nSo, between OPTION 1 and OPTION 2, I’d recommend typically going with OPTION 2. Especially as your ANOVA designs become more complex.\n\n32.4.2.1 WALKTHROUGH PROBLEM: Run the requisite tests for normality using the residuals from dataset_aov\n\n\n32.4.3 Homogeneity of Variance\nAnother assumption of ANOVA is the homogeneity of variance between groups. An easy-way to get an eyeball test of this assumption is two perform a box plot of the data. Here I am performing this plot using ggplot2:\n\nggplot(data =dataset, aes(x=GROUP_FACTOR,y=RECALL)) +\n  geom_boxplot()\n\n\n\n\nHuge differences in the IQR regions may be a clue that the homogeneity assumption is violated. We can run more specific tests in R including the Levene Test and Fligner-Killeen Test of Homogeneity of Variances. You are familiar with the Levene Test from a few weeks ago. The Figner-Killeen Test is offered as an alternative if you are concerned with violations of the normality assumption.\nAs is typically the case \\(p<.05\\) indicates a violation of this assumption:\n\n# Levene Test of Homogeneity of Variances\ncar::leveneTest(RECALL~GROUP_FACTOR, data=dataset)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  4  0.8932 0.4759\n      45               \n\n# Fligner-Killeen Test of Homogeneity of Variances\nfligner.test(RECALL~GROUP_FACTOR, data=dataset)\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  RECALL by GROUP_FACTOR\nFligner-Killeen:med chi-squared = 1.9733, df = 4, p-value = 0.7407\n\n\n\n32.4.4 What to do if the assumptions are violated?\nIf either assumption is violated, one option that you have is to transform you data. We’ve talked several times in class about the pros and cons of doing this, and there are several resources that provide examples of how this is done. Another option is to use a non-parametric test such as the Kruskal-Wallis Test if the data is not normal, or Welch’s ANOVA is the variances are not homogeneous. That said, one of reasons that ANOVA is so popular is that it has been demonstrated to be robust in the face of violated assumptions (as long as the sample sizes are equal). For example, in Design and Analysis of Experiments (1999, p. 112) Dean & Voss argue that the maximum group variance may be as high as 3× the minimum group variance without any issue. With this in mind, a question (gray area) before us is how much of a violation is there in the data? And if not so much, you may be fine just running an ANOVA regardless."
  },
  {
    "objectID": "week08/8_1-one-way-anova.html#running-the-anova-in-r-using-aov-or-lm",
    "href": "week08/8_1-one-way-anova.html#running-the-anova-in-r-using-aov-or-lm",
    "title": "\n32  The One-way ANOVA\n",
    "section": "\n32.5 Running the ANOVA in R using aov() or lm()\n",
    "text": "32.5 Running the ANOVA in R using aov() or lm()\n\nThere are many, many ways to build an ANOVA model in R. Throughout the semester we will be highlighting three: lm(), aov(), and using the afex package. This week we will concentrate on aov() which is the standard method, as well as the lm() method that you have used before, just to reinforce that ANOVA and regression are one in the same. In fact, SPOILER ALERT, aov() is just a fancy wrapper for lm(), AND FWIW I don’t really like using aov(). I only mention it here as if you go looking for how to run ANOVA in R online, many sites will say use aov().\nLike lm() from weeks past, aov() asks us to enter our dependent and independent variables into the model in the formula format DV ~ IVs. In this case, we only have a single IV, GROUP_FACTOR. Thus our model is:\n\naov.model <- aov(RECALL~GROUP_FACTOR,data = dataset)\n\nFrom here, an anova() of the model gives us our ANOVA table. Note that car::Anova() accomplished the same thing, though gives us a different format. The added benefit is that if we so choose, we can change how our Sums of Squares are calculated. This isn’t important for simple One-way ANOVA, but see the Field text, Jane Superbrain 11.1 for an explanation.\n\nanova(aov.model)\n\nAnalysis of Variance Table\n\nResponse: RECALL\n             Df Sum Sq Mean Sq F value    Pr(>F)    \nGROUP_FACTOR  4 351.52  87.880  9.0848 1.815e-05 ***\nResiduals    45 435.30   9.673                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(aov.model, type=\"3\")\n\nAnova Table (Type III tests)\n\nResponse: RECALL\n             Sum Sq Df F value    Pr(>F)    \n(Intercept)  490.00  1 50.6547 6.831e-09 ***\nGROUP_FACTOR 351.52  4  9.0848 1.815e-05 ***\nResiduals    435.30 45                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAlso note that we can get residuals from the aov() output as well. In fact take a look at the object’s $class… see I told you, lm()!!!\n\nresid(aov.model)\n\n            1             2             3             4             5 \n 2.000000e+00  1.000000e+00 -1.000000e+00  1.000000e+00  3.000000e+00 \n            6             7             8             9            10 \n-3.000000e+00 -1.000000e+00 -2.000000e+00  5.551115e-17  5.551115e-17 \n           11            12            13            14            15 \n 1.000000e-01  2.100000e+00 -9.000000e-01 -9.000000e-01 -9.000000e-01 \n           16            17            18            19            20 \n 4.100000e+00 -9.000000e-01 -3.900000e+00  1.100000e+00  1.000000e-01 \n           21            22            23            24            25 \n 5.551115e-17  2.000000e+00 -3.000000e+00 -5.000000e+00  3.000000e+00 \n           26            27            28            29            30 \n 5.551115e-17  2.000000e+00  2.000000e+00 -1.000000e+00  5.551115e-17 \n           31            32            33            34            35 \n-1.400000e+00 -2.400000e+00  2.600000e+00 -2.400000e+00 -4.400000e+00 \n           36            37            38            39            40 \n 9.600000e+00 -1.400000e+00 -3.400000e+00  5.600000e+00 -2.400000e+00 \n           41            42            43            44            45 \n-2.000000e+00  7.000000e+00  2.000000e+00 -7.000000e+00 -2.000000e+00 \n           46            47            48            49            50 \n-1.000000e+00  2.000000e+00  3.000000e+00 -1.000000e+00 -1.000000e+00 \n\n\nEven better, a more comprehensive anova table can be obtained by submitting our aov.model to sjstats::anova_stats(), which gives us our effect sizes as well! This function calculates eta-squared (\\(\\eta^2\\)), partial-eta-squared (\\(\\eta_p^2\\)), omega squared (\\(\\omega^2\\)) and partial omega-squared (\\(\\omega_p^2\\)) as well as Cohen’s f. Typically for One-Way ANOVA psychologists report eta-squared (\\(\\eta^2\\)), though there are arguments that omega squared (\\(\\omega^2\\)) may be the more preferred measure. Note that pander is for web formatting this page.\n\npacman::p_load(pander)\npacman::p_load(sjstats)\nsjstats::anova_stats(aov.model) %>% pander()\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\n\n \nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\nGROUP_FACTOR\nGROUP_FACTOR\n4\n351.5\n87.88\n9.085\n0\n\n\n…2\nResiduals\n45\n435.3\n9.673\nNA\nNA\n\n\n\n\nTable continues below\n\n\n\n\n\n\n\n\n \netasq\npartial.etasq\nomegasq\npartial.omegasq\n\n\n\nGROUP_FACTOR\n0.447\n0.447\n0.393\n0.393\n\n\n…2\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\n \nepsilonsq\ncohens.f\npower\n\n\n\nGROUP_FACTOR\n0.398\n0.899\n0.999\n\n\n…2\nNA\nNA\nNA"
  },
  {
    "objectID": "week08/8_1-one-way-anova.html#plotting-your-data",
    "href": "week08/8_1-one-way-anova.html#plotting-your-data",
    "title": "\n32  The One-way ANOVA\n",
    "section": "\n32.6 Plotting your data",
    "text": "32.6 Plotting your data\nFor publication quality ANOVA plots, there are typically three acceptable plots used to convey your results, box plots, bar plots, and line plots. As the ANOVA become more complex, we tend towards using line plots (box plots and bar plots may become very busy in complex designs). Unless there are other compelling reasons we tend to plot the means (although note that box plots usually give you medians). Since you have already produced box plots and barplots, I’ll show examples of how to do a line plot.\nTo create a line plot with points at the means and error lines representing the 95% CI (known as a point range), we can use call that you are familiar with and a new geom pointrange. Whats nice about point range is that it allows you to create your points and the error bars all in a single line.\n\nggplot2::ggplot(data =dataset,mapping = aes(x = GROUP_FACTOR, y = RECALL)) +\n  stat_summary(fun.data = mean_cl_normal, \n               size=1, \n               color=\"black\", \n               geom=\"pointrange\")\n\n\n\n\nNow to add the lines to this plot. For this you will need another stat_summary() line specifying that the vertices of the lines should be the means, fun = mean and a parameter that specifies how the lines should be grouped. Since we have a One-way ANOVA, group=1. When we built to more complex designs you may elect to group=FACTOR_NAME. Something like this is useful to say make some lines dashed and some lines solid according to levels on a factor. More on this in two weeks when we get to factorial ANOVA.\nWhile we’re at it let’s fix those axes titles. We can use the functions xlab() and ylab() to do so.\n\n# this is from before, saving as object \"p\"\np <- ggplot2::ggplot(data =dataset,mapping = aes(x = GROUP_FACTOR, y = RECALL)) +\n  stat_summary(fun.data = mean_cl_normal, \n               size=1, \n               color=\"black\", \n               geom=\"pointrange\") +\n  # adding lines\n  stat_summary(fun = mean, \n               size = 1, \n               color = \"black\", \n               mapping=aes(group=1), \n               geom=\"line\") +\n  # and fixing the axis titles:\n  xlab(\"Group\")+ylab(\"Words recalled\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nshow(p)\n\n\n\n\nFinally, for aesthetic reasons you may elect to expand the y-axis. For example to make the range of y-axis values (0,20):\n\np + expand_limits(y=c(0,20))\n\n\n\n\nThat being said, this data probably lends itself to a bar plot. This is similar to how you built your bar plots from as when doing a t-test. As a matter of fact:\n\n32.6.0.1 WALKTHROUGH PROBLEM: Create a bar plot with error bars for our dataset."
  },
  {
    "objectID": "week08/8_1-one-way-anova.html#reporting-your-data-the-write-up",
    "href": "week08/8_1-one-way-anova.html#reporting-your-data-the-write-up",
    "title": "\n32  The One-way ANOVA\n",
    "section": "\n32.7 Reporting your data / the write up",
    "text": "32.7 Reporting your data / the write up\nThere are two components to reporting your omnibus ANOVA. First, You need to report your output as related to the test:\n\n\nthe F value,\n\nthe degrees of freedom (between and within),\nthe p-value\n\neffect size: typically with between-effects ANOVA we report eta squared, although note that Howell advocates for omega squared. Recall that submitting the model to sjstats::anova_stats() gives you both.\n\nSecond, as we typically focus on means with ANOVA, it is typically a good idea to report means and some report of of the distribution of each group (typically either standard deviation or standard error; although 95% CI may be useful in certain scenarios).\nSo for example reporting the Rhyming group:\n\n\nM ± SD: 6.90 ± 2.13\n\nM ± SE: 6.90 ± 0.67"
  },
  {
    "objectID": "week08/8_1-one-way-anova.html#what-the-omnibus-anova-tells-you",
    "href": "week08/8_1-one-way-anova.html#what-the-omnibus-anova-tells-you",
    "title": "\n32  The One-way ANOVA\n",
    "section": "\n32.8 What the omnibus ANOVA tells you",
    "text": "32.8 What the omnibus ANOVA tells you\nWhile it may be useful to report the means, you need to be mindful of what the omnibus ANOVA tells you. Remember, that the the null hypothesis of the omnibus ANOVA is that “there are no differences between observed means”. Our significant result tells us that there are differences between our means, but does not tell us what those specific differences are. So while it may be useful to report general relationships, e.g., “Recall for people in the Intentional group tended to be greater than for the Counting group” you cannot say definitively “Recall was significantly greater for the Intentional Group”. Typically when you only have tested the omnibus ANOVA, you only speak in generalities (e.g., “Recall tended to increase with level of processing.”)\nAlways be mindful that you don’t over interpret your data."
  },
  {
    "objectID": "week08/8_1-one-way-anova.html#running-an-anova-using-lm-its-a-regression-afterall",
    "href": "week08/8_1-one-way-anova.html#running-an-anova-using-lm-its-a-regression-afterall",
    "title": "\n32  The One-way ANOVA\n",
    "section": "\n32.9 Running an ANOVA using lm() (it’s a regression afterall)",
    "text": "32.9 Running an ANOVA using lm() (it’s a regression afterall)\nNow that we’ve got practical matters out of the way, I want to take some time to dig a little deeper into the connections between ANOVA this week and our work on correlations and regressions in the past.\nAs we’ve already mentioned (and spent time discussing in class) ANOVA is just an extension of the simple linear model that we covered last week, where ANOVA is used when our predictors are discrete. In fact aov() is simply a wrapper for the lm() function that we used last week. For example, let’s run our model using lm() and then pipe it into anova() or sjstats::anova_stats().\n\nlm.model <- lm(RECALL~GROUP_FACTOR, data=dataset)\nanova(lm.model)\n\nAnalysis of Variance Table\n\nResponse: RECALL\n             Df Sum Sq Mean Sq F value    Pr(>F)    \nGROUP_FACTOR  4 351.52  87.880  9.0848 1.815e-05 ***\nResiduals    45 435.30   9.673                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsjstats::anova_stats(lm.model) %>% pander()\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\n\n \nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\nGROUP_FACTOR\nGROUP_FACTOR\n4\n351.5\n87.88\n9.085\n0\n\n\n…2\nResiduals\n45\n435.3\n9.673\nNA\nNA\n\n\n\n\nTable continues below\n\n\n\n\n\n\n\n\n \netasq\npartial.etasq\nomegasq\npartial.omegasq\n\n\n\nGROUP_FACTOR\n0.447\n0.447\n0.393\n0.393\n\n\n…2\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\n \nepsilonsq\ncohens.f\npower\n\n\n\nGROUP_FACTOR\n0.398\n0.899\n0.999\n\n\n…2\nNA\nNA\nNA\n\n\n\n\n\nAll aov() does is take an lm object and produce an ANOVA table from the results. If we were simply to look at the lm() model we see that it gives us the info in our ANOVA table at the end of the summary, including the F-statistic (9.085), the degrees of freedom (4 and 45) and the p-value (1.815e-05):\n\nlm(RECALL~GROUP_FACTOR, data=dataset) %>% summary()\n\n\nCall:\nlm(formula = RECALL ~ GROUP_FACTOR, data = dataset)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -7.00  -1.85  -0.45   2.00   9.60 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)               7.0000     0.9835   7.117 6.83e-09 ***\nGROUP_FACTORRhyming      -0.1000     1.3909  -0.072 0.943004    \nGROUP_FACTORAdjective     4.0000     1.3909   2.876 0.006138 ** \nGROUP_FACTORImagery       6.4000     1.3909   4.601 3.43e-05 ***\nGROUP_FACTORIntentional   5.0000     1.3909   3.595 0.000802 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.11 on 45 degrees of freedom\nMultiple R-squared:  0.4468,    Adjusted R-squared:  0.3976 \nF-statistic: 9.085 on 4 and 45 DF,  p-value: 1.815e-05\n\n\nTaking a look at this output, we see that the lm() model also gives us a lot of other additional useful info. For example the \\(R^2\\) of the model may be understood as the effect size of the ANOVA. However, when we report it for One-Way ANOVA we express it as… dun, dun, dunnnn… eta-squared!!, or \\(\\eta^2\\).\nOne more time with feeling, for simple oneway ANOVA, \\(R^2\\) and \\(\\eta^2\\) are the same damn thing!!\nZooming in on the coefficients:\n\n\n                        Estimate Std. Error     t value     Pr(>|t|)\n(Intercept)                  7.0  0.9835311  7.11721300 6.830993e-09\nGROUP_FACTORRhyming         -0.1  1.3909230 -0.07189471 9.430043e-01\nGROUP_FACTORAdjective        4.0  1.3909230  2.87578833 6.137702e-03\nGROUP_FACTORImagery          6.4  1.3909230  4.60126133 3.425167e-05\nGROUP_FACTORIntentional      5.0  1.3909230  3.59473541 8.019114e-04\n\n\nWe see information about the means of each group relative to the (Intercept). This is why I stressed earlier that it may be useful to rearrange the order of your levels such that R enters your control group into the model first. In this case, the first predictor entered is assigned to the (Intercept). The remaining predictors in the model are then presented relative to the first. Since we entered “Counting” first, the estimate of the (Intercept) represents its mean. The means for each remaining group are the sum of its estimate coefficient and the (Intercept). So for example the mean of the Rhyming group is \\((-0.1)+(7.0)=6.9\\).\nThe coefficients section also gives us one other useful bit of information, the \\(t\\)-values of the estimate. As we learned a few weeks ago, for a simple regression the beta coefficient gives us information about the slope of the regression line and the corresponding \\(t\\)-value is a test of the null \\(beta=0\\). So for a simple regression this tells us if our slope is significantly different from 0.\nA similar logic applies to the ANOVA model. As I mentioned above, deriving the means of each level of our IV is a matter of summing the (Intercept) and the beta estimate for that level. It should be apparent, then, that the beta estimates here represent the slope of a line that between the intercept and the mean of the level (where the distance between the predictor and coefficient is treated as a unit 1). Therefore, a significant \\(t\\)-value for beta tells us that the slope between the two means is significant, or that those two means are significantly different from one another.\nKeep in mind the only comparisons that are being made here are between the individual levels of our IV and the control (Intercept). So while this output allows us to make a claim about the difference between the means of the Counting (Intercept) group compared to each of the other groups, it does not allow us to make a claim about differences between our other levels. For example no information about a statistical test of differences between the Rhyming and Imagery groups is conveyed here. However, assuming you are interested in deviations from your control, you can get info here quickly. There is a caveat here, in that our alpha criterion will need to be adjusted to be more conservative than .05. More on this next week."
  },
  {
    "objectID": "week08/8_1-one-way-anova.html#reporting-your-results.",
    "href": "week08/8_1-one-way-anova.html#reporting-your-results.",
    "title": "\n32  The One-way ANOVA\n",
    "section": "\n32.10 Reporting your results.",
    "text": "32.10 Reporting your results.\n\n32.10.1 Reporting just the ANOVA\nFor this week’s assignment, you are going to be asked to simply report the ANOVA. In future weeks you will see that simply reporting the ANOVA is not the end of your analysis, but only the beginning of additional, necessary follow-ups. But that is for next week, for now let’s take a look at what one might expect to be reported in an ANOVA write-up.\nGiven our example:\n\nwords RECALL is our dependent measure\nGROUP_FACTOR is our independent variable, with five levels (or groups).\n\nUsing sjstats to generate an ANOVA table (pander makes it pretty for this web page:\n\nsjstats::anova_stats(aov.model) %>% pander()\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\n\n \nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\nGROUP_FACTOR\nGROUP_FACTOR\n4\n351.5\n87.88\n9.085\n0\n\n\n…2\nResiduals\n45\n435.3\n9.673\nNA\nNA\n\n\n\n\nTable continues below\n\n\n\n\n\n\n\n\n \netasq\npartial.etasq\nomegasq\npartial.omegasq\n\n\n\nGROUP_FACTOR\n0.447\n0.447\n0.393\n0.393\n\n\n…2\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\n \nepsilonsq\ncohens.f\npower\n\n\n\nGROUP_FACTOR\n0.398\n0.899\n0.999\n\n\n…2\nNA\nNA\nNA\n\n\n\n\n\nFrom this table, we need our:\n\n\ndf: 4 and 45\n\nF-statistic: 9.085\n\np.value: even though it says zero, its “<.001”\n\netasq: .447; the symbol for etasq is \\(\\eta^2\\)\n\n\nTaking all of this info, and plugging into a write-up template (inserted points in bold):\n“We hypothesized that different types of memorization strategies would result in different levels of memorization between our five groups, as measured by the number of words recalled. To test this hypothesis we conducted a One-way Analysis of Variance. Our analysis revealed a significant effect for Groups, F(4, 45) = 9.08, p < .001, \\(\\eta^2\\) = .45; see Figure 1.”\nFor now, this is all you can say with only running a One-way ANOVA using this method, and that’s all I would want for the homework for this week.\n\n32.10.2 Reporting the ANOVA, lm() output\nLet’s revisit the lm() output. Recall that the lm() method gives us the added benefit of comparing each level of the GROUP_FACTOR to the control level. This is what’s known as treatment contrasts (see Winter, Chapter 7). In this case, our control level is Counting and every other level (group) is compared against Counting and captured in the resulting beta coefficients:\n\naov_model_lm <- lm(RECALL~GROUP_FACTOR, data = dataset)\nsummary(aov_model_lm)\n\n\nCall:\nlm(formula = RECALL ~ GROUP_FACTOR, data = dataset)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -7.00  -1.85  -0.45   2.00   9.60 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)               7.0000     0.9835   7.117 6.83e-09 ***\nGROUP_FACTORRhyming      -0.1000     1.3909  -0.072 0.943004    \nGROUP_FACTORAdjective     4.0000     1.3909   2.876 0.006138 ** \nGROUP_FACTORImagery       6.4000     1.3909   4.601 3.43e-05 ***\nGROUP_FACTORIntentional   5.0000     1.3909   3.595 0.000802 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.11 on 45 degrees of freedom\nMultiple R-squared:  0.4468,    Adjusted R-squared:  0.3976 \nF-statistic: 9.085 on 4 and 45 DF,  p-value: 1.815e-05\n\n\nFrom this summary output we would not only extract our F-statistic, df, p-value and R-squared (remember that \\(R^2\\) is \\(\\eta^2\\)) as we do for the previous write-up, but now we can talk about differences between the other levels and Counting as expressed as a t value (we are running t-tests). We then combine this info with info related to the means and standard errors (note that I got the means and ses by creating a summary tibble():\n\ndataset %>% \n  group_by(GROUP_FACTOR) %>%\n  summarise(mean = mean(RECALL) %>% round(2),\n            se = sd(RECALL)/sqrt(n()) # ses = sd / sqrt number of participants\n            ) %>% \n  # this bit of code does some rounding (optional)\n  mutate_if(is.numeric,\n            round,\n            digits = 2)\n\n# A tibble: 5 × 3\n  GROUP_FACTOR  mean    se\n  <fct>        <dbl> <dbl>\n1 Counting       7    0.58\n2 Rhyming        6.9  0.67\n3 Adjective     11    0.79\n4 Imagery       13.4  1.42\n5 Intentional   12    1.18\n\n\nPutting all of this info together, we might say:\n\n“We hypothesized that different types of memorization strategies would result in different levels of memorization between our five groups, as measured by the number of words recalled. To test this hypothesis we conducted a One-way Analysis of Variance. Our analysis revealed a significant effect for Groups, F(4, 45) = 9.08, p < .001, \\(\\eta^2\\) = .45. As can be seen in Figure 1, we found that the average number of words recalled for the Adjective (\\(M\\) = 11.0, SE = 0.79; \\(t\\)(45) = 2.88), Imagery (\\(M\\) = 13.4, SE = 1.42; \\(t\\)(45) = 4.60), and Intentional (\\(M\\) = 12.0, SE = 1.18; \\(t\\)(45) = 3.60) memorization groups were all greater than the Counting group (\\(M\\) = 7.0, SE = 0.58); ps < .01”.\n\n\nnote that I find it acceptable to lump the p-values together in this manner when doing multiple comparisons, as we are doing here.\n\nIn both cases BE SURE TO PRODUCE A CAMERA READY FIGURE and REFER TO IT IN THE TEXT!!!"
  },
  {
    "objectID": "week08/8_2-anova-regression.html#means-method-v.-regression-method",
    "href": "week08/8_2-anova-regression.html#means-method-v.-regression-method",
    "title": "\n33  ANOVA and Regression\n",
    "section": "\n33.1 Means method v. Regression method",
    "text": "33.1 Means method v. Regression method\nAdmittedly I’m anticipating some work that we will be doing in a few weeks, but I wanted to take some time to circle back around to a few points from our class discussion, the previous walkthrough and our lectures on regression. I’ve repeatedly commented on how ANOVA and Regression are indeed one in the same. In the previous walkthrough we performed ANOVA using both a dedicated function aov() as well as the same function we used a few weeks back for regression lm(). In doing so, we also noted how the \\(F\\) statistic calculated by aov() is the same that’s calculated for lm() (and those of you that can remember all the way back to the regression week that you were asked to report your \\(F\\) value along with your \\(R^2\\) in your regression results when talking about the fit of the overall model.\nBy now you may ask: “Well, if ANOVA is simply linear regression then why involve different functions?” To add insult to injury, would you be even more peeved if I told you that the \\(F\\) statistic for regression is calculated differently from the \\(F\\) statistic for ANOVA, even though they give you the exact same result!!! Ugh… Why, WHY?!?!?!, WHYYYYY?!?!?!?!?!\nSadly, tradition and convention are all I can tell you. While in both cases the \\(F\\) value or ratio is an expression of amount of variance that your model (lm or aov) explains given total variance that exists in the data. In the ANOVA case we are concerned with the means of discrete groups or IVs. Since lm deals with continuous predictors (or IVs) then it makes little sense to talk about means, other than the grand mean. In both instances, however, the derived \\(F\\) is an expression of the model’s fit.\nMy goal for this section is to demonstrate to you that the regression method we used to derive the the \\(F\\) ratios during our week on regression is identical to the means method Cohen introduced you to this week. First to recap the regression method, our equation from the correlation week’s lecture slides was:\n\\[F=\\frac{r^2 / df_{reg}}{(1-r^2)/df_{res}}\\]\n\nwhere \\(r^2\\) is the model’s coefficient of determination\n\n\\(df_{reg}\\): the model’s \\(df\\) (based on # of predictors), and\n\n\\(df_{res}\\): the model’s residual \\(df\\)\n\n\nmoreover,\n\\[r^2=\\frac{SS_Y-SS_{res}}{SS_Y}\\]\n\nwhere \\(SS_Y\\) is the sum of squared differences between the observed values \\(Y\\) and the grand mean \\(\\bar{Y}\\)\n\n\n\\(SS_{res}\\) is the sum of squared differences between the observed values \\(Y\\) and the model-predicted values \\(\\hat{Y}\\) (or distance to regression line).\n\nFor those of you interested in reading further on the mathematics, pay close attention to the Flora chapter assigned this week.\n\n33.1.1 Comparing two models\nTo conceptualize lets take another look at our data again. But first it might help to take another look at my earlier spiel on means and other models here and our related discussion on t-tests as linear models here.\nGo ahead, I’ll wait…\nOK, now that you’re back let’s have another look at our example data from the previous walkthrough. First let’s read in the clean dataset from walkthrough 1\n\ndataset <- read_rds('clean_dataset.rds')\n\nIn the plot below, the dotted line represents the grand_mean model, where we are using the mean of all data to predict individual scores. This corresponds to our assumption that all samples come from the same population and are randomly distributed (aka random error). It is from this means model that \\(SS_Y\\) is calculated.\nVisually the means model looks like this:\n\nggplot(data = dataset,mapping = aes(x = GROUP_NUM, y = RECALL, col=factor(GROUP_NUM))) +\n  geom_hline(yintercept=mean(dataset$RECALL)) +\n  geom_point(position = position_jitter(height = 0L, seed = 1L, width=0.2)) +\n  geom_linerange(aes(x=GROUP_NUM, ymax=RECALL, ymin=mean(dataset$RECALL)),\n                 position = position_jitter(height = 0L, seed = 1L, width=0.2)) + \n  theme_cowplot() +\n  theme(legend.position = \"none\")\n\nWarning: Use of `dataset$RECALL` is discouraged.\nℹ Use `RECALL` instead.\n\n\n\n\n\nThe solid line represents the mean of the entire dataset, or the grand mean. Each of our groups are color coded and the lines connecting the data points to the grand mean are the residuals. We can express the grand means model as:\n\ngrand_mean_model <- lm(RECALL~1, data = dataset) \n# Here, \"1\"\" indicates assume scores simply vary randomly about the grand mean\n\nFurther \\(SS_Y\\) is simply the sum of the squared residuals:\n\nSS_Y <- sum(grand_mean_model$residuals^2)\nSS_Y\n\n[1] 786.82\n\n\nIn contrast, the group_means model represents the model (line) of best fit for our ANOVA model which, in addition to assuming random error about the grand mean, also assumes that GROUP (encoding) is a predictor of variation that we see in the data (aka systematic error). Its from this model which \\(SS_{res}\\) is calculated. This model may be expressed:\n\n# add GROUP as a predictor to our grand means model (RECALL~1)\ngroup_means_model <- lm(RECALL~1 + GROUP_FACTOR, data = dataset) \n\n# also shorthand\n# group_means_model <- lm(RECALL~1 + GROUP_FACTOR, data = dataset)\n\nand visualized as:\n\n# add group predicteds for plotting\ndataset$group_predicteds <- group_means_model$fitted\n\nggplot(data = dataset,mapping = aes(x = GROUP_NUM, y = RECALL, col=factor(GROUP_NUM))) +\n  geom_point(position = position_jitter(height = 0L, seed = 1L, width=0.2)) +\n  stat_summary(geom = \"hpline\", width = 1, size = 1.5, fun=\"mean\") + \n  geom_linerange(aes(x=GROUP_NUM, ymax=RECALL, ymin=group_predicteds),\n                 position = position_jitter(height = 0L, seed = 1L, width=0.2)) + \n  theme_cowplot() +\n  theme(legend.position = \"none\")\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\nNote that the horizontal lines are the predicted values in this model… the group means.\nThe resulting sum of squares for this model \\(SS_{res}\\) is:\n\nSS_res <- sum(group_means_model$residuals^2)\nSS_res\n\n[1] 435.3\n\n\nThus \\(R^2\\) is simply a statement about the difference in the grand mean model and the experimental model (in this case our ANOVA model with GROUP predictor) residuals as a percentage of \\(SS_Y\\). Looking at the \\(R^2\\) of the group_means_model show us that it is .4468.\n\nbroom::glance(group_means_model)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.447         0.398  3.11      9.08 0.0000182     4  -125.  262.  274.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n\n\\(R^2\\) as calculated from our SS_res and SS_Y:\n\nr2 <- (SS_Y-SS_res)/SS_Y\nr2\n\n[1] 0.4467604\n\n\nGiven this value we can calculate the \\(F\\)-ratio using the equation above, noting that the model’s degrees of freedom based on the number of predictors = \\(df_{reg}\\) = 4 and the model’s residual degrees of freedom = \\(df_{res}\\) = 45 (more on this below)\n\nf_ratio <- (r2/4)/((1-r2)/45)\nf_ratio\n\n[1] 9.084769\n\n\nIn addition, with ANOVA we are dealing with discrete predictors rather than continuous. Visually, as you can see that the data in the plot are stacked into columns, with the mean of each group located at the center of the columns’ density. This also means that rather than a continuous range of predicted values \\(\\hat{Y}\\) we have a set number of \\(\\hat{Y}\\), one for each group mean that we are considering in the ANOVA. Therefore:\n\n\n\\(SS_Y\\) may be thought of in terms of the differences between every score compared to the grand mean.\n\n\\(SS_{res}\\) may be thought of as the differences between every score within the group and the group mean \\(\\hat{Y}\\)\n\n\nTo put it more succinctly, with ANOVA we are directly comparing our GROUP predictor model’s ability to account for variance in the data above what we might expect by chance. Unpacking this a bit, we have a grand_mean model and a group_mean model. As you will see in a few weeks we can use a Likelihood Ratio Test to directly compare to models to see if the second accounts for significantly more variance than the first. In R this is accomplished using…. dun, dun, dun anova():\n\n# Here, \"1\"\" indicates let scores simply vary randomly\ngrand_mean_model <- lm(RECALL~1, data = dataset) \n\n# add GROUP as a predictor to our original model\ngroup_means_model <- lm(RECALL~1 + GROUP_FACTOR, data = dataset) \n\n# run Likelihood Ratio Test to compare two models\nanova(grand_mean_model, group_means_model, test=\"LRT\")\n\nAnalysis of Variance Table\n\nModel 1: RECALL ~ 1\nModel 2: RECALL ~ 1 + GROUP_FACTOR\n  Res.Df    RSS Df Sum of Sq  Pr(>Chi)    \n1     49 786.82                           \n2     45 435.30  4    351.52 2.464e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe output above tells you the structure (formula) of each model and that Model 2 accounts for significantly more variance than Model 1 (as assessed on the Chi-sq. distribution). For what it’s worth, let’s run this same comparison of models using a different test, F:\n\n# run Likelihood Ratio Test to compare two models\nanova(grand_mean_model, group_means_model, test=\"F\")\n\nAnalysis of Variance Table\n\nModel 1: RECALL ~ 1\nModel 2: RECALL ~ 1 + GROUP_FACTOR\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1     49 786.82                                  \n2     45 435.30  4    351.52 9.0848 1.815e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThose numbers look familiar (df and F)? What does that suggest is really going on with ANOVA?\n\n33.1.2 Means method calculations\nNow lets turn to our F-ratio equation for the means method:\n\\[F=\\frac{MS_{bet}}{MS_{error}}\\]\nwhere\n\n\n\\(MS_{error}\\) = \\(SS_{res}/df_{res}\\)\n\nand \\(MS_{bet}\\) = (\\((SS_{Y}-SS_{res})/df_{reg}\\);\n\nnote that \\(df_{res}\\) may be calculated as \\(k(n-1)\\) where \\(k\\) is the number of IV and \\(n\\) is the number of scores in each group; \\(df_{reg}\\) is \\(k-1\\).\nso for our model, aov_model (our ANOVA) we can calculate the F-ratio using the means method as follows:\n\naov_model <- aov(RECALL~GROUP_FACTOR,dataset)\n\n# calculate TSS from grand mean residuals\nSS_Y <- ((dataset$RECALL-mean(dataset$RECALL))^2) %>% sum()\n\n# calculate RSS from model residuals (group means compared to grand mean)\nSS_res <- (aov_model$residuals^2) %>% sum()\n\nk <- 5 # predictors\nn <- 10 # number in each group\n\n# between df\ndf_reg <- k-1\n\n# within df\ndf_res <- k*(n-1)\n\n# MS treatments\nMS_treat <- (SS_Y-SS_res)/df_reg\n\n# MS error\nMS_error <- SS_res/df_res\n\n\nF.ratio <- (MS_treat/MS_error) %>% print()\n\n[1] 9.084769"
  },
  {
    "objectID": "week08/8_2-anova-regression.html#bringing-it-together-its-all-about-the-residuals-baby",
    "href": "week08/8_2-anova-regression.html#bringing-it-together-its-all-about-the-residuals-baby",
    "title": "\n33  ANOVA and Regression\n",
    "section": "\n33.2 Bringing it together: it’s all about the residuals, baby",
    "text": "33.2 Bringing it together: it’s all about the residuals, baby\nSo on one hand we have the F-ratio being calculated using \\(r^2\\) and on the other we have it being calculated using the mean square. Underlying both is a calculation of various \\(SS\\) and it is using this fact that we can show that the two methods are equivalent. Drawn out, we have on the left our \\(r^2\\) calculation and on the right the mean square calculation:\n\\[F=\\frac{r^2 / df_{reg}}{(1-r^2)/df_{res}} =\\frac{MS_{bet}}{MS_{error}}\\]\nrewriting our \\(r^2\\) in terms of \\(SS\\) (sums of squares) we get:\n\\[\\frac{\\frac{SS_Y-SS_{res}}{SS_Y} / df_{reg}}{(1-\\frac{SS_Y-SS_{res}}{SS_Y})/df_{res}} =\\frac{MS_{bet}}{MS_{error}}\\]\nand rewriting our \\(MS\\) side of the equation in terms of \\(SS\\): \\[\\frac{\\frac{SS_Y-SS_{res}}{SS_Y} / df_{reg}}{(1-\\frac{SS_Y-SS_{res}}{SS_Y})/df_{res}} =\\frac{(SS_{Y}-SS_{res})/df_{reg}}{SS_{res}/df_{res}}\\]\nmultiplying the left side of the equation by 1, or \\(\\frac{SS_Y}{SS_Y}\\):\n\\[\\frac{(SS_Y-SS_{res}) / df_{reg}}{(SS_Y-(SS_Y-SS_{res}))/df_{res}} =\\frac{(SS_{Y}-SS_{res})/df_{reg}}{SS_{res}/df_{res}}\\]\nwhich gives us:\n\\[\\frac{(SS_{Y}-SS_{res})/df_{reg}}{SS_{res}/df_{res}}=\\frac{(SS_{Y}-SS_{res})/df_{reg}}{SS_{res}/df_{res}}\\]\nSee… told you… SAME DAMN THING!\nWhat does this mean for you… well perhaps nothing. Honestly, if you are using your computer for stats a correct calculation is a correct calculation. However, conceptually it may be easier to get and keep yourself in general linear model mode, as it will make incorporating more complex modeling techniques (higher-order ANOVA, ANCOVA, mixed-models, multi-level models, growth curve models) a more intuitive step in the future."
  },
  {
    "objectID": "week08/8_2-anova-regression.html#critical-point-unpacking-those-residuals-baby",
    "href": "week08/8_2-anova-regression.html#critical-point-unpacking-those-residuals-baby",
    "title": "\n33  ANOVA and Regression\n",
    "section": "\n33.3 CRITICAL POINT: unpacking those residuals, baby",
    "text": "33.3 CRITICAL POINT: unpacking those residuals, baby\nIn class I asked you to note two very important ideas, one in the context of ANOVA and the other for regression. For ANOVA, we noted that:\n\nthe null hypothesis for ANOVA is that the mean scores of all sample groups are equal to one another\nmathematically, that if the null hypothesis is true, then each of the group means are not only equal to one another, but to the grand mean of the scores from all combined groups (mean of dumping everyone’s score together)\n\nTaking a look a the previous section, \\(SS_Y\\) (aka, Total Sum of Squares) are the residuals with respect to the grand mean of all groups. In other words, these are the amount of error / residuals that you would expect if the null hypothesis was TRUE!!!. Tying this more explicitly to some of the terminology that we used in lecture for ANOVA (the variance ratio method), these residuals represent what you would expect our within-groups variability to be if the null hypothesis is true (that is the null hypothesis assumes that all of this data comes from a single group… your hypothesis test is designed to refute that notion).\n\\(SS_{res}\\) represents the within group residuals related to a model that assumes group differences. That is this is what the within residuals look like when each data point is tied to a particular group mean, instead of the grand mean. When the null hypothesis is true, \\(SS_{res} = SS_Y\\). The further you group means are from one another, the further \\(SS_{res}\\) decreases from \\(SS_Y\\). In the variance-ratio terminology, the further the \\(MS_{error}\\) gets from the Total Sum of Squares (MS) as is the random variation in data points (error) moves from being accounted for by the grand mean to the individual means. In turn, this suggests that the differences in individual means is sucking-up some of that random variance (i.e. has some explanatory value). The amount of the variation that the group means accounts for can be taken be taking the \\(SS_Y\\) from the null hypothesis residuals and subtracting the \\(SS_{res}\\) from the group predictor residuals. What remains are the residuals accounted for by mean differences! These are the residuals for our effect (\\(MS_{bet}\\)). In the context of ANOVA these raw sums of squares are understood in terms of the degrees of freedom for our predictors and groups… the mean squares (MS) are the sums of squares divided be the appropriate degrees of freedom.\nIdeally we want the residuals tied to our effect (\\(MS_{bet}\\)) to be greater than the random variance left over \\(MS_{error}\\) which is why we evaluate the resulting F-ratio \\(\\frac{MS_{bet}}{MS_{error}}\\) in terms of being “how much greater than 1”. To put this into terms that we are familiar with from the our discussions of Effect Size, our between groups effect is our signal, and the random within groups variation is our noise.\nIf you are looking at this video, https://youtu.be/YrFehOKUJBw, and working with the accompanying excel file, the gist is the further the results of the Within SS box are from the Total SS box, to more likely you are to get a significant effect (the greater the Between SS box).\nFWIW, this logic also applies to regression, where:\n\nthe null hypothesis is that the predictive model accounts for no variance (\\(r^2\\) = 0).\nin a simple regression this would also mean that the \\(beta\\) coefficient representing your slope would also be zero, with an intercept at, wait for it… the grand mean!!\n\nFrom here everything plays out as above, beginning with the initial residuals, \\(SS_Y\\) or (TSS) being based upon the grand mean, and the \\(SS_{res}\\) being those residuals around the line of best fit.\nHope this helps understand what’s going on behind the scenes!\nSee you next week!"
  },
  {
    "objectID": "week09/9_1-posthocs.html",
    "href": "week09/9_1-posthocs.html",
    "title": "\n34  Multiple comparisons in One-way ANOVA, pt. 1: post hoc tests\n",
    "section": "",
    "text": "35 Using estimated marginal means, emmeans()\nIn previous years I would have stopped here and left you with the impression that you may need to call different packages depending on what post-hoc (or later, contrast) you intend to run. While this is perfectly acceptable, more and more I am using the estimated marginal means (package) to perform just about all of my post hoc analyses and contrasts. From the emmeans vignette (link):\n(The notion of a reference grid will become more apparent when we more into factorial ANOVA For now, let’s just focus on practical implementation).\nI typically use this framework as it allows for a higher level of flexibility in analyzing contrasts while still being unified. We’ll see just how far we can go in the upcoming weeks. For now, let’s use the emmeans package to perform our post-hoc analyses from above.\nWe start by throwing our model into the emmeans() function, specifying our test as a function of our comparison. We also need to tell R what adjustment (correction) we desire. To remind ourselves, our model is:\nFor a pairwise test with a bonferroni correction we need to specify what factor (or predictor or IV) we are running our pairwise comparisons on (spec =), and that we are adjusting by using the bonferonni (bonf). This takes the general form:\nemmeans also allows us to run a pairwise Tukey test:"
  },
  {
    "objectID": "week09/9_1-posthocs.html#tldr-steps-for-running-a-posthoc-test-of-means",
    "href": "week09/9_1-posthocs.html#tldr-steps-for-running-a-posthoc-test-of-means",
    "title": "\n34  Multiple comparisons in One-way ANOVA, pt. 1: post hoc tests\n",
    "section": "\n34.1 TLDR; Steps for running a posthoc test of means",
    "text": "34.1 TLDR; Steps for running a posthoc test of means\nThis walkthrough requires the following to be installed / loaded in R\n\nBe sure that the column that contains your IV is indeed being treated as a factor. If it is levels(IV) will list your levels, also the column containing the IV will contain <fct>.\nbuild an ANOVA model using lm(), aov(), or afex() (output to lm)\nrun your post-hoc comparisions using the p.adjustment of your choice (tukey, holm, bonferroni)\n\nExample (I recommend running this line-by-line)\n\n# Preliminaries\n## load in data\ndataset <- read_table2(\"https://www.uvm.edu/~statdhtx/methods8/DataFiles/Tab12-1.dat\")\n\nWarning: `read_table2()` was deprecated in readr 2.0.0.\nℹ Please use `read_table()` instead.\n\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  ID = col_character(),\n  Group = col_double(),\n  Time = col_double()\n)\n\n# Step 1\n## check data, identify columns\ndataset\n\n# A tibble: 40 × 3\n   ID    Group  Time\n   <chr> <dbl> <dbl>\n 1 01        1     3\n 2 02        1     5\n 3 03        1     1\n 4 04        1     8\n 5 05        1     1\n 6 06        1     1\n 7 07        1     4\n 8 08        1     9\n 9 09        2     2\n10 10        2    12\n# ℹ 30 more rows\n\n## Group is numerically coded. Fixing this and turning to a factor\ndataset$Group <- recode_factor(dataset$Group, \"1\"=\"MS\",\"2\"=\"MM\",\"3\"=\"SS\",\"4\"=\"SM\",\"5\"=\"McM\")\n\n# Step 2: Run the model\nmodel_aov <- lm(Time~Group, data = dataset)\n\n# Step 3: Test the model for significant F-value\nmodel_aov %>% sjstats::anova_stats()\n\nterm      | df |    sumsq |  meansq | statistic | p.value | etasq | partial.etasq | omegasq | partial.omegasq | epsilonsq | cohens.f | power\n--------------------------------------------------------------------------------------------------------------------------------------------\nGroup     |  4 | 3497.600 | 874.400 |    27.325 |  < .001 | 0.757 |         0.757 |   0.725 |           0.725 |     0.730 |    1.767 |     1\nResiduals | 35 | 1120.000 |  32.000 |           |         |       |               |         |                 |           |          |      \n\n# Step 4: post-hoc analysis in this example \"tukey\", but could also be \"bonf\" or \"holm\"\nemmeans(object = model_aov,specs = ~Group, adjust=\"tukey\") %>% pairs()\n\n contrast estimate   SE df t.ratio p.value\n MS - MM        -6 2.83 35  -2.121  0.2340\n MS - SS        -7 2.83 35  -2.475  0.1198\n MS - SM       -20 2.83 35  -7.071  <.0001\n MS - McM      -25 2.83 35  -8.839  <.0001\n MM - SS        -1 2.83 35  -0.354  0.9965\n MM - SM       -14 2.83 35  -4.950  0.0002\n MM - McM      -19 2.83 35  -6.718  <.0001\n SS - SM       -13 2.83 35  -4.596  0.0005\n SS - McM      -18 2.83 35  -6.364  <.0001\n SM - McM       -5 2.83 35  -1.768  0.4078\n\nP value adjustment: tukey method for comparing a family of 5 estimates"
  },
  {
    "objectID": "week09/9_1-posthocs.html#comparing-means-in-the-anova-model",
    "href": "week09/9_1-posthocs.html#comparing-means-in-the-anova-model",
    "title": "\n34  Multiple comparisons in One-way ANOVA, pt. 1: post hoc tests\n",
    "section": "\n34.2 Comparing means in the ANOVA model",
    "text": "34.2 Comparing means in the ANOVA model\nIn AoV, part 1, introduced the One-Way ANOVA. ANOVA is useful when we are comparing 3 or more group means such that the null hypothesis is:\n\\[\\mu_1=\\mu_2=\\mu_3...=\\mu_n\\].\nIn this case, if a single mean is revealed to be significantly different from the others, then the null is rejected. However, rejecting the null only tells us that at least one mean was different from the others; it does not tell us which one or how many. For example with just three means, it could be the case that:\n\n\\(\\mu_1≠\\mu_2=\\mu_3\\)\n\\(\\mu_1=\\mu_2≠\\mu_3\\)\n\\(\\mu_1=\\mu_3≠\\mu_2\\)\n\\(\\mu_1≠\\mu_2≠\\mu_3\\)\n\nSimply getting a significant F-value does not tell us this at all. In order to suss out any differences in our groups we are going to need to make direct comparisons between them.\nEnter multiple contrasts. Multiple contrasts are a way of testing the potential inequalities between group means like those above. As always, both Navarro and Poldrack do wonderful jobs of laying out the mathematics and logic of multiple comparisons. As with Part 1 I focus on practical implementation and spend some time focusing a bit on potential landmines and theoretical concerns as I see them.\nThis vignette assumes that you have the following packages installed and loaded in R:\n\n# use pacman to check, install, and load necessary packages\npacman::p_load(agricolae,\n               cowplot, \n               tidyverse, \n               emmeans,\n               multcomp,\n               psych,\n               sjstats)"
  },
  {
    "objectID": "week09/9_1-posthocs.html#the-data-siegels-1975-study-on-the-effects-of-morphine",
    "href": "week09/9_1-posthocs.html#the-data-siegels-1975-study-on-the-effects-of-morphine",
    "title": "\n34  Multiple comparisons in One-way ANOVA, pt. 1: post hoc tests\n",
    "section": "\n34.3 The data: Siegel’s 1975 study on the effects of morphine",
    "text": "34.3 The data: Siegel’s 1975 study on the effects of morphine\nTo start, lets download Siegel’s (1975) data set on Morphine Tolerance. This data set can be found on the web. Before diving into the data, check a description of the experiment in the Siegel_summary.pdf file in the walkthroughs folder. When you are done, come back a we’ll work on analyzing this data.\n\n# grab data from online location:\ndataset <- read_table2(\"https://www.uvm.edu/~statdhtx/methods8/DataFiles/Tab12-1.dat\")\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  ID = col_character(),\n  Group = col_double(),\n  Time = col_double()\n)\n\n# convert dataset$Group number codes to named factor levels:\ndataset$Group <- recode_factor(dataset$Group, \"1\"=\"MS\",\"2\"=\"MM\",\"3\"=\"SS\",\"4\"=\"SM\",\"5\"=\"McM\")\n\n# get descriptive stats for this data by Group\npsych::describeBy(dataset$Time,dataset$Group)\n\n\n Descriptive statistics by group \ngroup: MS\n   vars n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 8    4 3.16    3.5       4 3.71   1   9     8 0.43    -1.59 1.12\n------------------------------------------------------------ \ngroup: MM\n   vars n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 8   10 5.13   10.5      10 4.45   2  19    17 0.15    -0.99 1.81\n------------------------------------------------------------ \ngroup: SS\n   vars n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 8   11 6.72   10.5      11 8.15   3  21    18 0.23    -1.69 2.38\n------------------------------------------------------------ \ngroup: SM\n   vars n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 8   24 6.37     23      24 5.93  17  36    19 0.59    -1.07 2.25\n------------------------------------------------------------ \ngroup: McM\n   vars n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 8   29 6.16   28.5      29 5.93  20  40    20 0.28    -1.07 2.18\n\n\nAnd a quick peek at this data:\n\nggplot(data = dataset,aes(x=Group,y=Time)) +\n  stat_summary(fun = mean, geom = \"bar\") +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", aes(width=.25)) +\n  scale_y_continuous(expand = c(0,0)) + expand_limits(y=c(0,35)) + theme_cowplot()"
  },
  {
    "objectID": "week09/9_1-posthocs.html#running-the-one-way-anova",
    "href": "week09/9_1-posthocs.html#running-the-one-way-anova",
    "title": "\n34  Multiple comparisons in One-way ANOVA, pt. 1: post hoc tests\n",
    "section": "\n34.4 Running the One-way ANOVA",
    "text": "34.4 Running the One-way ANOVA\nNow that our data is properly coded we can run our omnibus ANOVA. My own personal preference is to run the ANOVA using lm(). This makes like a lot easier when dealing with contrasts, especially if you decide to employ the method that Field suggests in his guide. FWIW, I typically use another method as seen below, but I’ll talk a little bit about why I prefer it to Fields method. That said, recall from Part 1 that using the aov() function gives you the same result. Depending on which you choose, you can use the summary(morphine_mdl) or anova(morphine_mdl) to switch back and forth to get the info that you desire:\n\n# running the ANOVA using lm:\nmorphine_mdl <- lm(formula = Time~Group,data = dataset)\n# using the anova() function to display as ANOVA table\nanova(morphine_mdl)\n\nAnalysis of Variance Table\n\nResponse: Time\n          Df Sum Sq Mean Sq F value    Pr(>F)    \nGroup      4 3497.6   874.4  27.325 2.443e-10 ***\nResiduals 35 1120.0    32.0                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# full anova output (preferred)\nsjstats::anova_stats(morphine_mdl)\n\nterm      | df |    sumsq |  meansq | statistic | p.value | etasq | partial.etasq | omegasq | partial.omegasq | epsilonsq | cohens.f | power\n--------------------------------------------------------------------------------------------------------------------------------------------\nGroup     |  4 | 3497.600 | 874.400 |    27.325 |  < .001 | 0.757 |         0.757 |   0.725 |           0.725 |     0.730 |    1.767 |     1\nResiduals | 35 | 1120.000 |  32.000 |           |         |       |               |         |                 |           |          |      \n\n\nSo we see here that we have: \\(F(4,35)=27.33,p<.001,\\eta_p^2=.75\\)\nRemember again that the only thing that the omnibus ANOVA tells us is that there is an inequality in our means. In this respect, the omnibus begs more questions than it answers—which means are different from which. In order to get this answer we need to run direct comparisons between our means. There are two ways of going about this, we can either (1) plan beforehand what differences in means are especially relevant for us and focus on those, or (2) take a look at all potential differences without any specified predictions. In Case 1, we are performing planned contrasts; in Case 2, we use post hoc tests. More often than not, you will see researchers analyzing differences in means using post hoc tests—that is they run the ANOVA, find that it is significant, and run a battery of pairwise comparisons. It is sometimes the case that of that battery of comparisons, only a select few are actually theoretically relevant. However, if there is a theory-driven case to be made that you are predicting differences between a few select means in your data, then there is an argument to be made that you should run your planned contrasts independent of your ANOVA. That is, you are technically only permitted to run post-hoc tests if your ANOVA is significant (you can only go looking for differences in means if your ANOVA tells you that they exist), whereas planned contrasts can be run regardless of the outcome of the omnibus ANOVA (indeed, some argue that they obviate the need to run the omnibus ANOVA altogether).\nMy guess is that most of you have experience with post-hoc tests. They are more commonly performed tend to be touched upon in introductory stats courses. So we will spend a little time on these first before proceeding to a more in depth treatment of planned contrasts."
  },
  {
    "objectID": "week09/9_1-posthocs.html#post-hoc-tests",
    "href": "week09/9_1-posthocs.html#post-hoc-tests",
    "title": "\n34  Multiple comparisons in One-way ANOVA, pt. 1: post hoc tests\n",
    "section": "\n34.5 Post-hoc tests",
    "text": "34.5 Post-hoc tests\nWe use a post-hoc test when we want to test for differences in means that we have not explicitly predicted prior to conducting our experiment. As a result, whenever we perform a post-hoc test, we need to adjust our critical p-values to correct for inflation of Type 1 error. Recall from earlier discussions that the odds of committing a Type 1 error (falsely rejecting the null) is \\(1-(1-\\alpha)^c\\) where \\(\\alpha\\) is you critical p-value and \\(c\\) is the number of comparisons that are to be performed. Typically we keep this at .05, so when conducting a single test, the likelihood of committing a Type 1 error is: \\(1-(1-.05)^1=1-0.95^1=0.05\\)\nHowever as we increase the number of comparisons, assuming an \\(\\alpha\\) of 0.05:\n\n2 comparisons = \\(1-.95^2=0.0975\\)\n\n3 comparisons = \\(1-.95^3=0.1426\\)\n\n4 comparisons = \\(1-.95^4=0.1855\\)\n\n5 comparisons = \\(1-.95^5=0.2262\\)\n\n\nObviously, we need to control for this. The post-hoc methods that were introduced this week are all similar in that they involve comparing two means (a la t-test) but differ in how the error is controlled. For example a Bonferroni-Dunn correction (which is often used as a post-hoc correction, although initially intended for correcting planned comparisons) adjusts for this by partitioning the significance (by diving your original alpha by the number of comparisons). A popular variant of this method, the Holm test, is a multistage test. It proceeds by ordering the obtained t-values from smallest to largest. We then evaluate the largest t according to the Bonferroni-Dunn correction \\(\\alpha/c\\). Each subsequent comparison t value, \\(n\\) is evaluated against the correction \\(\\alpha/(c-n)\\). Please note I mention the these two methods with post-hoc analyses, although in true they are intended for planned comparisons. However, in instances in which the number of comparisons is relatively small, I’ve often seen them employed as post-hocs.\nSo how many comparisons is relatively small? I’d suggest best form is to use the above methods when you have 5 or fewer comparisons, meaning that your critical \\(\\alpha\\) is .01. That said, with a post hoc test, you really do not have a choice in the number of comparisons you can make, you need to test for all possible comparisons on the IV. Why? well if not you are simply cherry picking your data. For example it would be poor form to take a look at our data like so:\nPlot:\n\nggplot(data = dataset,aes(x=Group,y=Time)) +\n  stat_summary(fun.y = mean, geom = \"bar\") +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", aes(width=.25)) +\n  scale_y_continuous(expand = c(0,0)) + expand_limits(y=c(0,35)) + \n  theme_cowplot()\n\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\nℹ Please use the `fun` argument instead.\n\n\n\n\n\nand then decide that you only want to compare ‘McM’ to ‘MS’ because that’s where you see the greatest differences. Or that you simply want to take a look at “MM” and “SS” without considering the rest.\nSince you did not plan for or explicitly predict these differences from the outset, you are simply banking on what I like to say might be a “historical accident”, that you simply stumbled into these results. As such, it’s deemed as proper for to test all contingencies.\nIn the case above there are \\((5!)/(2!)(5-2)!\\) = 10 combinations. If we were to run a Bonferroni correction in this case or critical \\(p\\) would need to be \\(.05/10=.005\\) which is an extremely conservative value, and thus dramatically inflates the likelihood of Type II error. In cases like this, Tukey’s HSD is the traditionally preferred method, as it takes into account the characteristics of your data (in particular the standard error of the distribution) when calculating the critical \\(p\\) value. As such in cases where many post-hoc, pairwise comparisons are made, Tukey’s HSD is less conservative than a Bonferroni adjustment.\nOne final method that is becoming more en vogue is the Ryan, Einot, Gabriel, Welsch method (REGWQ). Whereas Tukey’s method holds the critical \\(p\\) constant for all comparisons (at the loss of power) the REGWQ allows for an adjustment for the number of comparisons. It is currently being promoted as the most desirable post-hoc method.\n\n34.5.1 Bonferonni-Dunn and Holm tests\nIn R there are several ways in which we can call post hoc corrections. For example we can call the Bonferonni and Holm adjustments using pairwise.t.test() function from the base package (already installed). The pairwise.t.test() method asks you to input:\n\n\nx = your DV\n\ng = your grouping factor\n\np.adjust.method = the name of your desired correction in string format\n\nFirst let’s run the pairwise.t.tests with no adjustment (akin to uncorrected \\(p\\) values):\n\npairwise.t.test(x = dataset$Time, g = dataset$Group,p.adjust.method = \"none\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  dataset$Time and dataset$Group \n\n    MS      MM      SS      SM   \nMM  0.041   -       -       -    \nSS  0.018   0.726   -       -    \nSM  3.1e-08 1.9e-05 5.4e-05 -    \nMcM 1.9e-10 8.9e-08 2.6e-07 0.086\n\nP value adjustment method: none \n\n\nYou see above that we get a cross-matrix containing the \\(p\\) values for each cross pair (row × column). Remember this is something we would never do in a post hoc (no corrections) but I wanted to first run this to illustrate a point. Now let’s run the the Bonferroni and Holm corrections:\n\n34.5.2 Bonferroni example (pairwise.t.test())\n\npairwise.t.test(x = dataset$Time, g = dataset$Group,p.adjust.method = \"bonferroni\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  dataset$Time and dataset$Group \n\n    MS      MM      SS      SM     \nMM  0.41051 -       -       -      \nSS  0.18319 1.00000 -       -      \nSM  3.1e-07 0.00019 0.00054 -      \nMcM 1.9e-09 8.9e-07 2.6e-06 0.85818\n\nP value adjustment method: bonferroni \n\n\nYou’ll note that the p-values displayed here are 10x the p-values from the uncorrected matrix. To demonstrate this:\n\nuncorrected <- pairwise.t.test(x = dataset$Time, g = dataset$Group,p.adjust.method = \"none\")\nbonf_corrected <- pairwise.t.test(x = dataset$Time, g = dataset$Group,p.adjust.method = \"bonferroni\")\n\nbonf_corrected$p.value/uncorrected$p.value\n\n    MS        MM SS SM\nMM  10        NA NA NA\nSS  10  1.377801 NA NA\nSM  10 10.000000 10 NA\nMcM 10 10.000000 10 10\n\n\n\nnote that the 1.378 value is the result of p being capped at \\(p=1\\) in the Bonferroni corrected comparison.\n\nRemember from a few paragraphs back that there are 10 possible combinations so the Bonferonni test would need to divide the critical alpha by 10. What this means is that anytime you perform a correction, R actually adjusts the \\(p\\) values for you; therefore you may interpret the output against your original (familywise) \\(\\alpha\\). So here, any values that are still less than .05 after the corrections are significant.\nMoving on…\n\n34.5.3 Holm example (pairwise.t.test())\n\npairwise.t.test(x = dataset$Time, g = dataset$Group,p.adjust.method = \"holm\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  dataset$Time and dataset$Group \n\n    MS      MM      SS      SM     \nMM  0.12315 -       -       -      \nSS  0.07327 0.72579 -       -      \nSM  2.8e-07 0.00011 0.00027 -      \nMcM 1.9e-09 7.1e-07 1.8e-06 0.17164\n\nP value adjustment method: holm"
  },
  {
    "objectID": "week09/9_1-posthocs.html#tukey-hsd-and-regwq-tests",
    "href": "week09/9_1-posthocs.html#tukey-hsd-and-regwq-tests",
    "title": "\n34  Multiple comparisons in One-way ANOVA, pt. 1: post hoc tests\n",
    "section": "\n34.6 Tukey HSD and REGWQ tests",
    "text": "34.6 Tukey HSD and REGWQ tests\nIn order to run Tukey’s HSD and REGWQ methods we call upon the agricolae package. In this case, we need to input our lm() model into the function, as well as identify our “treatment” (in this case our “Group” factor). For example:\n\n34.6.1 Tukey HSD example (agricolae)\n\nmorphine_mdl <- lm(Time~Group,data = dataset) # from above\nagricolae::HSD.test(morphine_mdl,trt = \"Group\",group = T,console = T) \n\n\nStudy: morphine_mdl ~ \"Group\"\n\nHSD Test for Time \n\nMean Square Error:  32 \n\nGroup,  means\n\n    Time      std r Min Max\nMcM   29 6.164414 8  20  40\nMM    10 5.126960 8   2  19\nMS     4 3.162278 8   1   9\nSM    24 6.369571 8  17  36\nSS    11 6.718843 8   3  21\n\nAlpha: 0.05 ; DF Error: 35 \nCritical Value of Studentized Range: 4.065949 \n\nMinimun Significant Difference: 8.131899 \n\nTreatments with the same letter are not significantly different.\n\n    Time groups\nMcM   29      a\nSM    24      a\nSS    11      b\nMM    10      b\nMS     4      b\n\n\nNote that the group and console arguments pertain to the output. You typically will want to keep console set to TRUE as that simply prints the output of your test. The group argument controls how the output is presented. Above we set it to TRUE. This results in an output that groups the treatment means into subsets where treatments with the same letter are not significantly different from one another, known as compact letter displays. For example, as are not significantly different from each other, bs are not significantly different from each other, but as are different from bs. Conversely if you wanted to see each comparison you can set this to FALSE:\n\nagricolae::HSD.test(morphine_mdl,trt = \"Group\",group = FALSE,console = TRUE) \n\n\nStudy: morphine_mdl ~ \"Group\"\n\nHSD Test for Time \n\nMean Square Error:  32 \n\nGroup,  means\n\n    Time      std r Min Max\nMcM   29 6.164414 8  20  40\nMM    10 5.126960 8   2  19\nMS     4 3.162278 8   1   9\nSM    24 6.369571 8  17  36\nSS    11 6.718843 8   3  21\n\nAlpha: 0.05 ; DF Error: 35 \nCritical Value of Studentized Range: 4.065949 \n\nComparison between treatments means\n\n         difference pvalue signif.        LCL        UCL\nMcM - MM         19 0.0000     ***  10.868101  27.131899\nMcM - MS         25 0.0000     ***  16.868101  33.131899\nMcM - SM          5 0.4078          -3.131899  13.131899\nMcM - SS         18 0.0000     ***   9.868101  26.131899\nMM - MS           6 0.2340          -2.131899  14.131899\nMM - SM         -14 0.0002     *** -22.131899  -5.868101\nMM - SS          -1 0.9965          -9.131899   7.131899\nMS - SM         -20 0.0000     *** -28.131899 -11.868101\nMS - SS          -7 0.1198         -15.131899   1.131899\nSM - SS          13 0.0005     ***   4.868101  21.131899\n\n\nFinally, if you do decide to group (group=TRUE), you can take the outcome of this function and use it to generate a nice group plot. This is useful for quick visual inspection.\n\nagricolae::HSD.test(morphine_mdl,trt = \"Group\",group = T,console = T) %>% plot()\n\n\nStudy: morphine_mdl ~ \"Group\"\n\nHSD Test for Time \n\nMean Square Error:  32 \n\nGroup,  means\n\n    Time      std r Min Max\nMcM   29 6.164414 8  20  40\nMM    10 5.126960 8   2  19\nMS     4 3.162278 8   1   9\nSM    24 6.369571 8  17  36\nSS    11 6.718843 8   3  21\n\nAlpha: 0.05 ; DF Error: 35 \nCritical Value of Studentized Range: 4.065949 \n\nMinimun Significant Difference: 8.131899 \n\nTreatments with the same letter are not significantly different.\n\n    Time groups\nMcM   29      a\nSM    24      a\nSS    11      b\nMM    10      b\nMS     4      b\n\n\n\n\n\n\n34.6.2 REGWQ example (agricolae)\nThe same applies to REGW, using the REGW.test() function (with group=F, I’m showing all of the comparisons):\n\nagricolae::REGW.test(morphine_mdl,trt = \"Group\",group = F,console = T) \n\n\nStudy: morphine_mdl ~ \"Group\"\n\nRyan, Einot and Gabriel and Welsch multiple range test\nfor Time \n\nMean Square Error:  32 \n\nGroup,  means\n\n    Time      std r Min Max\nMcM   29 6.164414 8  20  40\nMM    10 5.126960 8   2  19\nMS     4 3.162278 8   1   9\nSM    24 6.369571 8  17  36\nSS    11 6.718843 8   3  21\n\nComparison between treatments means\n\n         difference pvalue signif.         LCL        UCL\nMcM - MM         19 0.0000     ***  12.1234674  25.876533\nMcM - MS         25 0.0000     ***  17.4611210  32.538879\nMcM - SM          5 0.3056          -2.6279930  12.627993\nMcM - SS         18 0.0000     ***   9.8681013  26.131899\nMM - MS           6 0.0995       .  -0.8765326  12.876533\nMM - SM         -14 0.0001     *** -21.5388790  -6.461121\nMM - SS          -1 0.9846          -8.6279930   6.627993\nMS - SM         -20 0.0000     *** -26.8765326 -13.123467\nMS - SS          -7 0.0771       . -14.5388790   0.538879\nSM - SS          13 0.0001     ***   6.1234674  19.876533\n\n\nCompact letter displays are nice (SPSS generates them, too), but as seems to always be the case, there is some controversy as to whether we should use them. Taken from this vignette:\n\nCLD displays promote visually the idea that two means that are “not significantly different” are to be judged as being equal; and that is a very wrong interpretation. In addition, they draw an artificial “bright line” between P values on either side of alpha, even ones that are very close."
  },
  {
    "objectID": "week09/9_1-posthocs.html#alternative-to-cld",
    "href": "week09/9_1-posthocs.html#alternative-to-cld",
    "title": "\n34  Multiple comparisons in One-way ANOVA, pt. 1: post hoc tests\n",
    "section": "\n35.1 Alternative to CLD()",
    "text": "35.1 Alternative to CLD()\nOne thing to note, the curator of the emmeans() package is not a fan of cld (Compact Letter Displays) and instead has created pwpp as a visualization tool.\n\nemmeans(morphine_mdl, specs = ~Group) %>% pwpp()\n\n\n\n\nYou can visit this vignette to get a feel for what is at issue here."
  },
  {
    "objectID": "week09/9_1-posthocs.html#effect-sizes",
    "href": "week09/9_1-posthocs.html#effect-sizes",
    "title": "\n34  Multiple comparisons in One-way ANOVA, pt. 1: post hoc tests\n",
    "section": "\n35.2 Effect sizes",
    "text": "35.2 Effect sizes\nTypically when reporting the effect size of the difference between two means we use Cohen’s \\(d\\). However, calculating Cohen’s $d$ in a posthoc contrast is slightly more involved than the method used for a regular t-test. This is because with a regular t-test you only have 2 means from 2 samples that you have collected. In the case of pairwise contrasts in ANOVA, while you are only comparing two means, those means are nested within a larger group (e.g., when comparing MS and MM, we still need to account for the fact that we also collected samples from SS, SM, and McM). That is, you need to understand the difference between the two means in the context of the entire model.\nSimply put, in our calculations we need to account for the influence of all of our collected groups. This is done by placing the contrasted difference in the context of the Root Mean Square Error, or the square root of the Mean Square Error of the residuals in our ANOVA model. Recall that typically Cohen’s \\(d\\) is the difference between the two means divided by their pooled standard deviation. Here, \\(d\\) is the difference between the two means divided by sigma, or the estimated standard deviation of the errors of the linear model.\nTo do this we’re going to need two things from the original model. Let’s take a look at the model summary:\n\nmorphine_mdl %>% summary()\n\n\nCall:\nlm(formula = Time ~ Group, data = dataset)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -9.00  -3.25   0.00   3.00  12.00 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    4.000      2.000   2.000   0.0533 .  \nGroupMM        6.000      2.828   2.121   0.0411 *  \nGroupSS        7.000      2.828   2.475   0.0183 *  \nGroupSM       20.000      2.828   7.071 3.09e-08 ***\nGroupMcM      25.000      2.828   8.839 1.93e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.657 on 35 degrees of freedom\nMultiple R-squared:  0.7574,    Adjusted R-squared:  0.7297 \nF-statistic: 27.32 on 4 and 35 DF,  p-value: 2.443e-10\n\n\nThe values line that is important to us is the Residual standard error ___ on __ degrees of freedom—in this case the values 5.657 and 35 respectively.\nAssuming you have run your comparisons using emmeans() you can calculate your effect sizes for each comparison using emmeans::eff_size().\neff_size() takes three arguments:\n\nthe emmeans object\nthe estimated standard deviation of the errors of the linear model, sigma\n\nthe residual degrees of freedom from the model, df.residual\n\n\nThe functions sigma() and df.residual() allow us to get this information directly from the model. A typical call would look something like this\n\n# save your contrasts to an object\nmdl_contrasts <- emmeans(morphine_mdl, specs = ~Group)\n\n# use the saved object in the following function\neff_size(mdl_contrasts,sigma = sigma(morphine_mdl), df.residual(morphine_mdl))\n\n contrast effect.size    SE df lower.CL upper.CL\n MS - MM       -1.061 0.516 35    -2.11  -0.0135\n MS - SS       -1.237 0.521 35    -2.30  -0.1789\n MS - SM       -3.536 0.655 35    -4.86  -2.2065\n MS - McM      -4.419 0.727 35    -5.90  -2.9428\n MM - SS       -0.177 0.500 35    -1.19   0.8392\n MM - SM       -2.475 0.581 35    -3.65  -1.2955\n MM - McM      -3.359 0.641 35    -4.66  -2.0570\n SS - SM       -2.298 0.570 35    -3.46  -1.1400\n SS - McM      -3.182 0.628 35    -4.46  -1.9067\n SM - McM      -0.884 0.511 35    -1.92   0.1536\n\nsigma used for effect sizes: 5.657 \nConfidence level used: 0.95 \n\n\nThat said, there is some debate as to whether this is the most appropriate way to calculate posthoc effect sizes, or whether posthoc effect sizes are in general a proper thing to calculate. I’ve personally never had a reviewer ask for one, BUT if I had to provide on I would use this method.\nFWIW, this won’t be the last time that we need to call back to the original (omnibus) ANOVA when conducting posthoc tests. Things get a little messier next week!"
  },
  {
    "objectID": "week09/9_1-posthocs.html#reporting-and-anova-with-post-hoc-analyses.",
    "href": "week09/9_1-posthocs.html#reporting-and-anova-with-post-hoc-analyses.",
    "title": "\n34  Multiple comparisons in One-way ANOVA, pt. 1: post hoc tests\n",
    "section": "\n35.3 Reporting and ANOVA with post hoc analyses.",
    "text": "35.3 Reporting and ANOVA with post hoc analyses.\nIn your report, you need to include information for the main ANOVA as well as information related to\n\nthe omnibus ANOVA\na statement on what multiple comparisons were run and how corrected\nnote which comparisons were significant.\n\nIf we work off of our last example, you’ll note that there are quite a few comparisons that we could discuss (10 in fact). In cases like these you could either put this information into a formatted table, or simply highlight a few that are especially relevant. For example, looking at the emmeans() outcome as well as the CLD plot from agricolae::HSD.test we see that McM and SM (a’s) in the CLD plot are both significantly greater than the remain conditions (b’s). Based on this I would write something like:\n… Our ANOVA revealed a significant effect for Morphine treatment group, F(4, 35) = 27.325, p < .001. Tukey pairwise comparisons revealed that the mean tolerance times for the both the SM (\\(M±SD\\): 24.00 ± 6.37) and McM (29.00 ± 6.16) groups were greater than the remaining three groups (\\(ps\\) < .05), but not different from one another. The mean times for the remaining three conditions were not significantly different from one another, \\(ps > .05\\) (see Figure 1)\n\nassuming Figure 1 is a camera ready plot you’ve created in ggplot(). Any of the barplots in this walkthrough will suffice."
  },
  {
    "objectID": "week09/9_2-planned_contrasts.html#tldr-steps-for-running-an-a-priori-contrasts",
    "href": "week09/9_2-planned_contrasts.html#tldr-steps-for-running-an-a-priori-contrasts",
    "title": "\n35  Multiple comparisons in One-way ANOVA, pt. 2: planned contrasts\n",
    "section": "\n35.1 TLDR; Steps for running an a-priori contrasts:",
    "text": "35.1 TLDR; Steps for running an a-priori contrasts:\nThis walkthrough requires the following to be installed / loaded in R\n\nBe sure that the column that contains your IV is indeed being treated as a factor. If it is levels(IV) will list your levels, also the column containing the IV will contain <fct>. If not mutate() as column with your IV as a factor, ex: newCol = factor(oldCol)\n\ncheck the order of your IV using levels(). If you need to reorder, use fct_relevel()\n\ncreate your contrast matrix according to the levels obtained in Step 1\nbuild an ANOVA model using lm(), aov(), or afex() (output to lm)\nrun your contrasts using emmeans(), inputting\n\n\nyour ANOVA model from Step 3\nyour contrasts from Step 2\nyour familywise correction (if any)\n\nExample (run line-by-line):\n\n# Preliminaries\n## load in data\ndataset <- read_table2(\"https://www.uvm.edu/~statdhtx/methods8/DataFiles/Tab12-1.dat\")\n\nWarning: `read_table2()` was deprecated in readr 2.0.0.\nℹ Please use `read_table()` instead.\n\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  ID = col_character(),\n  Group = col_double(),\n  Time = col_double()\n)\n\n## check data, identify columns\ndataset\n\n# A tibble: 40 × 3\n   ID    Group  Time\n   <chr> <dbl> <dbl>\n 1 01        1     3\n 2 02        1     5\n 3 03        1     1\n 4 04        1     8\n 5 05        1     1\n 6 06        1     1\n 7 07        1     4\n 8 08        1     9\n 9 09        2     2\n10 10        2    12\n# ℹ 30 more rows\n\n## Group is numerically coded. Fixing this and turning to a factor\ndataset$Group <- recode_factor(dataset$Group, \"1\"=\"MS\",\"2\"=\"MM\",\"3\"=\"SS\",\"4\"=\"SM\",\"5\"=\"McM\")\n\n# Step 1: Check levels\nlevels(dataset$Group)\n\n[1] \"MS\"  \"MM\"  \"SS\"  \"SM\"  \"McM\"\n\n# Step 2: build contrasts\n## MS v. MM\ncontrast1 <- c(1,-1,0,0,0) \n\n## MS + MM v. SS + SM + McM\ncontrast2 <- c(1/2,1/2,-1/3,-1/3,-1/3)\n\nmy_contrasts <- list(\"MS v. MM\" = contrast1,\n                      \"MS + MM v. SS + SM + McM\" = contrast2\n                     )\n\n# Step 3\nmorphine_mdl <- lm(Time~Group, data = dataset) \n\n# Step 4\nemmeans(object = morphine_mdl,specs = ~Group, adjust=\"bonf\") %>% \n  contrast(method = my_contrasts)\n\n contrast                 estimate   SE df t.ratio p.value\n MS v. MM                     -6.0 2.83 35  -2.121  0.0411\n MS + MM v. SS + SM + McM    -14.3 1.83 35  -7.851  <.0001"
  },
  {
    "objectID": "week09/9_2-planned_contrasts.html#a-few-considerations-regarding-planned-contrasts",
    "href": "week09/9_2-planned_contrasts.html#a-few-considerations-regarding-planned-contrasts",
    "title": "\n35  Multiple comparisons in One-way ANOVA, pt. 2: planned contrasts\n",
    "section": "\n35.2 A few considerations regarding planned contrasts",
    "text": "35.2 A few considerations regarding planned contrasts\nIf you have reasons to predict differences between particular sets of group means that are theory-driven, then you may perform a priori or planned contrasts. Logically, planned contrasts are similar to post-hoc tests in that we are comparing against two means, but there are some differences that make planned contrasts more powerful.\n\nSince your predictions are made prior to collecting data you, technically do not need to get a significant result on the omnibus ANOVA to run your contrasts. However, when doing post-hoc tests, if the omnibus ANOVA fails to reject the null, convention is that you are not permitted to run follow-up post hoc tests.\nSince you are making predictions prior to seeing the outcome of your observed data, then you are safe to make a limited number of comparisons without the charge of cherry-picking. For example, if you have predicted-ahead that there would be differences between groups “MS” and “McM”, or groups “MM” and “MS” then you are free to run those comparisons and those comparisons-only. This is especially useful, since by limiting the number of comparisons you can keep the required corrections relatively minimal. This allows you to effectively reduce the problem of Type I error inflation while limiting the possibility of Type II error. For example, recall that using a Bonferonni correction on this data in the post-hoc case mandates that I use an adjusted \\(\\alpha\\) of .005 (.05/10 comparisons), even if I was only really interested in these two comparisons. Here, I am allowed to only perform these two, so my adjusted \\(p\\) is .025.\nDepending on the number of comparisons, you may be justified in not performing any \\(p\\) correction at all. For example some recommend no need for correction if the number of contrasts is low or when the comparisons are complementary (e.g. orthogonal). See here, here, and here for discussion of this issue.\nWe can easily perform a variety of comparisons using planned contrasts. For example, say we are interested in whether MS is different from the mean of the remaining groups (McM+MM+SM+SS), or that MM+MS is different from McM+SM+SS. We can test this using planned contrasts."
  },
  {
    "objectID": "week09/9_2-planned_contrasts.html#the-logic-of-planned-contrasts",
    "href": "week09/9_2-planned_contrasts.html#the-logic-of-planned-contrasts",
    "title": "\n35  Multiple comparisons in One-way ANOVA, pt. 2: planned contrasts\n",
    "section": "\n35.3 The logic of planned contrasts",
    "text": "35.3 The logic of planned contrasts"
  },
  {
    "objectID": "week09/9_2-planned_contrasts.html#performing-planned-contrasts-in-r",
    "href": "week09/9_2-planned_contrasts.html#performing-planned-contrasts-in-r",
    "title": "\n35  Multiple comparisons in One-way ANOVA, pt. 2: planned contrasts\n",
    "section": "\n35.4 Performing planned contrasts in R",
    "text": "35.4 Performing planned contrasts in R\nAs outlined in both Winter and Field, planned contrasts begin by creating contrast weights. The idea with contrast weights is that the groups that are being compared should sum to equal -1 and +1 respectively, resulting in a null test against 0 (i.e. the two weighted means are equal). Any groups that are not being included in the comparison should be assigned weights of 0. You’ve already seen contrasts before, when looking at the lm() ANOVA output. We remarked that this output allows you to make a comparison of each group against the control group.\nFirst let’s load in the data and recode the factors:\n\n## load in data\ndataset <- read_table2(\"https://www.uvm.edu/~statdhtx/methods8/DataFiles/Tab12-1.dat\")\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  ID = col_character(),\n  Group = col_double(),\n  Time = col_double()\n)\n\n## Group is numerically coded. Fixing this and turning to a factor\ndataset$Group <- recode_factor(dataset$Group, \"1\"=\"MS\",\"2\"=\"MM\",\"3\"=\"SS\",\"4\"=\"SM\",\"5\"=\"McM\")\n\ndataset\n\n# A tibble: 40 × 3\n   ID    Group  Time\n   <chr> <fct> <dbl>\n 1 01    MS        3\n 2 02    MS        5\n 3 03    MS        1\n 4 04    MS        8\n 5 05    MS        1\n 6 06    MS        1\n 7 07    MS        4\n 8 08    MS        9\n 9 09    MM        2\n10 10    MM       12\n# ℹ 30 more rows\n\n\nLooking at Siegel’s data we see that MS defaults as the control:\n\n# Step 1: Check levels\nlevels(dataset$Group)\n\n[1] \"MS\"  \"MM\"  \"SS\"  \"SM\"  \"McM\"\n\n\nWe can check which contrasts any model we run will default to, by invoking contrasts():\n\ncontrasts(dataset$Group)\n\n    MM SS SM McM\nMS   0  0  0   0\nMM   1  0  0   0\nSS   0  1  0   0\nSM   0  0  1   0\nMcM  0  0  0   1\n\n\nThese sorts of contrasts are known as treatment contrasts… measuring the effect of each treatment against a control. While there are several types of contrasts built into R we can create custom contrasts as well.\nFor example assume we are comparing “MS” to “MM” and were not concerned with the remaining groups. Then our contrast weights would be:\n\nMS = +1\nMM = -1\nMcM = SM = SS = 0\n\nHow about we want to compare McM + MM against the remaining three groups?\n\nMS + MM = 1\nMcM + SM + SS = -1\n\nFrom there we distribute the weight equally between the number of groups on each side of the contrast, so\n\nMS = +1/2; MM = +1/2\nMcM = -1/3; SM = -1/3; SS = -1/3\n\nin R we can construct each of these contrasts like so:\n\n# checking the order of groups:\nlevels(dataset$Group)\n\n[1] \"MS\"  \"MM\"  \"SS\"  \"SM\"  \"McM\"\n\n# building contrasts\n\n# MS v. MM\ncontrast1 <- c(1,-1,0,0,0) \n\n# MS + MM v. SS + SM + McM\ncontrast2 <- c(1/2,1/2,-1/3,-1/3,-1/3)\n\nWhen performing planned contrasts in R I recommend using the emmeans package rather than the base package examples in the Field text. For those that are interested as to why, in my experience the base method has a hard time dealing with non-orthogonal contrasts.\nTo perform the contrast requires 3 steps:\n\nrun your omnibus ANOVA lm() model and save it as an object\ninput your lm() model into the emmeans() function, specifying the name of your IV and save it to an object\nsubmit your emmeans object to the contrasts() function indicating what your contrasts are.\n\n\nmorphine_mdl <- lm(formula = Time~Group,data = dataset)\nemm_mdl <- emmeans(morphine_mdl,specs = ~Group)\ncontrast(emm_mdl, method = list(contrast1))\n\n contrast          estimate   SE df t.ratio p.value\n c(1, -1, 0, 0, 0)       -6 2.83 35  -2.121  0.0411\n\n\nThe above result tells me:\n\n\ncontrast: the defined contrast\n\nestimate: the difference in means between my contrasted groups\n\nSE: a measure of variability\n\nt.ratio: obtained from the t-test between groups\n\np.value: the resulting p-value\n\nBased upon these results I conclude that “MS” and “MM” are significantly different from one another. I make no claims about the other three conditions.\nRecall that contrast2 was testing [MS + MM] v. [SS + SM + McM]. If this was my only contrast I could run it independently as above. However, we can also test multiple contrasts simultaneously with a single test. We just add additional contrasts to the list in the method. So, for both contrast1 and contrast2:\n\ncontrast(emm_mdl, \n         method = list(contrast1,contrast2),\n         )\n\n contrast                                                              \n c(1, -1, 0, 0, 0)                                                     \n c(0.5, 0.5, -0.333333333333333, -0.333333333333333, -0.333333333333333\n estimate   SE df t.ratio p.value\n     -6.0 2.83 35  -2.121  0.0411\n    -14.3 1.83 35  -7.851  <.0001\n\n\n\n35.4.1 Making the contrasts output easier to read\nOne thing you may have noticed above is that your contrasts aren’t labeled very transparently. Looking at the output you would have to remember what was being contrasted in contrast1 and contrast2. You can save yourself a little headache if you label your contrasts while constructing the matrix. Let’s say I wanted to run these four contrasts based upon my levels:\n\nlevels(dataset$Group)\n\n[1] \"MS\"  \"MM\"  \"SS\"  \"SM\"  \"McM\"\n\n\nNow setting up my contrasts to match:\n\ncontrast1 <- c(0,-1,0,0,1)\ncontrast2 <- c(-1,0,1,0,0)\ncontrast3 <- c(0,1,-1,0,0)\ncontrast4 <- c(-1/3,-1/3,-1/3,1/2,1/2)\n\nFrom here you can modify the previous code to assign names to the rows in your contrast matrix. For the sake of ease, I like to create an object, my_contrasts that includes the list of all contrasts. FWIW, on of the annoying things about the contrast function is that it requires the method argument of contrasts to be in a list(), even if its just one contrast.\n\nmy_contrasts <- list(\"MM v. McM\" = contrast1,\n                     \"MS v. SS\" = contrast2,\n                     \"MM v. SS\" = contrast3,\n                     \"MS+MM+SS v. SM+McM\" = contrast4\n                     )\n\nmy_contrasts\n\n$`MM v. McM`\n[1]  0 -1  0  0  1\n\n$`MS v. SS`\n[1] -1  0  1  0  0\n\n$`MM v. SS`\n[1]  0  1 -1  0  0\n\n$`MS+MM+SS v. SM+McM`\n[1] -0.3333333 -0.3333333 -0.3333333  0.5000000  0.5000000\n\n\nNow if I run my contrasts it includes my descriptions of what the contrasts are:\n\ncontrast(emm_mdl, method = my_contrasts)\n\n contrast           estimate   SE df t.ratio p.value\n MM v. McM              19.0 2.83 35   6.718  <.0001\n MS v. SS                7.0 2.83 35   2.475  0.0183\n MM v. SS               -1.0 2.83 35  -0.354  0.7258\n MS+MM+SS v. SM+McM     18.2 1.83 35   9.950  <.0001\n\n\nNote that this defaults to the holm correction. Can we think of anyway to run a “bonferroni” adjustment to the same comparisons?\n\ncontrast(emm_mdl, \n         method = my_contrasts,\n         adjust = \"bonferroni\"\n)\n\n contrast           estimate   SE df t.ratio p.value\n MM v. McM              19.0 2.83 35   6.718  <.0001\n MS v. SS                7.0 2.83 35   2.475  0.0733\n MM v. SS               -1.0 2.83 35  -0.354  1.0000\n MS+MM+SS v. SM+McM     18.2 1.83 35   9.950  <.0001\n\nP value adjustment: bonferroni method for 4 tests \n\n\n\n35.4.2 Effect size for planned contrasts\nThis is some text that’s recycled from the post-hoc test walkthrough. I’m going to shout again one more time with feeling. Typically when reporting the effect size off the difference between two means we use Cohen’s \\(d\\). However, calculating Cohen’s $d$ in a planned contrast is slightly more involved than the method used for a regular t-test. This is because with a regular t-test you only have 2 means from 2 samples that you have collected. In the case of Planned Contrasts in ANOVA, while you are only comparing two means, those means are nested within a larger group (e.g., comparing MS and MM, we still need to account for the fact that we also collected samples from SS, SM, and McM) or may be derived from multiple samples (e.g., contrasting the mean of MS + MM against the mean of SS + SM + McM). Simply put, in our calculations we need to account for the influence of all of our collected groups. This is done by placing the contrasted difference in the context of the Root Mean Square Error, or the square root of the Mean Square Error of the residuals in our ANOVA model.\nSimply put, in our calculations we need to account for the influence of all of our collected groups. This is done by placing the contrasted difference in the context of the Root Mean Square Error, or the square root of the Mean Square Error of the residuals in our ANOVA model. Recall that typically Cohen’s \\(d\\) is the difference between the two means divided by their pooled standard deviation. Here, \\(d\\) is the difference between the two means divided by sigma, or the estimated standard deviation of the errors of the linear model.\nTo do this we’re going to need two things from the original model. Let’s take a look at the model summary:\n\nmorphine_mdl %>% summary()\n\n\nCall:\nlm(formula = Time ~ Group, data = dataset)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -9.00  -3.25   0.00   3.00  12.00 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    4.000      2.000   2.000   0.0533 .  \nGroupMM        6.000      2.828   2.121   0.0411 *  \nGroupSS        7.000      2.828   2.475   0.0183 *  \nGroupSM       20.000      2.828   7.071 3.09e-08 ***\nGroupMcM      25.000      2.828   8.839 1.93e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.657 on 35 degrees of freedom\nMultiple R-squared:  0.7574,    Adjusted R-squared:  0.7297 \nF-statistic: 27.32 on 4 and 35 DF,  p-value: 2.443e-10\n\n\nThe values line that is important to us is the Residual standard error ___ on __ degrees of freedom—in this case the values 5.657 and 35 respectively.\nAssuming you have run your comparisons using emmeans() you can calculate your effect sizes for each comparison using emmeans::eff_size().\neff_size() takes three arguments:\n\nthe emmeans object\nthe estimated standard deviation of the errors of the linear model, sigma\n\nthe residual degrees of freedom from the model, df.residual\n\na vector containing our contrasts— e.g., c(-1/2,-1/2,1/3,1/3,1/3)\n\n\nThe functions sigma() and df.residual() allow us to get the relevant information directly from the model. A typical call would look something like this\n\n# create and emmeans object specifying your factor\nemm_mdl <- emmeans(emm_mdl, specs = ~Group)\n\n# specify your contrasts, this goes in the method call\nmy_contrasts <- list(\"MM v. McM\" = contrast1,\n                     \"MS v. SS\" = contrast2,\n                     \"MM v. SS\" = contrast3,\n                     \"MS+MM+SS v. SM+McM\" = contrast4\n                     )\n\n# run your contrasts\nplanned_contrasts <- contrast(emm_mdl, # emm_mdl from above\n                              method = my_contrasts,\n                              adjust = \"bonferroni\"\n                              )\n\n# use the saved object in the following function\neff_size(emm_mdl,\n         sigma = sigma(morphine_mdl), \n         df.residual(morphine_mdl),\n         method=my_contrasts\n         )\n\n contrast           effect.size    SE df lower.CL upper.CL\n MM v. McM                3.359 0.641 35    2.057    4.660\n MS v. SS                 1.237 0.521 35    0.179    2.296\n MM v. SS                -0.177 0.500 35   -1.193    0.839\n MS+MM+SS v. SM+McM       3.211 0.501 35    2.193    4.230\n\nsigma used for effect sizes: 5.657 \nConfidence level used: 0.95"
  },
  {
    "objectID": "week09/9_2-planned_contrasts.html#reporting-your-results",
    "href": "week09/9_2-planned_contrasts.html#reporting-your-results",
    "title": "\n35  Multiple comparisons in One-way ANOVA, pt. 2: planned contrasts\n",
    "section": "\n35.5 Reporting your results",
    "text": "35.5 Reporting your results\nWhen reporting your results, we typically report:\n\nthe omnibus ANOVA\na statement on what multiple comparisons were run and how corrected\nnote which comparisons were significant.\n\nSince our comparisons, if corrected, rely on adjusted p.values its kosher to simply state that \\(p<\\alpha\\). If you elect to use the actual p values, then you need to note that they are corrected.\nFor planned comparisons, we might say:\n\nOur prevailing hypothesis predicted that response latency for groups SS, MM, and MS would be significantly less than groups SM and McM. To test this hypothesis we performed a planned contrast (no corrections). Our results revealed statistically significant difference between these groups, \\(t\\)(35) = 9.95, \\(p\\) < .001, \\(d\\) = 3.21, where the response latency in MS, MM, and SS was lower than SM and McM.\n\nNote that the \\(df\\) in the t-test are the error \\(df\\) from the omnibus model (35)."
  },
  {
    "objectID": "week10/10_1-main_effects.html#tldr",
    "href": "week10/10_1-main_effects.html#tldr",
    "title": "\n36  Factorial ANOVA - The omnibus ANOVA and Main effects\n",
    "section": "\n36.1 TLDR;",
    "text": "36.1 TLDR;\nTo run a factorial ANOVA we can use the lm() method we are familiar with. In this walkthrough we only focus on what can be done with the omnibus ANOVA assuming no interactions:\nThe general steps for running a factorial ANOVA:\n\nconstruct your ANOVA model using lm\n\nuse the residuals of the model to test for normality and heteroscadicity\ntest the model, checking for the presence of an interaction, and any main effects.\nIf no interaction proceed with any necessary post hoc analyses on the main effects.\n\nThis week we will be focusing on Steps 1, 2, and 4, i.e., we won’t be formally looking at potential interaction effects. However, as we will see next week, analyzing and interpreting interaction effects is a critical part of factorial ANOVA.\nFor now, I want you focused on building intuitions about dealing with main effects.\n\n36.1.1 Example\n\n# Preliminaries\n## load in data\ndataset <- read_csv(\"https://raw.githubusercontent.com/tehrandavis/graduate_statistics/main/practice_datasets/factorial_ANOVA_dataset_no_interactions.csv\")\n\nRows: 36 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Lecture, Presentation\ndbl (2): Score, PartID\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n## in this case, fixing IV columns...\ndataset$Lecture <- recode_factor(dataset$Lecture, \"1\"=\"Phys\",\"2\"=\"Soc\",\"3\"=\"Hist\")\n\ndataset$Presentation <- recode_factor(dataset$Presentation, \"1\"=\"Comp\",\"2\"=\"Stand\")\n\n\n# Step 1: create the additive ANOVA model\n# note that this is not a full factorial model, but what we are focusing on this week:\naov_model <- lm(Score~Lecture + Presentation,data = dataset)\n\n# Step 2a: test normalty / heteroscadicity using model visually, best to run from your console:\nperformance::check_model(aov_model)\n\n\n\n# Step 2b: check normality and homogeneity assumptions using tests:\n\n# test residuals of model for normality:\naov_model %>% resid() %>% shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.98464, p-value = 0.8873\n\n# test homogeneity:\n# note we are using a test from the performance package due to (somewhat) artificial limitations of the instruction method (see below)\naov_model %>% performance::check_homogeneity()\n\nWarning: Variances differ between groups (Bartlett Test, p = 0.040).\n\n# Step 3: check the F-ratio for significance\n# note I'm only slecting columns from the anova table that are relevant to us:\n\naov_model %>% sjstats::anova_stats() %>%\n  dplyr::select(c(\"term\",\"df\",\"statistic\",\"p.value\",\"partial.etasq\"))\n\nterm         | df | statistic | p.value | partial.etasq\n-------------------------------------------------------\nLecture      |  2 |     3.779 |   0.034 |         0.191\nPresentation |  1 |    22.670 |  < .001 |         0.415\nResiduals    | 32 |           |         |              \n\n# Step 4: Post hoc analyses on main effects\nemmeans(aov_model, specs = pairwise~Lecture, adjust=\"tukey\")\n\n$emmeans\n Lecture emmean   SE df lower.CL upper.CL\n Phys      40.0 2.14 32     35.6     44.4\n Soc       31.8 2.14 32     27.5     36.2\n Hist      34.5 2.14 32     30.1     38.9\n\nResults are averaged over the levels of: Presentation \nConfidence level used: 0.95 \n\n$contrasts\n contrast    estimate   SE df t.ratio p.value\n Phys - Soc      8.17 3.03 32   2.696  0.0291\n Phys - Hist     5.50 3.03 32   1.815  0.1807\n Soc - Hist     -2.67 3.03 32  -0.880  0.6565\n\nResults are averaged over the levels of: Presentation \nP value adjustment: tukey method for comparing a family of 3 estimates \n\nemmeans(aov_model, specs = pairwise~Presentation, adjust=\"tukey\")\n\n$emmeans\n Presentation emmean   SE df lower.CL upper.CL\n Comp           41.3 1.75 32     37.8     44.9\n Stand          29.6 1.75 32     26.0     33.1\n\nResults are averaged over the levels of: Lecture \nConfidence level used: 0.95 \n\n$contrasts\n contrast     estimate   SE df t.ratio p.value\n Comp - Stand     11.8 2.47 32   4.761  <.0001\n\nResults are averaged over the levels of: Lecture"
  },
  {
    "objectID": "week10/10_1-main_effects.html#analysis-of-variance-factorial-anova",
    "href": "week10/10_1-main_effects.html#analysis-of-variance-factorial-anova",
    "title": "\n36  Factorial ANOVA - The omnibus ANOVA and Main effects\n",
    "section": "\n36.2 Analysis of Variance: Factorial ANOVA",
    "text": "36.2 Analysis of Variance: Factorial ANOVA\nIn this week’s vignette we are simply building upon the previous two weeks coverage of One-way ANOVA and multiple comparisons. I’m assuming you’ve taken a look at all of the assigned material related to these topics. This week we up the ante by introducing more complex ANOVA models, aka factorial design. As we discussed in class, a factorial ANOVA design is required (well, for the purposes of this course) when your experimental design has more than one IV. Our examples this week focus on situations involving two IVs, however, what is said here applies for more complex designs involving 3, 4, 5, or however many IV’s you want to consider. Well, maybe not however many… as we we’ll see this week and the next, the more IVs you include in your analysis, the more difficult interpreting your results becomes. This is especially true if you have interaction effects running all over the place. But perhaps I’m getting a little bit ahead of myself. Let’s just way I wouldn’t recommend including more than 3 or 4 IVs in your ANOVA at a single time and for now leave it at that."
  },
  {
    "objectID": "week10/10_1-main_effects.html#main-effect-main-effect-and-interactions-oh-my",
    "href": "week10/10_1-main_effects.html#main-effect-main-effect-and-interactions-oh-my",
    "title": "\n36  Factorial ANOVA - The omnibus ANOVA and Main effects\n",
    "section": "\n36.3 Main effect, main effect, and interactions… oh my!",
    "text": "36.3 Main effect, main effect, and interactions… oh my!\nWhen we are performing a factorial ANOVA we are performing a series of independent comparisons of means as a function of our IVs (this assumption of independence is one of the reasons that we don’t typically concern ourselves with adjusting our p-values in the omnibus factorial ANOVA). For any given number of IVs, or factors, we test for a main effect of that factor on the data—that is “do means grouped by levels within that factor differ from one another not taking into consideration the influence of any of the other IVs”. Our tests for interactions do consider the possibility that our factors influence one another—that is,”do the differences that are observed in one factor depend on the intersecting level of another?”\nFor the sake of simplicity, we will start with a 2 × 2 ANOVA and work our way up by extending the data set. Given our naming conventions, saying that we have a 2 × 2 ANOVA indicates that there are 2 IVs and each has 2 levels. A 2 × 3 ANOVA indicates that there are 2 IVs, and that the first IV has 2 levels and the second has 3 levels; a 2 × 3 × 4 ANOVA indicates that we have 3 IVs, the first has 2 levels, the second has 3 levels, and the third has 4 levels.\nOur example ANOVA comes from a study testing the effects of smoking on performance in different types of putatively (perhaps I’m showing my theoretical biases here) information processing tasks.\nThere were 3 types of cognitive tasks:\n\n1 = a pattern recognition task where participants had to locate a target on a screen;\n2 = a cognitive task where participants had to read a passage and recall bits of information from that passage later;\n3 = participants performed a driving simulation.\n\nAdditionally, 3 groups of smokers were recruited\n\n1 = those that were actively smoking prior to and during the experiment;\n2 = those that were smokers, but did not smoke 3 hours prior to the experiment;\n3 = non-smokers.\n\nAs this is a between design, each participants only completed one of the cognitive tasks."
  },
  {
    "objectID": "week10/10_1-main_effects.html#example-1-a-22-anova",
    "href": "week10/10_1-main_effects.html#example-1-a-22-anova",
    "title": "\n36  Factorial ANOVA - The omnibus ANOVA and Main effects\n",
    "section": "\n36.4 Example 1: a 2×2 ANOVA",
    "text": "36.4 Example 1: a 2×2 ANOVA\nLet’s grab the data from from the web. Note that for now we are going to ignore the covar column.\n\ndataset <- read_delim(\"https://www.uvm.edu/~statdhtx/methods8/DataFiles/Sec13-5.dat\", \"\\t\", escape_double = FALSE, trim_ws = TRUE)\n\nRows: 135 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\ndbl (4): Task, Smkgrp, score, covar\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndataset\n\n# A tibble: 135 × 4\n    Task Smkgrp score covar\n   <dbl>  <dbl> <dbl> <dbl>\n 1     1      1     9   107\n 2     1      1     8   133\n 3     1      1    12   123\n 4     1      1    10    94\n 5     1      1     7    83\n 6     1      1    10    86\n 7     1      1     9   112\n 8     1      1    11   117\n 9     1      1     8   130\n10     1      1    10   111\n# ℹ 125 more rows\n\n\nLooking at this data, the first think that we need to do is recode the factors. Recall that we can do this using recode_factor…\n\n36.4.1 Ninja-up\nAs an alternative to mutate(), you can add a new column to a data frame by invoking dataframe$new_column. This saves us a bit of typing. Also, for weeks I’ve told you not to overwrite existing columns, and I maintain that is best practice for beginners. However, in this example we will overwrite.\n\ndataset$Task <- recode_factor(dataset$Task, \"1\" = \"Pattern Recognition\", \"2\" = \"Cognitive\", \"3\" = \"Driving Simulation\") \n\ndataset$Smkgrp <- recode_factor(dataset$Smkgrp, \"3\" = \"Nonsmoking\", \"2\" = \"Delayed\", \"1\" = \"Active\")\n\nTo get a quick view of our data structure we can use two kinds of calls:\nsummary() provides us with info related to each column in the data frame. If a column contains a factor it provides frequency counts of each level. It the column is numeric it provides summary stats:\n\nsummary(dataset)\n\n                  Task           Smkgrp       score           covar      \n Pattern Recognition:45   Nonsmoking:45   Min.   : 0.00   Min.   : 64.0  \n Cognitive          :45   Delayed   :45   1st Qu.: 6.00   1st Qu.: 99.0  \n Driving Simulation :45   Active    :45   Median :11.00   Median :111.0  \n                                          Mean   :18.26   Mean   :112.5  \n                                          3rd Qu.:26.00   3rd Qu.:128.0  \n                                          Max.   :75.00   Max.   :191.0  \n\n\nIn addition, I like to use the ezDesign() function from the ez package to get a feel for counts in each cell. This is useful for identifying conditions that may have missing data.\n\nez::ezDesign(data = dataset, \n             x=Task, # what do you want along the x-axis\n             y=Smkgrp, # what do you want along the y-axis\n             row = NULL, # are we doing any sub-divisions by row...\n             col = NULL) # or column\n\n\n\n\nThis provides us with a graphic representation of cell counts. In this case, every condition (cell) has 15 participants. As you can see right now this is a 3 x 3 ANOVA.\nTo start, let’s imagine that we are only comparing the active smokers to the nonsmokers, and that we are only concerned with the pattern recognition v driving simulation. In this circumstance we are running a 2 (smoking group: active v. passive) × 2 (task: pattern recognition v. driving simulation) ANOVA. We can do a quick subsetting of this data using the filter() command. For our sake, let’s create a new object with this data, dataset_2by2:\n\n# subsetting the data. Remember that \"!=\" means \"does not equal\"; \"&\" suggests that both cases must be met, so\ndataset_2by2 <- filter(dataset, Smkgrp!=\"Delayed\" & Task!=\"Cognitive\")\n\nTo get a quick impression of what this dataset looks like, we can use the summary() function, or ezDesign():\n\n# getting a summary of dataset_2by2:\nsummary(dataset_2by2)\n\n                  Task           Smkgrp       score           covar       \n Pattern Recognition:30   Nonsmoking:30   Min.   : 0.00   Min.   : 64.00  \n Cognitive          : 0   Delayed   : 0   1st Qu.: 2.75   1st Qu.: 98.75  \n Driving Simulation :30   Active    :30   Median : 8.00   Median :111.00  \n                                          Mean   : 7.90   Mean   :110.73  \n                                          3rd Qu.:11.00   3rd Qu.:123.00  \n                                          Max.   :22.00   Max.   :168.00  \n\nez::ezDesign(data = dataset_2by2, \n             x=Task, # what do you want along the x-axis\n             y=Smkgrp, # what do you want along the y-axis\n             row = NULL, # are we doing any sub-divisions by row...\n             col = NULL) # or column\n\n\n\n\nYou may notice from the summary above that the groups that were dropped “Delayed” smokers and “Cognitive” task still show up in the summary, albeit now with 0 instances (you’ll also notice that the remaining groups decreased in number, can you figure out why?). In most cases, R notices this an will automatically drop these factors in our subsequent analyses. However, if needed (i.e. it’s causing errors), these factors can be dropped by invoking the droplevels()function like so:\n\ndataset_2by2 <- dataset_2by2 %>% droplevels()\nsummary(dataset_2by2)\n\n                  Task           Smkgrp       score           covar       \n Pattern Recognition:30   Nonsmoking:30   Min.   : 0.00   Min.   : 64.00  \n Driving Simulation :30   Active    :30   1st Qu.: 2.75   1st Qu.: 98.75  \n                                          Median : 8.00   Median :111.00  \n                                          Mean   : 7.90   Mean   :110.73  \n                                          3rd Qu.:11.00   3rd Qu.:123.00  \n                                          Max.   :22.00   Max.   :168.00  \n\n\nAnd to see the cell counts:\n\nez::ezDesign(data = dataset_2by2, x=Task,y=Smkgrp,row = NULL,col = NULL)"
  },
  {
    "objectID": "week10/10_1-main_effects.html#making-sense-of-plots",
    "href": "week10/10_1-main_effects.html#making-sense-of-plots",
    "title": "\n36  Factorial ANOVA - The omnibus ANOVA and Main effects\n",
    "section": "\n36.5 Making sense of plots",
    "text": "36.5 Making sense of plots\nLet’s go ahead and plot the data using a line plot with 95% CI error bars. Note that these plots (up until the last section) are not APA-complete!!!!\n\n36.5.1 Interaction plots\nInteraction plots take into consideration the influence of each of the IVs on one another—in this case the mean and CI of each smoking group (Active v. Nonsmoking) as a function of Task (Driving Simulation v. Pattern Recognition). There is an additional consideration when plotting multiple predictors. That said, all we are doing is extending the plotting methods that you have been using for the past few weeks. The important addition here is the addition of group= in the first line the ggplot. For example:\n\nggplot2::ggplot(data = dataset, mapping=aes(x=Smkgrp,y=score,group=Task))\n\nindicates that we are:\n\nusing the dataset data set\nputting first IV, Smkgrp, on the x-axis\nputting our dv, score on the y-axis\nand grouping our data by our other IV, Task\n\nThis last bit is important as it makes clear that the resulting mean plots should be of the cell means related to Smkgrp x Task\nFor example, a line plot might look like this. Note that I am assigning a different shape and linetype to each level of Task:\n\n# line plot\nggplot(data = dataset_2by2, mapping=aes(x=Smkgrp,\n                                        y=score,\n                                        group=Task,\n                                        shape = Task)) +\n  stat_summary(geom=\"pointrange\",\n               fun.data = \"mean_se\") + \n  stat_summary(geom = \"line\", \n               fun = \"mean\", \n               aes(linetype = Task)) +\n  theme_cowplot()\n\n\n\n\nOne thing you may have noticed is that is very difficult to distinguish between my Task groups at the Non-smoking level, the points and error bars are overlapping one another. One way to fix this is to use the position_dodge() function. This effectively shifts the data points to the left and right to separate them from one another. You need to perform an identical position_dodge() for each element you intend to shift. In this case, we need to include it in both the pointrange and line parts of our both.\nRewriting the above (while also getting rid of color per APA format)\n\n# line plot with dodging\nggplot(data = dataset_2by2, \n       mapping=aes(x=Smkgrp,\n                   y=score,\n                   group=Task,\n                   shape = Task)\n       ) +\n  stat_summary(geom=\"pointrange\",\n               fun.data = \"mean_se\",\n               position = position_dodge(0.25),\n) + \n  stat_summary(geom = \"line\", \n               fun = \"mean\", \n               aes(linetype = Task),\n               position = position_dodge(0.25)\n) +\n  theme_cowplot()\n\n\n\n\nA brief inspection of the plot can be quite informative. Let’s start with the interaction, in fact: you should always start with the interaction. Since this is an “interaction” plot, often a quick visual inspection will allow us to predict whether our subsequent ANOVA will likely yield an interaction effect (it’s good practice to plot your data before running your ANOVA). A simple rule of thumb is that if you see the lines converging or intersecting then more than likely an interaction is present. However, whether it’s statistically significant is another question. You might think that this rule of thumb is useful if you use a line plot, and well, you’d be right. What about a bar plot, you ask? * Note that when generating a grouped barplot, you may need to position dodge to ensure your bars don’t overlap. Typically selecting a value of 0.9 ensures they do not overlap, but also that your grouped bars are touching one another with no gap between. For practice try changing the position_dodge values in the plot below to see how that effects the plot.\n\n# barplot\nggplot(data = dataset_2by2, mapping=aes(x=Smkgrp,y=score,group=Task)) +\n  stat_summary(geom = \"bar\",\n               fun = \"mean\",\n               color=\"black\", \n               aes(fill=Task), \n               position=position_dodge(.9)) +\n    stat_summary(geom=\"errorbar\", \n                 width=.3, \n                 fun.data = \"mean_se\", \n                 position = position_dodge(.9)) +\n  theme(legend.position=\"none\") +\n  scale_fill_manual(values = c(\"light grey\", \"white\"))\n\n\n\n\nNote that the grey fills above are the “Driving Simulation” group and the white are the “Pattern Recognition”. Take a look at the means. If the relative difference between grouped means changes as you move from one category on the x-axis to the next, you likely have an interaction. Note that this is a general rule of thumb and applies to the line plots as well (the reason that the lines intersect is because of these sorts of changes). In this case, the bar (cell) means on the “Active” grouping are nearly identical, while the bar means in the “Nonsmoking” grouping are much further apart. So we likely have interaction.\n\n36.5.2 Plotting main effects\nIf we wanted we could also create separate plots related to our mean effects. Remember that a main effect takes a look at the difference in means for one IV independent of the other IV(s). For example, consider Smkgrp, we are looking at the means of Nonsmoking and Active indifferent to Task. These plots would look something like this:\n\nSmoke_main_effect_plot <- ggplot2::ggplot(data = dataset_2by2, mapping=aes(x=Smkgrp,y=score, group=1)) + \n  stat_summary(geom=\"pointrange\",\n               fun.data = \"mean_se\", \n               position=position_dodge(0)) + \n  stat_summary(geom = \"line\", \n               fun = \"mean\", \n               position=position_dodge(0)) + \n  coord_cartesian(ylim=c(4,12)) +\n  theme_cowplot()\n\nshow(Smoke_main_effect_plot)\n\n\n\n\n\nTask_main_effect_plot <- ggplot2::ggplot(data = dataset_2by2, mapping=aes(x=Task,y=score, group=1)) + \n  stat_summary(geom=\"pointrange\",\n               fun.data = \"mean_se\", \n               position=position_dodge(0)) + \n  stat_summary(geom = \"line\", \n               fun = \"mean\", \n               position=position_dodge(0)) + \n  coord_cartesian(ylim=c(4,12)) +\n  theme_cowplot()\n\nshow(Task_main_effect_plot)\n\n\n\n\nand would take a look at changes due to each IV without considering the other. Here we might infer that there is a main effect for both of our IVs.\nThat said, the original interaction plot is useful as well in assessing main effects as well. In this case, to infer whether there might be main effects we can imagine where the means would be if we collapsed our grouped plots (this is exactly what the main effects take a look at). To help with your imagination I’m going to plot our main effect means on the interaction plot. Here the grey-filled triangles represent the the collapsed Smoking Group means indifferent to task. To get these, just imagine finding the midpoint of the two circles in each level of Smoking group. The slope suggests the possibility a main effect.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nTo imagine the collapsed Task means, we can just find the y-values that intersect with the midpoints of each line (note the red line is the mean value of the Driving Simulation group):\n\n\n\n\n\nThe difference in y-intercepts suggests the possibility of a main effect.\nSo in summary, I’m guessing from plot I’ve got 2 main effects and an interaction. For this week, we are only going to focus on the main effects. In the future, you’ll see that we would need to test for the interaction as well, and if it was present we would need to “deal” with that first.\nFor now, let’s focus on testing the main effects."
  },
  {
    "objectID": "week10/10_1-main_effects.html#running-the-anova-lm-method",
    "href": "week10/10_1-main_effects.html#running-the-anova-lm-method",
    "title": "\n36  Factorial ANOVA - The omnibus ANOVA and Main effects\n",
    "section": "\n36.6 Running the ANOVA lm() method",
    "text": "36.6 Running the ANOVA lm() method\nNow we can run the using the lm() method as we have previously done with the One-way ANOVA. The new wrinkle is simply adding our additional IV terms to the the formula equation:\n\\[y=IV_1+IV_2+...+IV_n\\]\nwhere the first and second terms capture our main effects and the third is our interaction.\nUsing our data in R this formula becomes:\n\naov_model <- lm(score~Smkgrp+Task,data = dataset_2by2)"
  },
  {
    "objectID": "week10/10_1-main_effects.html#assumption-testing",
    "href": "week10/10_1-main_effects.html#assumption-testing",
    "title": "\n36  Factorial ANOVA - The omnibus ANOVA and Main effects\n",
    "section": "\n36.7 Assumption testing",
    "text": "36.7 Assumption testing\n\n36.7.1 visual inspection\nLet’s quickly assess whether the prescribed model satisfies the assumptions for general linear models. Visually we may call upon the performance::check_model(). to get a series of visual inspection checks in a single plot. Note that you encounter errors in your .qmd, you may try to run this directly from your Console\n\naov_model %>% performance::check_model()\n\n\n\n\nAlternatively, running the above with panel = FALSE allows you to see each plot individually.\n\naov_model %>% performance::check_model(panel = FALSE)\n\nFor our purposes this semester, the Homogeneity of Variance and Normality of Residuals plots are most important. Visual inspection suggests that while normality may not be and issue, there is potentially something awry with the homogeneity of variance.\n\n36.7.2 test for assessing normality\nIf we want to check the normality of residuals we may use the Shapiro-Wilkes Test. We can call it directly using shapiro.test() or from the performance library. I show both methods below (you only need to choose ONE. In week’s past you’ve been using the shapiro.test():\n\naov_model$residuals %>% shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.98464, p-value = 0.8873\n\naov_model %>% performance::check_normality()\n\nOK: residuals appear as normally distributed (p = 0.887).\n\n\nKeep in mind that the Shapiro-Wilkes test for normality is notoriously conservative. In week’s past we discussed an alternative based on this method from Kim (2013) “Statistical notes for clinical researchers: assessing normal distribution using skewness and kurtosis”:\n\n# recommendation from Kim (2013)\nsource(\"https://raw.githubusercontent.com/tehrandavis/graduate_statistics/main/custom_functions/skew_kurtosis_z.R\")\n\naov_model$residuals %>% skew_kurtosis_z()\n\n       upr.ci crit_vals\nskew_z   0.75      1.96\nkurt_z  -0.02      1.96\n\n\nBased on what you see what conclusions might you make?\n\n36.7.3 test for assessing homogeneity of variance\nTypically, we can submit our aov_model to car::leveneTest to test for homogeneity. However, for this week only, this will produce an error. This is because the car::leveneTest() demands that the IVs in a factorial ANOVA be crossed. That is, the Levene’s test requires that we include the interaction term in our model. This week, we are NOT looking at interactions, nor am I asking you to include the terms in our model. Because of this, we will us an alternate method, the Bartlet test while noting that in future weeks, and under most practical circumstances this will not be an issue. FWIW, the performance method works as a reliable alternative to car::leveneTest in most circumstances, so you could elect to just use this instead using method = \"levene\".\n\naov_model %>% performance::check_homogeneity(method = \"bartlet\")\n\nWarning: Variances differ between groups (Bartlett Test, p = 0.040).\n\n\nOur test says we have a violation (which is consistent with our visual inspection above. Let’s take a look at the 3x time rule. To do this we need to get a feel for the variance within each cell. While we are at it we might as well get the means too. Which leads to the next section (don’t worry we’ll come back to this)"
  },
  {
    "objectID": "week10/10_1-main_effects.html#getting-cell-means-and-sd",
    "href": "week10/10_1-main_effects.html#getting-cell-means-and-sd",
    "title": "\n36  Factorial ANOVA - The omnibus ANOVA and Main effects\n",
    "section": "\n36.8 Getting cell means and sd",
    "text": "36.8 Getting cell means and sd\nUp until this point we’ve used psych::describeBy() to generate summary stats. This becomes a little more difficult as the designs become more complex. Alternatively you might elect to build a data table yourself. You learned how to do this a few weeks ago using the summarise function:\n\ndataset_2by2 %>% \n  group_by(Task, Smkgrp) %>%\n  summarise(mean = mean(score), # get group means\n            var = var(score)) # get group variances\n\n`summarise()` has grouped output by 'Task'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 4\n# Groups:   Task [2]\n  Task                Smkgrp      mean   var\n  <fct>               <fct>      <dbl> <dbl>\n1 Pattern Recognition Nonsmoking  9.93 42.5 \n2 Pattern Recognition Active      9.4   1.97\n3 Driving Simulation  Nonsmoking  9.93 36.1 \n4 Driving Simulation  Active      2.33  5.24\n\n\nAnother alternative, that I would recommend over the psych package is to use summarySE() from the Rmisc package:\n\nRmisc::summarySE(data = dataset_2by2, # your dataset\n                 measurevar = \"score\", # your dv\n                 groupvars = c(\"Task\",\"Smkgrp\")) # your between-groups IV(s)\n\n                 Task     Smkgrp  N    score       sd        se        ci\n1 Pattern Recognition Nonsmoking 15 9.933333 6.518837 1.6831565 3.6100117\n2 Pattern Recognition     Active 15 9.400000 1.404076 0.3625308 0.7775512\n3  Driving Simulation Nonsmoking 15 9.933333 6.005553 1.5506271 3.3257644\n4  Driving Simulation     Active 15 2.333333 2.288689 0.5909368 1.2674335\n\n\nRecalling that we had an issue with heteroscadicity, we can now use the obtained sd values to assess whether our violation is too great. Let’s save our summary table:\n\ncell_summary_table <- Rmisc::summarySE(data = dataset_2by2, # your dataset\n                                       measurevar = \"score\", # your dv\n                                       groupvars = c(\"Task\",\"Smkgrp\"))\n\nfrom this we can call the sd values and square them to get our variance:\n\n# varience is sd-squared:\ncell_summary_table$sd^2\n\n[1] 42.495238  1.971429 36.066667  5.238095\n\n\nHouston we have a problem! The largest variance there is definitely greater than 3x the smallest. What to do? For now nothing, I don’t want you to go down that rabbit hole just yet. But look for info on dealing with this later."
  },
  {
    "objectID": "week10/10_1-main_effects.html#getting-marginal-means-for-main-effects",
    "href": "week10/10_1-main_effects.html#getting-marginal-means-for-main-effects",
    "title": "\n36  Factorial ANOVA - The omnibus ANOVA and Main effects\n",
    "section": "\n36.9 getting marginal means for main effects",
    "text": "36.9 getting marginal means for main effects\nWe can also use this function to report the means related to the main effects (irrespective of interaction).\nFor example, Smoking effect I can re-write the call above, simply dropping Task from groupvars:\n\nRmisc::summarySE(data = dataset_2by2,measurevar = \"score\",groupvars = \"Smkgrp\")\n\n      Smkgrp  N    score       sd        se       ci\n1 Nonsmoking 30 9.933333 6.158444 1.1243730 2.299601\n2     Active 30 5.866667 4.049124 0.7392655 1.511968\n\n\nand for the Task effect:\n\nRmisc::summarySE(data = dataset_2by2,measurevar = \"score\",groupvars = \"Task\")\n\n                 Task  N    score       sd        se       ci\n1 Pattern Recognition 30 9.666667 4.641145 0.8473533 1.733032\n2  Driving Simulation 30 6.133333 5.905774 1.0782418 2.205252\n\n\nNote that If you are reporting means related to the main effects, you need to report these marginal means!\nHOWEVER… this data has an interaction, and remember what I said above, if the data has an interaction, you need to deal with that first. We’ll show how to work with data that has interactions next week."
  },
  {
    "objectID": "week10/10_1-main_effects.html#testing-the-anova",
    "href": "week10/10_1-main_effects.html#testing-the-anova",
    "title": "\n36  Factorial ANOVA - The omnibus ANOVA and Main effects\n",
    "section": "\n36.10 Testing the ANOVA",
    "text": "36.10 Testing the ANOVA\nOk. Now let’s look at our ANOVA table. Note that instead of the full print out, I’m asking sjstats to only give me the values I’m interested in: the term (effect), df, (F) statistic, p-value, and effect size, in this case partial.eta\n\nsjstats::anova_stats(aov_model) %>% select(\"term\",\"df\",\"statistic\",\"p.value\",\"partial.etasq\")\n\nterm         | df | statistic | p.value | partial.etasq\n-------------------------------------------------------\nLecture      |  2 |     3.779 |   0.034 |         0.191\nPresentation |  1 |    22.670 |  < .001 |         0.415\nResiduals    | 32 |           |         |              \n\n\nYou’ll note that both main effects are significant, meaning that we would need to perform pairwise, post-hoc comparisions for each effect. BUT you’ll also note that each of our factors ONLY HAD TWO LEVELS, meaning that the F-test in the ANOVA table IS a pairwise comparison. In cases like this, when the effect only has 2 levels, we are done (any post hoc analysis would be redundant).\nAssuming you feel comfortable with everything in this walkthrough, let’s proceed to the next, which includes cases where our factors have three (or more) levels."
  },
  {
    "objectID": "week10/10_2-factorial_w_post_hocs.html#example-a-2-3-anova",
    "href": "week10/10_2-factorial_w_post_hocs.html#example-a-2-3-anova",
    "title": "\n37  Factorial ANOVA - Post hoc analysis of Main effects\n",
    "section": "\n37.1 Example: A 2 × 3 ANOVA",
    "text": "37.1 Example: A 2 × 3 ANOVA\nThirty-six college students were randomly assigned to 3 groups (N=12). Students in each group were asked to watch and take notes on a 50 min lecture. One week later all students were tested on the content of their lectures, and their scores were compared. Groups differed by the lecture’s subject matter, where:\n\n\n1 = Physics lecture\n\n2 = Social Science lecture\n\n3 = History lecture\n\nThe lectures were presented in two manners\n\n1 = via computer\n2 = standard method, lecturer in a lecture hall\n\nThe researchers hypothesized that students that learned the material in the standard lecture would perform better than those that learned via computer.\n\ndataset_no_inter <- read_csv(\"https://raw.githubusercontent.com/tehrandavis/graduate_statistics/main/practice_datasets/factorial_ANOVA_dataset_no_interactions.csv\")\n\nRows: 36 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Lecture, Presentation\ndbl (2): Score, PartID\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndataset_no_inter$Lecture <- recode_factor(dataset_no_inter$Lecture, \"1\"=\"Phys\",\"2\"=\"Soc\",\"3\"=\"Hist\")\n\ndataset_no_inter$Presentation <- recode_factor(dataset_no_inter$Presentation, \"1\"=\"Comp\",\"2\"=\"Stand\")\n\nsummary(dataset_no_inter)\n\n Lecture   Presentation     Score           PartID     \n Phys:12   Comp :18     Min.   :18.00   Min.   : 1.00  \n Soc :12   Stand:18     1st Qu.:26.75   1st Qu.: 9.75  \n Hist:12                Median :35.50   Median :18.50  \n                        Mean   :35.44   Mean   :18.50  \n                        3rd Qu.:42.50   3rd Qu.:27.25  \n                        Max.   :53.00   Max.   :36.00  \n\n\n\n37.1.1 Building the ANOVA model\nI’m going to use lm().\n\naov_model <- lm(Score~Presentation + Lecture, data = dataset_no_inter)\n\n\n37.1.2 Testing assumptions\n\n37.1.3 visual\n\naov_model %>% performance::check_model(panel=FALSE)\n\n\naov_model %>% performance::check_normality()\n\nOK: residuals appear as normally distributed (p = 0.887).\n\naov_model %>% performance::check_homogeneity()\n\nWarning: Variances differ between groups (Bartlett Test, p = 0.040).\n\n\nThe model checks both for normality of residuals but fails homogeneity of variance. Following upon given the robustness of ANOVA (3x rule):\n\ndataset_no_inter %>% \n  group_by(Lecture, Presentation) %>%\n  summarise(mean = mean(Score),\n            var = var(Score))\n\n`summarise()` has grouped output by 'Lecture'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 4\n# Groups:   Lecture [3]\n  Lecture Presentation  mean    var\n  <fct>   <fct>        <dbl>  <dbl>\n1 Phys    Comp          46    48.8 \n2 Phys    Stand         34   121.  \n3 Soc     Comp          40    23.2 \n4 Soc     Stand         23.7   7.47\n5 Hist    Comp          38    19.2 \n6 Hist    Stand         31   106.  \n\n\nPotential issues, but moving on.\n\n37.1.4 Interaction plot\nLet’s plot the data. You’ll note in this plot I’m adding some code to get it into better APA shape. More on this in the next walkthrough. For now just focus on content.\n\n# setting original parameters\np <- ggplot2::ggplot(data = dataset_no_inter, mapping=aes(x=Lecture,y=Score,group=Presentation))\n\n# making a basic line plot\nline_p <- p + stat_summary(geom=\"pointrange\",fun.data = \"mean_se\", size=0.75, position=position_dodge(.25), aes(shape=Presentation)) + \n  stat_summary(geom = \"line\", fun = \"mean\", position=position_dodge(.25), aes(linetype=Presentation))\n\n# adding APA elements\nline_p <- line_p + theme(\n    axis.title = element_text(size = 16, face = \"bold\", lineheight = .55),\n    axis.text = element_text(size = 12),\n    legend.title = element_text(size = 12, face = \"bold\"),\n    legend.position = c(.75,.9)) +\n  xlab(\"Lecture type\") + \n  ylab (\"Performance score\") +\n  theme(plot.margin=unit(c(.25,.25,.25,.25),\"in\")) + \n  theme_cowplot()\n\nshow(line_p)\n\n\n\n\n\n37.1.5 Testing the ANOVA\n\naov_model %>% sjstats::anova_stats() %>% \n  select(\"term\",\"df\",\"statistic\",\"p.value\",\"partial.etasq\")\n\nterm         | df | statistic | p.value | partial.etasq\n-------------------------------------------------------\nPresentation |  1 |    22.670 |  < .001 |         0.415\nLecture      |  2 |     3.779 |   0.034 |         0.191\nResiduals    | 32 |           |         |"
  },
  {
    "objectID": "week10/10_2-factorial_w_post_hocs.html#post-hoc-analysis",
    "href": "week10/10_2-factorial_w_post_hocs.html#post-hoc-analysis",
    "title": "\n37  Factorial ANOVA - Post hoc analysis of Main effects\n",
    "section": "\n37.2 post hoc analysis",
    "text": "37.2 post hoc analysis\nAdmittedly, moving on to this step will ultimately be qualified by the presence of interactions (next week). For now, note that if you don’t have an interaction, you may simply proceed to run post-hoc analyses on any significant main effects in the manner you would with a One-way ANOVA. Easy, peasy, right. One thing to note, you need to make the appropriate multiple comparison corrections. For example, returning to our data with no interaction, we need to test for differences in both the Lecture and Presentation main effects.\n\nPresentation: This one is easy. We only have two levels of Presentation, so the omnibus \\(F\\) test tells us that our two groups are different. Nothing else to do here other than note which mean (Computer v. Standard) is greater than the other.\nLecture: We have three levels of lecture, so were are going to need to run a post-hoc analysis. In this case, we may call upon our old standbys, Tukey and Bonferroni.\n\nJust as last week, we can use emmeans() to run our post-hoc tests.\nFor example, to run a Tukey, you need call your ANOVA model. For the sake of clarity let’s rebuild the ANOVA model and save it to aov_model and then run emmeans()\nHere I’m saving it to the object aov_model:\n\naov_model <- lm(Score~Presentation + Lecture, data = dataset_no_inter)\n\nFrom here you may call upon the emmeans() function to derive your posthocs. By itself, emmeans produces the means by levels of the IV(s) listed in its spec= argument. It takes the lm() model as a first argument, and the IVs of interest as the second.\n\n# input your model into the emmeans,\n# interested in Lecture\n\nemmeans(aov_model,specs = ~Lecture)\n\n Lecture emmean   SE df lower.CL upper.CL\n Phys      40.0 2.14 32     35.6     44.4\n Soc       31.8 2.14 32     27.5     36.2\n Hist      34.5 2.14 32     30.1     38.9\n\nResults are averaged over the levels of: Presentation \nConfidence level used: 0.95 \n\n\nemmeans() alone gives us the estimated marginal means for each of our levels, to run a post-hoc comparison we then pipe it into pairs() and include the p value adjustment that we would like to make. allows another method for making contrasts (planned and posthoc). If you want to perform a Tukey test follow this procedure you can simply pipe the previous (or save to an object and submit) to pairs():\n\nemmeans(aov_model,specs = ~Lecture) %>% pairs(adjust=\"tukey\") \n\n contrast    estimate   SE df t.ratio p.value\n Phys - Soc      8.17 3.03 32   2.696  0.0291\n Phys - Hist     5.50 3.03 32   1.815  0.1807\n Soc - Hist     -2.67 3.03 32  -0.880  0.6565\n\nResults are averaged over the levels of: Presentation \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nIn this case it appears that our Tukey does reveal differences between means, when performance of those getting Physical Science Lectures is greater than both History and Social, but Social and History are not difference from one another.\nNote that you may call other p-value adjustments using these methods:\n\nemmeans(aov_model,specs = ~Lecture) %>% pairs(adjust=\"bonferroni\")\n\n contrast    estimate   SE df t.ratio p.value\n Phys - Soc      8.17 3.03 32   2.696  0.0333\n Phys - Hist     5.50 3.03 32   1.815  0.2365\n Soc - Hist     -2.67 3.03 32  -0.880  1.0000\n\nResults are averaged over the levels of: Presentation \nP value adjustment: bonferroni method for 3 tests"
  },
  {
    "objectID": "week10/10_2-factorial_w_post_hocs.html#what-about-planned-contrasts",
    "href": "week10/10_2-factorial_w_post_hocs.html#what-about-planned-contrasts",
    "title": "\n37  Factorial ANOVA - Post hoc analysis of Main effects\n",
    "section": "\n37.3 What about planned contrasts?",
    "text": "37.3 What about planned contrasts?\nYou need to be careful when running planned contrasts in factorial ANOVA. In general I would recommend only running planned contrasts on a single main effect, or a planned contrast on the effects of one of your factors at a single level of your other (though you still need to proceed with caution here).\nFor example, using the data from the last section, I would only run a planned contrast related to the main effect of Lecture Type, or a contrast of Lecture Type means only in Computer presentation conditions (or Standard presentation). DO NOT, I repeat DO NOT run contrasts that go across levels of your other factors. Well, truthfully, you can do whatever you want, but you may find that your ability to meaningfully interpret your results in such cases is extremely limited.\nWe can run planned contrasts using emmeans() as well. In this case, we need to specify the contrasts.\nFirst we need to obtain the emmeans() of the model including all cells (all factors). Using aov.model from the previous example:\n\nemmeans(aov_model, specs = ~Lecture+Presentation)\n\n Lecture Presentation emmean   SE df lower.CL upper.CL\n Phys    Comp           45.9 2.47 32     40.9     50.9\n Soc     Comp           37.7 2.47 32     32.7     42.8\n Hist    Comp           40.4 2.47 32     35.4     45.4\n Phys    Stand          34.1 2.47 32     29.1     39.1\n Soc     Stand          25.9 2.47 32     20.9     31.0\n Hist    Stand          28.6 2.47 32     23.6     33.6\n\nConfidence level used: 0.95 \n\n\nOK. From here let’s build two custom contrasts. First, let’s do the Lecture contrast on the main effect. In this case let’s assume I want to contrast Phys with the combined other two conditions. Using the output above, I identify which rows contain Phys and I ensure that the summation of those rows is 1. In this case there are two rows, the first and the fourth, and each gets 0.5. My remaining conditions must also equal -1. In this case there are four, so each is -0.25. Following the output above, then my contrast matrix is:\n\nlecture_contrast <- list( \"Phys v. Soc + Hist\" = c(.5,-.25,-.25,.5,-.25,-.25))\n\nFrom here I simply call contrast() with my contrast matrix as an argument. So the entire pipe goes from:\n\nemmeans(aov_model, specs = ~Lecture + Presentation) %>% contrast(lecture_contrast)\n\n contrast           estimate   SE df t.ratio p.value\n Phys v. Soc + Hist     6.83 2.62 32   2.604  0.0138\n\n\nAssuming I wanted to perform a set of orthogonal contrasts:\n\nPhys v. Soc and Hist and\nSoc v Hist\n\n\n# build the contrast matrix\ncontrast_matrix <- list(\"Phys v. Soc + Hist\" = c(.5,-.25,-.25,.5,-.25,-.25),\n                        \"Soc v Hist\" = c(0, -.5, .5, 0, -.5, .5)\n                        )\n\n# run the contrasts\nemmeans(aov_model, specs = ~Lecture + Presentation) %>% contrast(contrast_matrix)\n\n contrast           estimate   SE df t.ratio p.value\n Phys v. Soc + Hist     6.83 2.62 32   2.604  0.0138\n Soc v Hist             2.67 3.03 32   0.880  0.3853\n\n\nIn both cases, my p-values are unadjusted. I can add an adjustment to the contrast() argument like so:\n\n# tukey is most common\nemmeans(aov_model, specs = ~Lecture + Presentation) %>% contrast(contrast_matrix, adjust = \"tukey\")\n\nNote: adjust = \"tukey\" was changed to \"sidak\"\nbecause \"tukey\" is only appropriate for one set of pairwise comparisons\n\n\n contrast           estimate   SE df t.ratio p.value\n Phys v. Soc + Hist     6.83 2.62 32   2.604  0.0275\n Soc v Hist             2.67 3.03 32   0.880  0.6222\n\nP value adjustment: sidak method for 2 tests \n\n# or bonferroni is most conservative\nemmeans(aov_model, specs = ~Lecture + Presentation) %>% contrast(contrast_matrix, adjust = \"bonferroni\")\n\n contrast           estimate   SE df t.ratio p.value\n Phys v. Soc + Hist     6.83 2.62 32   2.604  0.0277\n Soc v Hist             2.67 3.03 32   0.880  0.7706\n\nP value adjustment: bonferroni method for 2 tests \n\n# or holm is more liberal\nemmeans(aov_model, specs = ~Lecture + Presentation) %>% contrast(contrast_matrix, adjust = \"holm\")\n\n contrast           estimate   SE df t.ratio p.value\n Phys v. Soc + Hist     6.83 2.62 32   2.604  0.0277\n Soc v Hist             2.67 3.03 32   0.880  0.3853\n\nP value adjustment: holm method for 2 tests"
  },
  {
    "objectID": "week10/10_2-factorial_w_post_hocs.html#more-examples-a-3-3-anova",
    "href": "week10/10_2-factorial_w_post_hocs.html#more-examples-a-3-3-anova",
    "title": "\n37  Factorial ANOVA - Post hoc analysis of Main effects\n",
    "section": "\n37.4 More examples: a 3 × 3 ANOVA",
    "text": "37.4 More examples: a 3 × 3 ANOVA\nIn the previous example we focused in the 2 × 3 scenario for ease. Let’s look at how we might deal with a 3 × 3 example. Let’s use our dataset from the previous walkthrough involving a 3 (Smoking group) by 3 (Task) design. Let’s run another example using this data. Let’s use this as a chance to brush up on creating APA visualizations:\n\ndataset <- read_delim(\"https://www.uvm.edu/~statdhtx/methods8/DataFiles/Sec13-5.dat\", \n                      \"\\t\", escape_double = FALSE, \n                      trim_ws = TRUE)\n\nRows: 135 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\ndbl (4): Task, Smkgrp, score, covar\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndataset$PartID <- seq_along(dataset$score)\ndataset$Task <- recode_factor(dataset$Task, \n                              \"1\" = \"Pattern Recognition\", \n                              \"2\" = \"Cognitive\", \n                              \"3\" = \"Driving Simulation\") \ndataset$Smkgrp <- recode_factor(dataset$Smkgrp, \n                                \"3\" = \"Nonsmoking\", \n                                \"2\" = \"Delayed\", \n                                \"1\" = \"Active\")\n\ndataset\n\n# A tibble: 135 × 5\n   Task                Smkgrp score covar PartID\n   <fct>               <fct>  <dbl> <dbl>  <int>\n 1 Pattern Recognition Active     9   107      1\n 2 Pattern Recognition Active     8   133      2\n 3 Pattern Recognition Active    12   123      3\n 4 Pattern Recognition Active    10    94      4\n 5 Pattern Recognition Active     7    83      5\n 6 Pattern Recognition Active    10    86      6\n 7 Pattern Recognition Active     9   112      7\n 8 Pattern Recognition Active    11   117      8\n 9 Pattern Recognition Active     8   130      9\n10 Pattern Recognition Active    10   111     10\n# ℹ 125 more rows\n\n\n\n37.4.1 Interaction plot\n\n# line plot\nggplot(data = dataset, mapping=aes(x=Smkgrp,y=score,group=Task)) +\n  stat_summary(geom=\"pointrange\",\n               fun.data = \"mean_cl_normal\", \n               position=position_dodge(.5)) + \n  stat_summary(geom = \"line\", \n               fun = \"mean\", \n               position=position_dodge(.5), \n               aes(linetype=Task)) +\n  theme_cowplot()\n\n\n\n\nBefore continuing it might be useful to get a feel for whats going on in the dataset. In this case, both the performance on the Cognitive and Driving simulation tasks seems to be impacted by the degree of smoking. However the Pattern recognition task does not appear to be affected.\nAnother way of viewing this is that scores on the Cognitive task tended to be greater than the other two Task conditions. Let’s hold onto our impressions of the data and move on.\n\n37.4.2 ANOVA model\nAs before we can build our ANOVA model and test it against the requisite assumptions:\n\naov_model <- lm(score~Smkgrp+Task,data = dataset)\n\n\naov_model %>% performance::check_model(panel=F)\n\n\naov_model %>% performance::check_normality()\n\nWarning: Non-normality of residuals detected (p < .001).\n\naov_model %>% performance::check_homogeneity()\n\nWarning: Variances differ between groups (Bartlett Test, p = 0.000).\n\n\nAs in the last walkthrough we’ll ignore the issues with our assumption checks\n\naov_model %>% sjstats::anova_stats() %>%\n  dplyr::select(c(\"term\",\"df\",\"statistic\",\"p.value\",\"partial.etasq\"))\n\nterm      |  df | statistic | p.value | partial.etasq\n-----------------------------------------------------\nSmkgrp    |   2 |     7.935 |   0.001 |         0.109\nTask      |   2 |   125.398 |  < .001 |         0.659\nResiduals | 130 |           |         |              \n\n\nHere we have significant main effects for Smkgrp and Task. We’ll need to run seperate post-hoc analyses for each of our observed effects (given that both factors have 3 levels). Before moving on, I would recommend writing out the main points of this table to refer to later in your write up.\n\n\nmain effect for Task, \\(F\\) (2, 130) = 125.40, \\(p\\) < .001, \\(n_p^2\\) = .66.\nmain effect for Smoking Group, \\(F\\) (2, 130) = 7.94, \\(p\\) = .001, \\(n_p^2\\) = .11"
  },
  {
    "objectID": "week10/10_2-factorial_w_post_hocs.html#post-hoc-analysis-1",
    "href": "week10/10_2-factorial_w_post_hocs.html#post-hoc-analysis-1",
    "title": "\n37  Factorial ANOVA - Post hoc analysis of Main effects\n",
    "section": "\n37.5 Post-hoc analysis",
    "text": "37.5 Post-hoc analysis\n\n37.5.1 Task\n\nmain_effect_Task <- emmeans(aov_model, ~Task) %>% pairs(adjust=\"tukey\")\n\nmain_effect_Task\n\n contrast                                 estimate   SE  df t.ratio p.value\n Pattern Recognition - Cognitive            -29.13 2.25 130 -12.927  <.0001\n Pattern Recognition - Driving Simulation     3.29 2.25 130   1.459  0.3139\n Cognitive - Driving Simulation              32.42 2.25 130  14.386  <.0001\n\nResults are averaged over the levels of: Smkgrp \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nThe results confirm that overall, Cognitive task performance was greater than the other two conditions. To get the descriptive stats for these contrasts, we can use summarySE, only specifying Task as our grouping viariable:\n\nRmisc::summarySE(data = dataset, measurevar = \"score\", groupvars = \"Task\")\n\n                 Task  N     score        sd        se       ci\n1 Pattern Recognition 45  9.644444  4.513392 0.6728168 1.355973\n2           Cognitive 45 38.777778 18.055330 2.6915297 5.424422\n3  Driving Simulation 45  6.355556  5.701497 0.8499290 1.712919\n\n\n\n37.5.2 Smkgrp\n\nmain_effect_Smkgrp <- emmeans(aov_model, ~Smkgrp) %>% pairs(adjust=\"tukey\")\n\nmain_effect_Smkgrp\n\n contrast             estimate   SE  df t.ratio p.value\n Nonsmoking - Delayed     3.69 2.25 130   1.637  0.2339\n Nonsmoking - Active      8.93 2.25 130   3.964  0.0004\n Delayed - Active         5.24 2.25 130   2.327  0.0556\n\nResults are averaged over the levels of: Task \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nand to get these descriptive stats, we run summarySE using only Smkgrp as our grouping variable:\n\nRmisc::summarySE(data = dataset, measurevar = \"score\", groupvars = \"Smkgrp\")\n\n      Smkgrp  N    score       sd       se       ci\n1 Nonsmoking 45 22.46667 20.36218 3.035414 6.117475\n2    Delayed 45 18.77778 19.35892 2.885857 5.816063\n3     Active 45 13.53333 14.13024 2.106412 4.245194"
  },
  {
    "objectID": "week10/10_2-factorial_w_post_hocs.html#example-write-up",
    "href": "week10/10_2-factorial_w_post_hocs.html#example-write-up",
    "title": "\n37  Factorial ANOVA - Post hoc analysis of Main effects\n",
    "section": "\n37.6 Example write up",
    "text": "37.6 Example write up\nLet’s use this space to provide an example write-up for our factorial ANOVA. To do this I need to refer back to values I generated in my ANOVA table, my post-hoc tests, and my descriptive statistics above.\n\nTo test our hypothesis we ran a 3 (Task) × 3 (Smoking Group) ANOVA on cognitive performance scores. Our ANOVA revealed a significant main effect for Task, \\(F\\) (2, 130) = 125.40, \\(p\\) < .001, \\(n_p^2\\) = .66. Post hoc analysis of Task revealed that participants scored higher in the Cognitive group (\\(M±SE\\) = 38.78 ± 2.69) than the Pattern Recognition (9.64 ± 0.67 ) and Driving Simulator (6.36 ± 0.84) groups (\\(ps\\) < .05).\nOur analysis also revealed a main effect for Smoking Group, \\(F\\) (2, 130) = 7.94, \\(p\\) = .001, \\(n_p^2\\) = .11. Post hoc analysis of Smoking Group revealed participants scored higher in the Nonsmoking group than the Active group (\\(p\\) < .05). No other statistically significant group differences were observed."
  },
  {
    "objectID": "week10/10_3-advanced_example_and_apa.html#step-0-loading-in-the-packages",
    "href": "week10/10_3-advanced_example_and_apa.html#step-0-loading-in-the-packages",
    "title": "\n38  Template for Analysis & APA Style\n",
    "section": "\n38.1 Step 0: Loading in the packages",
    "text": "38.1 Step 0: Loading in the packages\n\npacman::p_load(psych, tidyverse, cowplot, emmeans, ez)"
  },
  {
    "objectID": "week10/10_3-advanced_example_and_apa.html#step-1-data-wrangling",
    "href": "week10/10_3-advanced_example_and_apa.html#step-1-data-wrangling",
    "title": "\n38  Template for Analysis & APA Style\n",
    "section": "\n38.2 Step 1: Data wrangling",
    "text": "38.2 Step 1: Data wrangling\n\n\nimport your data\nis the data in the right format?\ndo you need to make any adjustments to the data frame?\n\n\n\nlibrary(readr)\ndataset <- read_csv(\"https://raw.githubusercontent.com/tehrandavis/graduate_statistics/main/practice_datasets/factorial_ANOVA_dataset_no_interactions.csv\")\n\nRows: 36 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Lecture, Presentation\ndbl (2): Score, PartID\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# unnecessary here, but if performing planned contrasts you need to factorize predictors (IVs)\ndataset$Presentation <- as.factor(dataset$Presentation)\ndataset$Lecture <- as.factor(dataset$Lecture)\n\ndataset\n\n# A tibble: 36 × 4\n   Lecture Presentation Score PartID\n   <fct>   <fct>        <dbl>  <dbl>\n 1 Phys    Comp            53      1\n 2 Phys    Comp            49      2\n 3 Phys    Comp            47      3\n 4 Phys    Comp            42      4\n 5 Phys    Comp            51      5\n 6 Phys    Comp            34      6\n 7 Phys    Stand           44      7\n 8 Phys    Stand           48      8\n 9 Phys    Stand           35      9\n10 Phys    Stand           18     10\n# ℹ 26 more rows\n\n\n\nIn this case the data looks to be in the correct format (long). Since I am not doing planned contrasts I don’t need to worry about factorizing my IV columns."
  },
  {
    "objectID": "week10/10_3-advanced_example_and_apa.html#step-2-look-at-your-data",
    "href": "week10/10_3-advanced_example_and_apa.html#step-2-look-at-your-data",
    "title": "\n38  Template for Analysis & APA Style\n",
    "section": "\n38.3 Step 2: Look at your data",
    "text": "38.3 Step 2: Look at your data\n\n\nplot the data: I like to do histograms by condition as well as a means plot (interaction plot) and a participant number grid to get a feel for what my distribution of scores looks like. (Note I’m not using this to make decisions on my data model assumptions!)\n\n\n\nggplot(data = dataset, aes(x = Score)) +\n    geom_histogram(fill = \"gray\", color = \"black\") +\n    facet_grid(rows = vars(Lecture), cols = vars(Presentation))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nI also take a quick glance at the means of my groups (interaction plot)\n\nggplot(data = dataset, aes(x = Lecture, y = Score, group = Presentation, color = Presentation)) +\n    stat_summary(fun.data = \"mean_se\",\n                 geom = \"pointrange\") +\n    stat_summary(fun = \"mean\",\n                 geom = \"line\",\n                 lty=\"solid\")\n\n\n\n\nAnd finally I take a look at the number of participants in each group (cell) to ensure that my design is roughly balanced.\n\nez::ezDesign(dataset, x = Lecture, y = Presentation)"
  },
  {
    "objectID": "week10/10_3-advanced_example_and_apa.html#step-3-build-your-model",
    "href": "week10/10_3-advanced_example_and_apa.html#step-3-build-your-model",
    "title": "\n38  Template for Analysis & APA Style\n",
    "section": "\n38.4 Step 3: Build your model",
    "text": "38.4 Step 3: Build your model\n\n\nbuild the ANOVA model\nnote that this week we are only focusing on the additive model.\n\n\n\n# additive model (this week)\naov_model <- lm(Score~Lecture + Presentation, data = dataset)"
  },
  {
    "objectID": "week10/10_3-advanced_example_and_apa.html#step-4-check-your-model-assumptions",
    "href": "week10/10_3-advanced_example_and_apa.html#step-4-check-your-model-assumptions",
    "title": "\n38  Template for Analysis & APA Style\n",
    "section": "\n38.5 Step 4: Check your model assumptions",
    "text": "38.5 Step 4: Check your model assumptions\n\n\ncheck the model assumptions (normality and homogeneity)\n\n\n\n# visual:\naov_model %>% performance::check_model(panel = FALSE)\n\n# test of normality (this is the Shapiro Test):\naov_model %>% performance::check_normality()\n\nOK: residuals appear as normally distributed (p = 0.887).\n\n# test of homogeneity\naov_model %>% performance::check_homogeneity()\n\nWarning: Variances differ between groups (Bartlett Test, p = 0.040).\n\n\nNote: that if you wanted to check normality using the bootstrapping method from the video, there is a mistake (I forgot an important step). FWIW, this would be better than Shapiro-test but it’s much more involved\n\n# note that if you wanted to check normality using the bootstrapping method from the video, these is a mistake (I forgot an important step).\n\n# 1. build a distribution of skews (note that because this is random, your values won't equal mine):\ndistribution_skews <- DescTools::Skew(x = aov_model$residuals,\n                ci.type = \"bca\", \n                method = 2, \n                conf.level = 0.95, \n                R = 1000)\n# 2. calculate the standard error:\nstandard_error_of_skews <- (distribution_skews[3]-distribution_skews[2])/3.92\n\n# 3. take obtained skew and divide by standard_error_of_skews\npsych::skew(aov_model$residuals) / standard_error_of_skews %>% \n  unname()\n\n[1] 0.7122781\n\n# 4. compare this value against critical values from Kim (2013) method, repeat for kurtosis\n\n# ----- OR ------- use my custom function that handles all of the above\n\nsource(\"https://raw.githubusercontent.com/tehrandavis/graduate_statistics/main/custom_functions/skew_kurtosis_z.R\")\n\naov_model$residuals %>% skew_kurtosis_z()\n\n       upr.ci crit_vals\nskew_z   0.71      1.96\nkurt_z  -0.02      1.96\n\n\n\n38.5.1 SUMMARY:\n\nTest of normality and homogeneity don’t suggest troublesome any violations"
  },
  {
    "objectID": "week10/10_3-advanced_example_and_apa.html#step-5-run-the-analysis-anova",
    "href": "week10/10_3-advanced_example_and_apa.html#step-5-run-the-analysis-anova",
    "title": "\n38  Template for Analysis & APA Style\n",
    "section": "\n38.6 Step 5: Run the analysis (ANOVA)",
    "text": "38.6 Step 5: Run the analysis (ANOVA)\n\npacman::p_load(sjstats)\nsjstats::anova_stats(aov_model)[1:8]\n\nterm         | df |    sumsq |   meansq | statistic | p.value | etasq | partial.etasq\n-------------------------------------------------------------------------------------\nLecture      |  2 |  416.222 |  208.111 |     3.779 |   0.034 | 0.121 |         0.191\nPresentation |  1 | 1248.444 | 1248.444 |    22.670 |  < .001 | 0.364 |         0.415\nResiduals    | 32 | 1762.222 |   55.069 |           |         |       |              \n\n\n\n38.6.1 SUMMARY\n\nMain effect Lecture: F(2, 30) = 3.83, p = .03, np2 = .20\nMain effect Presentation: F(1, 30) = 22.96, p < .001, np2 = .43\nremember that since this is a factorial ANOVA I report my partial eta square for effect size."
  },
  {
    "objectID": "week10/10_3-advanced_example_and_apa.html#step-6-run-additional-tests-based-what-your-anova-outcomes",
    "href": "week10/10_3-advanced_example_and_apa.html#step-6-run-additional-tests-based-what-your-anova-outcomes",
    "title": "\n38  Template for Analysis & APA Style\n",
    "section": "\n38.7 Step 6: Run additional tests based what your ANOVA outcomes",
    "text": "38.7 Step 6: Run additional tests based what your ANOVA outcomes\n\n\n\ndeal with the ANOVA outcomes ONE AT A TIME!!!\n\nsimple effects ANOVA (if there is an interaction… more on this next week)\npost hoc analyses\nor if you have planned contrasts run those here\n\n\n\nmake a quick summary of each result\n\ninclude any relevant means or tests (t-tests, p vals, etc)\n\n\n\n\nHere we are performing posthocs. We’ll have two sets (although for now the post hoc test on Presentation is redundant.\n\n38.7.1 Post hoc analysis for Lecture\n\nemmeans(aov_model, spec = ~Lecture) %>% pairs(adjust = \"tukey\")\n\n contrast    estimate   SE df t.ratio p.value\n Hist - Phys    -5.50 3.03 32  -1.815  0.1807\n Hist - Soc      2.67 3.03 32   0.880  0.6565\n Phys - Soc      8.17 3.03 32   2.696  0.0291\n\nResults are averaged over the levels of: Presentation \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\n\n38.7.1.1 SUMMARY\n\ntukeys suggests difference between Phys and Social t(30) = 2.71, p = .029\nPhys > Soc\n\n38.7.2 Post hoc analysis for Presentation\n\nemmeans(aov_model, spec = ~Presentation) %>% pairs(adjust = \"tukey\")\n\n contrast     estimate   SE df t.ratio p.value\n Comp - Stand     11.8 2.47 32   4.761  <.0001\n\nResults are averaged over the levels of: Lecture \n\n\n\n38.7.2.1 SUMMARY\n\ntukeys suggests difference between Comp and Stand\nComp > Stand\nt(30) = 4.792, p < .001"
  },
  {
    "objectID": "week10/10_3-advanced_example_and_apa.html#step-7-create-a-camera-ready-plot",
    "href": "week10/10_3-advanced_example_and_apa.html#step-7-create-a-camera-ready-plot",
    "title": "\n38  Template for Analysis & APA Style\n",
    "section": "\n38.8 Step 7: Create a camera ready plot",
    "text": "38.8 Step 7: Create a camera ready plot\nI really go into detail with this in the 3rd video (see CANVAS). For now, a few considerations:\n\n\npublication plots by default need to be in black/white/grayscale\nthe legend needs to fit within the axes of the plot\nwhen plotting multiple factors on a single plot, its useful to differentiate data series by shape, fill, color, or linetype depending on the plot. Sometimes its best to use these aesthetics in combination with one another.\nInclude a figure caption in the text below the plot output.\n\n\nFor example:\n\n# lecture on x-axis, score on y-axis, data groups by presentation:\nggplot(data = dataset, aes(x = Lecture, y = Score, group = Presentation)) +\n    # pointrange (mean ± se) with different shapes by presentation type:\n    stat_summary(fun.data = \"mean_se\",\n                 geom = \"pointrange\",\n                 aes(shape = Presentation),\n                 size = 1) +\n  # add lines (means), with different linetypes by presentation\n    stat_summary(fun = \"mean\",\n                 geom = \"line\",\n                 aes(linetype = Presentation)) +\n  # theme cowplot gets us a quick APA\n    theme_cowplot() +\n  # custom labels to make my axes convey more information\n  # if you con't include this the axis just contain the headers (Lecture, Presentation)\n    labs(y=\"Test Score\", x = \"Lecture Subject\") +\n  # adjusting the location of the legend:\n    theme(legend.position = c(.05,.9))\n\n\n\n\nFigure 1. Test scores as a function of subject of lecture and presentation type. Errors bars are standard error"
  },
  {
    "objectID": "week10/10_3-advanced_example_and_apa.html#step-8-write-ups",
    "href": "week10/10_3-advanced_example_and_apa.html#step-8-write-ups",
    "title": "\n38  Template for Analysis & APA Style\n",
    "section": "\n38.9 Step 8: write ups",
    "text": "38.9 Step 8: write ups\n\n\nyou can perform a write up in Word (or you word processor of choice)\n\nor you can do a write-up in Rstudio\n\nyou can use Markdown and MathJax (LaTeX) to do this.\n\n\n\n\nBefore starting, let’s get our summary statistics (mean and standard errors) to include in the write up.\nLecture:\n\nRmisc::summarySE(dataset, measurevar = \"Score\", groupvars = \"Lecture\")\n\n  Lecture  N    Score        sd       se       ci\n1    Hist 12 34.50000  8.393721 2.423058 5.333116\n2    Phys 12 40.00000 10.795622 3.116428 6.859211\n3     Soc 12 31.83333  9.311121 2.687889 5.916004\n\n\nPresentation\n\nRmisc::summarySE(dataset, measurevar = \"Score\", groupvars = \"Presentation\")\n\n  Presentation  N    Score       sd       se       ci\n1         Comp 18 41.33333 6.249706 1.473070 3.107906\n2        Stand 18 29.55556 9.438483 2.224672 4.693647\n\n\nTaking these summary statistics in addition to the ANOVA table summary I outlined above, I can perform a detailed write up. Be sure to include\n\nall tests run (ANOVA, post hocs, and their appropriate statistics\ninformation about group means and standard errors.\nexplicitly refer to the Figure.\nThis should follow a logical flow; in this case after presenting the general ANOVA, focus on post-hocs for one main effect and explain that info completely before moving onto the other.\n\nFor example:\n\nWe submitted our data to a 2 (Presentation) x 3 (Lecture) Analysis of Variance. Our ANOVA revealed a significant main effect for Presentation, \\(F\\)(1, 30) = 22.96, \\(p\\) < .001, \\(\\eta_p^2\\) = .43. Scores for groups given the computer presentation (\\(M ± SE\\): 41.33 ± 1.47) were greater than scores for the standard presentation (29.56 ± 2.22).Our ANOVA revealed a significant main effect for Lecture, \\(F\\)(2, 30) = 3.83, \\(p\\) = .03, \\(\\eta_p^2\\) = .20. Post hoc Tukey pairwise analysis revealed that the mean score of participants in the Physical (40.00 ± 3.12) group was greater than the score of the Social group (30.83 ± 2.69), \\(t\\)(30) = 2.71, \\(p\\) = .029. See Figure 1 for group comparisons."
  },
  {
    "objectID": "week12/12_1-afex_ANOVA.html#getting-the-data",
    "href": "week12/12_1-afex_ANOVA.html#getting-the-data",
    "title": "\n39  afex is for ANOVA\n",
    "section": "\n39.1 Getting the data:",
    "text": "39.1 Getting the data:\nTo start, let’s grab (and clean) some familiar data (this is from the Factorial ANOVA walkthrough). Note that I am cleaning up the numeric coding to reflect my factor levels\n\ndataset <- read_delim(\"https://www.uvm.edu/~statdhtx/methods8/DataFiles/Sec13-5.dat\", \"\\t\", escape_double = FALSE, trim_ws = TRUE)\n\nRows: 135 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\ndbl (4): Task, Smkgrp, score, covar\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndataset$Task <- recode_factor(dataset$Task, \"1\" = \"Pattern Recognition\", \"2\" = \"Cognitive\", \"3\" = \"Driving Simulation\") \ndataset$Smkgrp <- recode_factor(dataset$Smkgrp, \"1\" = \"Nonsmoking\", \"2\" = \"Delayed\", \"3\" = \"Active\")"
  },
  {
    "objectID": "week12/12_1-afex_ANOVA.html#hey-participant-you-got-any-id",
    "href": "week12/12_1-afex_ANOVA.html#hey-participant-you-got-any-id",
    "title": "\n39  afex is for ANOVA\n",
    "section": "\n39.2 Hey participant! You got any ID?!?",
    "text": "39.2 Hey participant! You got any ID?!?\nWhen I introduced ANOVA a few weeks back, you may remember that I stressed that it would be a good idea to ensure that your dataset contained a column that assigned each unique participant a unique ID. For the first few weeks of ANOVA, this was simply good practice, but ultimately not necessary. This is no longer the case. Moving forward, when running ANOVA that has a repeated measures / within-subjects design, you need to have a participant ID column, in order for R to match the appropriate data. That is if Johnny is has four scores in the data set (e.g., Johnny’s score on Monday, Tues, Thurs, and Fri) R uses the participant ID to match each of those scores with one another. This is because, when running a repeated measures design, the ANOVA needs to take into account within-subjects correlations of scores (i.e., Johnny’s scores are more tightly correlated with one another than they are with Jenny’s scores).\nFor now, we are just introducing afex::aov_ez() so we’ll simply create a column that assigns each row / score a unique participant ID. This would be consistent with the between-subjects designs we have encountered up to this point. However, note that if R finds instances where PartID are the same it will assume that data comes from the same participant (i.e., we have a within-subjects or repeated measures design). For our purposes now, this is easily solved by simply creating a column that runs from 1 to the number of observations in our data. After that we can proceed with the ANOVA.\n\n# create a PartID column and number, every score comes from a unique person\ndataset$PartID <- seq_along(dataset$score)"
  },
  {
    "objectID": "week12/12_1-afex_ANOVA.html#running-an-anova-the-aov_ez-way",
    "href": "week12/12_1-afex_ANOVA.html#running-an-anova-the-aov_ez-way",
    "title": "\n39  afex is for ANOVA\n",
    "section": "\n39.3 running an ANOVA, the aov_ez way",
    "text": "39.3 running an ANOVA, the aov_ez way\nBelow I spell out how to run an ANOVA using aov_ez(). Please note which need to be input as strings (i.e., have quotations around them), and which are input as objects or numbers.\n\naov_model <- afex::aov_ez(\n  id = \"PartID\", # column identifying your participants\n  dv = \"score\", # column that contains your dv\n  data = dataset, # your data frame\n  between = c(\"Smkgrp\",\"Task\") # column(s) containing your between IV(s)\n  # within = NULL, # column(s) containing your within IV(s)\n  # covariate = NULL, # column(s) containing your covariate(s)\n  # observed = NULL,\n  # fun_aggregate = NULL, \n  # type = 3, # type of Sum of Squares\n  # factorize = TRUE, \n  # check_contrasts = TRUE,\n  # return = \"afex_aov\",\n  # anova_table = list(es = \"pes\")\n  ) # adding partial eta squared effect size\n\nContrasts set to contr.sum for the following variables: Smkgrp, Task\n\n\nNote that above I specified every argument in the function. This was not necessary (indeed if you use Rstudio to help you fill this in, you’ll notice that many of the defaults are NULL). Briefly running down each of the arguments for aov_ez():\n\n\nid: column identifying your participants\n\ndv: column that contains your dv\n\ndata: your data frame\n\nbetween: column(s) containing your between IV(s)\n\nwithin: column(s) containing your within IV(s)\n\ncovariate: column(s) containing your covariate(s) for ANCOVA\n\nobserved: column of any variables are observed (i.e, measured) but not experimentally manipulated (typically NULL)\n\nfun_aggregate: function for aggregating the data before running the ANOVA, defaults to mean\n\ntype: type of Sum of Squares, see the Field text on when to change this value\n\nfactorize: should between subject factors be factorized before running the analysis. This is useful if your data is still numerically coded. This needs to be set to false if you have a covariate\n\ncheck_contrasts: should between-subject factors be checked and (if necessary) changed to be “contr.sum”, usually leave this as TRUE\n\n\nreturn: what kind of object do you want to return? e.g., lm, aov. I would recommend using the default afex_aov\n\n\nanova_table: list of further arguments passed to function producing the ANOVA table including what kind of effect size (partial v. general eta squared)? and any corrections (e.g., bonferroni).\n\nFor now I want to comment on a few of the choices I made. For type I selected Type III sum of squares. This is typically the type that we will choose in ANOVA, however see Field, Ch 11, “Jane Superbrain 11.1” for an excellent overview of the different Types of Sum of Squares and when to use different values. Also you’ll notice I made a few adjustments in my anova_table output via list(). These were to get the table to output partial eta-squared es=\"pes\" for my effect size.\nLet’s take at the aov_model we just created:\n\naov_model\n\nAnova Table (Type 3 tests)\n\nResponse: score\n       Effect     df    MSE          F  ges p.value\n1      Smkgrp 2, 126 107.83   8.41 *** .118   <.001\n2        Task 2, 126 107.83 132.90 *** .678   <.001\n3 Smkgrp:Task 4, 126 107.83     2.94 * .085    .023\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nWe can also get a more comprehensive look at our ANOVA table by:\n\naov_model %>% summary()\n\nAnova Table (Type 3 tests)\n\nResponse: score\n            num Df den Df    MSE        F     ges    Pr(>F)    \nSmkgrp           2    126 107.83   8.4098 0.11777  0.000373 ***\nTask             2    126 107.83 132.8954 0.67840 < 2.2e-16 ***\nSmkgrp:Task      4    126 107.83   2.9430 0.08545  0.022972 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n(This second way will factor more in our discussions of repeated measures ANOVA)\nThe output contains (from left to right) the effect tested, our degrees of freedom, the mean square error, F-value, partial eta squared (effect size) and p-value.\nLooking at the attributes of our model by typing attributes(aov_model) reveals that there are several $names associated with our aov_model. Each of these names change the format of the output of the model (“anova_table”, “aov”, “Anova”, “lm”) or let us peek back at the “data” in either $long or $wide format. For example, try running each of the following lines separately to see the output.\n\naov_model$anova_table\naov_model$Anova\naov_model$lm\naov_model$data\n\nFrom these options:\nI personally prefer $anova_table. As we’ll see when running repeated-measures / within-subjects ANOVA, $anova_table does an extra bit of work that will be useful to us.\n\naov_model$anova_table\n\nAnova Table (Type 3 tests)\n\nResponse: score\n            num Df den Df    MSE        F     ges    Pr(>F)    \nSmkgrp           2    126 107.83   8.4098 0.11777  0.000373 ***\nTask             2    126 107.83 132.8954 0.67840 < 2.2e-16 ***\nSmkgrp:Task      4    126 107.83   2.9430 0.08545  0.022972 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "week12/12_1-afex_ANOVA.html#running-your-assumptions-tests-using-performance",
    "href": "week12/12_1-afex_ANOVA.html#running-your-assumptions-tests-using-performance",
    "title": "\n39  afex is for ANOVA\n",
    "section": "\n39.4 Running your assumptions tests using performance",
    "text": "39.4 Running your assumptions tests using performance\nIn the past I would have spend time showing you how to extract residuals from the afex ANOVA in order to run you assumption checks. That said, since I’ve discovered the performance library earlier this year, I’m loving it more and more for handling these checks.\n\n39.4.1 normality using the performance package\nIn this case we can run a check of the normality assumption by taking our model performing the following steps\n\n39.4.1.1 visualization plots\nUsing performance We can produce two visualization plots to help us. First we can produce a desity plot (like a histogram) comparing the distribution of residuals to an ideal normal. This can be accomplished as follows:\n\nperformance::check_normality(aov_model) %>% plot()\n\n\n\n\nSecond we can produce a QQ plot using a similar call, but specify type=\"qq\":\n\n## QQ\nperformance::check_normality(aov_model) %>% plot(type = \"qq\")\n\n\n\n\n\n39.4.1.2 Shapiro-Wilkes\nConsidering both plots don’t look great, we can follow-up with a Shapiro Wilkes test. Same basic call, but instead of sending to a plot, we just print the output:\n\n# run norm check, must send to print\nperformance::check_normality(aov_model) %>% print()\n\nWarning: Non-normality of residuals detected (p < .001).\n\n\n\n39.4.2 normality bootstrapped\nIf we want to use the bootstrap method from my custom code, we can manually grab the residuals of our aov_model and send it to our function. To get the residuals we would need to type:\n\nresid(aov_model)\n\nFrom here we can submit these residuals to the appropriate methods for visual inspection and testing:\n\n# skew / kurtosis check:\nsource(\"https://raw.githubusercontent.com/tehrandavis/PSYC7014/master/custom_functions/skew_kurtosis_z.R\")\n\nresid(aov_model) %>% skew_kurtosis_z()\n\n       values crit_vals\nskew_z  -0.48      3.29\nkurt_z   2.93      3.29\n\n\n\n39.4.3 homogeneity assumption using performance\nI’m also loving performance since it doesn’t require us to rewrite the model for homogeneity checks (in years past this was a necessary step if using afex for ANOVA)\n\npacman::p_load(performance)\naov_model %>% performance::check_homogeneity()\n\nWarning: Variances differ between groups (Levene's Test, p = 0.000)."
  },
  {
    "objectID": "week12/12_1-afex_ANOVA.html#doing-more-with-afex",
    "href": "week12/12_1-afex_ANOVA.html#doing-more-with-afex",
    "title": "\n39  afex is for ANOVA\n",
    "section": "\n39.5 doing more with afex\n",
    "text": "39.5 doing more with afex\n\nThe afex package can be used for much more that simple running ANOVA. For example it’s got its own plotting functions and can even be used for multi-level modeling (although I typically on use it for quick and dirty modeling). Much of this is outside the scope of this class, but if you have time I certainly recommend you check out https://github.com/singmann/afex."
  },
  {
    "objectID": "week12/12_2-repeated_measures_anova.html#within-subject-v.-between-subjects",
    "href": "week12/12_2-repeated_measures_anova.html#within-subject-v.-between-subjects",
    "title": "\n40  Within Subjects / Repeated Measures ANOVA\n",
    "section": "\n40.1 Within-Subject v. Between Subjects",
    "text": "40.1 Within-Subject v. Between Subjects\nUp until now we have considered ANOVA in between subjects designs; when data at each level of each factor is from a different group of participants. This week we more onto within-subjects designs. As we mentioned in class we have a within-subject design whenever data from the same participants exists on at least two levels of a factor (or analogously occupied at least two cells within our interaction matrix).\nTo contrast some important distinctions let’s revisit our familiar data set contrasting test outcomes for students as a function of Lecture. I realize before, Lecture was crossed with at least one other factor, but for the sake on simplicity let’s just consider data from this single factor. The goal of this first section is to contrast results as a function whether this data is considered within-subjects or between-subjects.\nThis walkthough assumes you have the following packages:\n\npacman::p_load(tidyverse,\n               afex,\n               cowplot,\n               emmeans)\n\nOkay, not let’s load in some data:\n\nwithin_between <- read_csv(\"https://raw.githubusercontent.com/tehrandavis/graduate_statistics/main/practice_datasets/within_between.csv\")\n\nRows: 36 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Lecture\ndbl (3): BetweenSubjects_ID, WithinSubjects_ID, Score\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nwithin_between\n\n# A tibble: 36 × 4\n   BetweenSubjects_ID WithinSubjects_ID Lecture  Score\n                <dbl>             <dbl> <chr>    <dbl>\n 1                  1                 1 Physical    53\n 2                  2                 2 Physical    49\n 3                  3                 3 Physical    47\n 4                  4                 4 Physical    42\n 5                  5                 5 Physical    51\n 6                  6                 6 Physical    34\n 7                  7                 7 Physical    44\n 8                  8                 8 Physical    48\n 9                  9                 9 Physical    35\n10                 10                10 Physical    18\n# ℹ 26 more rows\n\n\n\n40.1.1 Within v Between ANOVA\nSo we have our dataset within_between. You’ll note that there are two subjects columns WithinSubjects which imagines 12 participants each going through all 3 Lecture types and BetweenSubjects where each participant (N=36) is assigned to a single Lecture type. Previously, we might have treated this as a between subjects design. Looking at the ezDesign of this design we see that every BetweenSubject is assigned to a single condition (as evidenced by count = 1)\n\nez::ezDesign(within_between,y=Lecture,x=BetweenSubjects_ID)\n\n\n\n\nSkipping past the preliminaries (e.g., testing for assumptions) and straight to running the BS ANOVA:\n\nbetween_aov <- lm(Score~Lecture, data = within_between)\nsjstats::anova_stats(between_aov)[1:8]\n\nterm      | df | sumsq |  meansq | statistic | p.value | etasq | partial.etasq\n------------------------------------------------------------------------------\nLecture   |  2 |  1194 | 597.000 |     4.284 |   0.022 | 0.206 |         0.206\nResiduals | 33 |  4599 | 139.364 |           |         |       |              \n\n\nHowever, let’s assume instead that this data set comes a within design. That is, instead of different participants in each Lecture group, the same group of people went through all three lectures:\n\nez::ezDesign(within_between,y=Lecture,x=WithinSubjects_ID)\n\n\n\n\nWe see that the ezDesign has changed. Instead of 36 participants each individually assigned to a single condition, we have 12 participants each assigned to all three conditions for a single trial (measure). As mentioned in class, conceptually, a within subjects design considers participant as a pseudo factor with each individual participant as a level. To fully capture this we should factorize our WithinSubjects_ID column.\nRunning the within-subjects ANOVA:\n\n\nterm                      | df |    sumsq |  meansq | statistic | p.value | etasq | partial.etasq\n-------------------------------------------------------------------------------------------------\nLecture                   |  2 | 1194.000 | 597.000 |    12.305 |  < .001 | 0.206 |         0.528\nfactor(WithinSubjects_ID) | 11 | 3531.667 | 321.061 |     6.618 |  < .001 | 0.610 |         0.768\nResiduals                 | 22 | 1067.333 |  48.515 |           |         |       |              \n\n\nBefore continuing on, I want you to take a moment and think about what the specification of the formula in the lm() model above actually means. Not only are we using Lecture as a predictor, but we are also treating participant WithinSubjects_ID as a factor as well. That is not only is the model accounting for the variation moving between levels of Lecture, but also the model is accounting for variability moving between each participant. If you want to continue to use the lm() method for model building, you will need to include participant as a factor in the model. However, I would not recommend continuing with lm(), especially as you end up with more complex within-subjects designs. In your future, you will probably elect to run either a mixed effects model using lmer or a repeated measures ANOVA. Mixed effect modeling is outside of the scope of what we cover this semester (although we’ve laid some foundations for it). As for repeated measures ANOVA, I would recommend moving onto a new method for running this in R. In the previous walkthrough I introduced aov_ez() from the afex library.\nThis might be a good time to start running ANOVA using afex::aov_ez(). Let’s re-run both of our models using afex::ez():\n\nbetween_aov <- afex::aov_ez(id = \"BetweenSubjects_ID\", # 36 people in 3 groups\n                           dv = \"Score\", \n                           data = within_between,\n                           between = \"Lecture\", # Lecture as between predictor\n                           within = NULL,\n                           type = 3, \n                           return = \"afex_aov\", \n                           anova_table=list(es = \"pes\") # partial eta sq.\n                           )\n\nConverting to factor: Lecture\n\n\nContrasts set to contr.sum for the following variables: Lecture\n\nwithin_aov <- afex::aov_ez(id = \"WithinSubjects_ID\", # 12 people going through 3 conditions\n                           dv = \"Score\", \n                           data = within_between,\n                           between = NULL,\n                           within = \"Lecture\", # Lecture as within predictor\n                           type = 3, \n                           return = \"afex_aov\", \n                           anova_table=list(es = \"pes\") # partial eta sq.\n                           )\n\n\n40.1.2 Comparing BS v WS ANOVA\nOkay, returning to our model outcomes. Keep in mind in both cases we are using the exact same scores.\nHere is our between ANOVA:\n\nbetween_aov\n\nAnova Table (Type 3 tests)\n\nResponse: Score\n   Effect    df    MSE      F  pes p.value\n1 Lecture 2, 33 139.36 4.28 * .206    .022\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nAnd here is our within ANOVA\n\nwithin_aov\n\nAnova Table (Type 3 tests)\n\nResponse: Score\n   Effect          df   MSE        F  pes p.value\n1 Lecture 1.26, 13.88 76.88 12.31 ** .528    .002\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\nSphericity correction method: GG \n\n\nIn both of these cases the data is exactly the same. What has changed is how we parse the variance (you’ll notice that the denominator degrees of freedom are different for the second ANOVA). In a within design, we need to take into account the “within-subject” variance. That is how individual subjects vary from one level of treatment to the other. In this respect, within designs are typically more powerful than analogous between designs. While the inherent differences between individual subjects is present in both types of designs, your within-subjects ANOVA model includes it in its analysis. In the present example, this increase in power is reflected by the lower MSE (48.52 v. 139.36) and subsequently, larger F-value (12.31 v. 4.28) and effect size (0.53 v. 0.21) in our within-subjects analysis.\nWell if that’s the case why not run within-subject (WS) designs all of the time. Well, typically psychologists do when the subject lends itself to WS-designs. BUT there are certainly times when they are not practical, for example, if you are concerned about learning, practice, or carryover effects where exposure to a treatment on one level might impact the other levels—if you were studying radiation poisoning and had a placebo v. radiation dose condition, it be likely that you wouldn’t run your experiment as a within—or at the very least you wouldn’t give them the radiation first. It would also be likely that you’d be in violation several standards of ethics.\n(You may also note that a correction is taking place on within_aov giving us those odd degrees of freedom. We’ll talk more about that in a bit.)"
  },
  {
    "objectID": "week12/12_2-repeated_measures_anova.html#a-note-on-univariate-v.-multivariate-methods",
    "href": "week12/12_2-repeated_measures_anova.html#a-note-on-univariate-v.-multivariate-methods",
    "title": "\n40  Within Subjects / Repeated Measures ANOVA\n",
    "section": "\n40.2 a note on Univariate v. Multivariate methods",
    "text": "40.2 a note on Univariate v. Multivariate methods\nUnivariate v. multivariate statistical methods refer to the number of dependent variables associated with the model per fundamental unit of analysis. In psychology research the fundamental unit of analysis is typically the participant—measures are taken at the level of the participant. A univariate model has one dependent variable whereas a multivariate method has two or more per participant. Perhaps some of you are more familiar with this distinction in the context on One-way MANOVA… which analyzes two or more dependent variables simultaneously in an ANOVA design. MANOVA (multivariate ANOVA) is preferred to running seperate univariate ANOVAs if one believes that the dependent variables involved are somehow correlated with one another. In this case MANOVA factors these correlations into the model testing the IVs.\nI bring all of this up as one way to conceptualize repeated-measures ANOVA is as multivariate method. Multiple measures are taken at the level of the participant. This will factor into our discussion of follow-up analyses."
  },
  {
    "objectID": "week12/12_2-repeated_measures_anova.html#a-note-on-sphericity-for-repeated-measures-anova",
    "href": "week12/12_2-repeated_measures_anova.html#a-note-on-sphericity-for-repeated-measures-anova",
    "title": "\n40  Within Subjects / Repeated Measures ANOVA\n",
    "section": "\n40.3 a note on sphericity for Repeated measures ANOVA",
    "text": "40.3 a note on sphericity for Repeated measures ANOVA\nIn addition to the assumptions that we are familiar with, repeated-measures ANOVA has the additional assumption of Spherecity of Variance / Co-variance. We talked at length in class re: Spherecity of Variance / Co-variance so I suggest revisiting the slides for our example. At the same time there are debates as to the importance of sphericity in the subjects data. One alternative method that avoids these issues is to invoke mixed models (e.g., lmer). However, if you really want to go down the rabbit hole check out Doug Bates reponse on appropriate dfs and p-values in lmer. You’ll note that these discussions were ten years ago and are still being debated (see here. That said, you may be seeing mixed models in your near future (i.e., next semester)\nFor now, we won’t go down the rabbit hole and just focus on the practical issues confronted when running a repeated-measures ANOVA."
  },
  {
    "objectID": "week12/12_2-repeated_measures_anova.html#example-1",
    "href": "week12/12_2-repeated_measures_anova.html#example-1",
    "title": "\n40  Within Subjects / Repeated Measures ANOVA\n",
    "section": "\n40.4 EXAMPLE 1",
    "text": "40.4 EXAMPLE 1\nTo start, we will use data related to the effectiveness of relaxation therapy to the number of episodes that chronic migraine sufferers reported. Data was collected from each subject over the course of 5 weeks. After week 2 the therapy treatment was implemented effectively dividing the 5 weeks into two phases, Pre (Weeks 1 & 2) and Post (Weeks 3,4, & 5).\n\n40.4.1 loading in the data:\n\nexample1 <- read_delim(\"https://www.uvm.edu/~statdhtx/methods8/DataFiles/Tab14-3.dat\", \n                       delim = \"\\t\")\n\nRows: 9 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\ndbl (6): Subject, Wk1, Wk2, Wk3, Wk4, Wk5\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nexample1\n\n# A tibble: 9 × 6\n  Subject   Wk1   Wk2   Wk3   Wk4   Wk5\n    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1       1    21    22     8     6     6\n2       2    20    19    10     4     4\n3       3    17    15     5     4     5\n4       4    25    30    13    12    17\n5       5    30    27    13     8     6\n6       6    19    27     8     7     4\n7       7    26    16     5     2     5\n8       8    17    18     8     1     5\n9       9    26    24    14     8     9\n\n\nYou’ll notice that the data set above is in wide format. Each subject is on a single row and each week is in its own column. Note that this is the preferred format for within subjects analysis for SPSS. However in R we want it in long format.\n\nexample1_long <- pivot_longer(example1,\n                              cols = -Subject,\n                              names_to = \"Week\", \n                              values_to = \"Migraines\")\n\nexample1_long$Week <- as.factor(example1_long$Week)\nexample1_long$Subject <- as.factor(example1_long$Subject)\n\nexample1_long\n\n# A tibble: 45 × 3\n   Subject Week  Migraines\n   <fct>   <fct>     <dbl>\n 1 1       Wk1          21\n 2 1       Wk2          22\n 3 1       Wk3           8\n 4 1       Wk4           6\n 5 1       Wk5           6\n 6 2       Wk1          20\n 7 2       Wk2          19\n 8 2       Wk3          10\n 9 2       Wk4           4\n10 2       Wk5           4\n# ℹ 35 more rows\n\n\nOk, much better, each Subject × Week observation is on a single row.\n\n40.4.2 plotting the data\nIn addition to plotting your means, one crucial step is to plot the changes in the within-subject (repeated measures) variable at the participant-level. This is especially useful for discerning whether the pattern of results is roughly similar for all participants -OR- if, instead there is large individual variability in the direction of the effect. In other words, “is your manipulation having roughly the same effect for every participant OR is the effect drastically different for participants.\nThere are 2 ways to do this: 1. the Facet plot or 2. Spaghetti Plot.\n\n40.4.3 the Facet plot\nEach window / facet is for a single subject (S1-S9):\n\nggplot(data = example1_long, aes(x=Week, y=Migraines, group = 1)) +\n  geom_point() + \n  geom_line() + \n  facet_wrap(~Subject, ncol = 3)\n\n\n\n\n\n40.4.4 the Spaghetti Plot\nIn this case each line represents an individual Subject (1-9):\n\nggplot(data = example1_long, aes(x=Week, y=Migraines, group = Subject)) +\n  geom_point(aes(col=Subject)) + \n  geom_line(aes(col=Subject))\n\n\n\n\nWhich you choose is ultimately up to you. I tend to use Spaghetti plots unless I have instances where I have a high number of participants. This can potentially make the spaghetti plot busy (a bunch of points and line). That said, the purpose of the Spaghetti Plot is to note inconsistencies in trends. A helpful tool for a potentially busy Spaghetti Plot is to use plotly. This produces an interactive plot where you can use the mouse cursor to identify important information:\n\nlibrary(plotly) # may need to install\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\ns_plot <- ggplot(data = example1_long, aes(x=Week, y=Migraines, group = Subject)) +\n  geom_point(aes(col=Subject)) + \n  geom_line(aes(col=Subject))\n\nplotly::ggplotly(s_plot)\n\n\n\n\n\nTry hovering over individual points and lines in the plot above. Also, see what happens when you click on one of the participant numbers in the legend.\n\n40.4.5 Means Plots\nOne additional concern that we must deal with when plotting within-subjects data is the error bars. Plotting the standard error or regular confidence intervals may be misleading for making statistical inferences. This is because the values when normally calculated do not account for within subject correlation. Luckily for us there is a correction that we can make using the Rmisc package. Please see Cousineau (2005) and Morey (2008) for details on this issue and subsequent correction.\nThe practical steps for this correction includes first norming the data to account for within subjects correlations.\n\nnormedData <- Rmisc::normDataWithin(data = example1_long,\n                                    measurevar = \"Migraines\",\n                                    idvar = \"Subject\", \n                                    betweenvars = NULL)\n\nnormedData\n\n   Subject Week Migraines MigrainesNormed\n1        1  Wk1        21       21.644444\n2        1  Wk2        22       22.644444\n3        1  Wk3         8        8.644444\n4        1  Wk4         6        6.644444\n5        1  Wk5         6        6.644444\n6        2  Wk1        20       21.844444\n7        2  Wk2        19       20.844444\n8        2  Wk3        10       11.844444\n9        2  Wk4         4        5.844444\n10       2  Wk5         4        5.844444\n11       3  Wk1        17       21.044444\n12       3  Wk2        15       19.044444\n13       3  Wk3         5        9.044444\n14       3  Wk4         4        8.044444\n15       3  Wk5         5        9.044444\n16       4  Wk1        25       18.844444\n17       4  Wk2        30       23.844444\n18       4  Wk3        13        6.844444\n19       4  Wk4        12        5.844444\n20       4  Wk5        17       10.844444\n21       5  Wk1        30       26.444444\n22       5  Wk2        27       23.444444\n23       5  Wk3        13        9.444444\n24       5  Wk4         8        4.444444\n25       5  Wk5         6        2.444444\n26       6  Wk1        19       19.244444\n27       6  Wk2        27       27.244444\n28       6  Wk3         8        8.244444\n29       6  Wk4         7        7.244444\n30       6  Wk5         4        4.244444\n31       7  Wk1        26       28.444444\n32       7  Wk2        16       18.444444\n33       7  Wk3         5        7.444444\n34       7  Wk4         2        4.444444\n35       7  Wk5         5        7.444444\n36       8  Wk1        17       20.444444\n37       8  Wk2        18       21.444444\n38       8  Wk3         8       11.444444\n39       8  Wk4         1        4.444444\n40       8  Wk5         5        8.444444\n41       9  Wk1        26       23.044444\n42       9  Wk2        24       21.044444\n43       9  Wk3        14       11.044444\n44       9  Wk4         8        5.044444\n45       9  Wk5         9        6.044444\n\n\nWe then calculate the se, sd, and ci values of our normed data, in this case MigrainesNormed.\nThe final step is to make a correction on these Normed values (Morey (2008)). This is done by taking the number of levels of the within factor (nWithinGroups) and applying the following correction:\n\n# get the number of levels in Week:\nnWithinGroups <- nlevels(example1_long$Week)\n\n# apply the correction factor:\ncorrectionFactor <- sqrt(nWithinGroups/(nWithinGroups - 1))\n\nThe range of our corrected errorbars are the sd, se, ci multiplied by this correctionFactor.\nFor example, the sd of participants’ Migraines in Wk1 is:\n\nsd(normedData$MigrainesNormed[normedData$Week==\"Wk1\"])*correctionFactor\n\n[1] 3.588369\n\n\nFortunately there is a function in Rmisc that handles this correction for us, summarySEwithin. It is very similar to the summarySE function you are familiar with, but asks you to specify which IVs are within-subjects (withinvars), between-subjects(betweenvars) and which column contains subject IDs idvar. Using our original example1_long data:\n\nRmisc::summarySEwithin(data=example1_long,\n                       measurevar = \"Migraines\",\n                       withinvars = \"Week\",\n                       betweenvars = NULL, \n                       idvar = \"Subject\")\n\n  Week N Migraines       sd        se       ci\n1  Wk1 9 22.333333 3.588369 1.1961229 2.758264\n2  Wk2 9 22.000000 2.993280 0.9977598 2.300838\n3  Wk3 9  9.333333 1.984663 0.6615545 1.525547\n4  Wk4 9  5.777778 1.474788 0.4915960 1.133623\n5  Wk5 9  6.777778 2.837252 0.9457507 2.180905\n\n\nUnfortunately, to date I haven’t found a way to get this to play nice with stat_summary() in ggplot(). HOWEVER, there is a simple work around. Since stat_summary() is simply summarizing our means and error values from the dataset and summarySEwithin is doing the exact same thing, we can simply pull these values straight from summarySEwithin with one MAJOR caveat. summarySEwithin reports the normed means and errors, however we need the original means. Note that this is not an issue when you are only dealing with within subjects factors, but if you are performing mixed ANOVA (combination within-subjects and between-subjects) these means can differ.\nTo address this problem user Hause Lin created a custom function summarySEwithin2 that reports both normed and unnormed means. You can find this script on their Github site here. I would recommend copying and pasting the code you your own “.R” file for future use. In the meantime we can directly source this code from their site:\n\nsource(\"https://gist.githubusercontent.com/hauselin/a83b6d2f05b0c90c0428017455f73744/raw/38e03ea4bf658d913cf11f4f1c18a1c328265a71/summarySEwithin2.R\")\n\nA similar script may be found on this ggplot tutorial site, which forms the basis of this alternative plotting method.\nNote that this “alternative” is how I in fact create most of my own ANOVA plots. But, I have my own custom function that is an adaptation of summarySEwithin2, I call it withinSummary. This is to address the fact that when writing your data up you need to use not only the actual means but also the actual se / sd / ci.\n\nsource(\"https://raw.githubusercontent.com/tehrandavis/graduate_statistics/main/custom_functions/withinSummary.R\")\n\nThis gives me both the $Actual and $Corrected values. I can use the descriptiveStats$Corrected data for plotting, for example: First we save the output of withinSummary to an object:\n\ndescriptiveStats <- withinSummary(data=example1_long,\n                                  measurevar = \"Migraines\",\n                                  withinvars = \"Week\",\n                                  idvar = \"Subject\")\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\ndescriptiveStats\n\n$Actual\n  Week N Migraines       sd       se       ci\n1  Wk1 9 22.333333 4.582576 1.527525 3.522480\n2  Wk2 9 22.000000 5.338539 1.779513 4.103564\n3  Wk3 9  9.333333 3.391165 1.130388 2.606680\n4  Wk4 9  5.777778 3.419714 1.139905 2.628625\n5  Wk5 9  6.777778 4.116363 1.372121 3.164117\n\n$Corrected\n  Week N Migraines MigrainesNormed       sd        se       ci\n1  Wk1 9 22.333333       22.333333 3.588369 1.1961229 2.758264\n2  Wk2 9 22.000000       22.000000 2.993280 0.9977598 2.300838\n3  Wk3 9  9.333333        9.333333 1.984663 0.6615545 1.525547\n4  Wk4 9  5.777778        5.777778 1.474788 0.4915960 1.133623\n5  Wk5 9  6.777778        6.777778 2.837252 0.9457507 2.180905\n\n\nFrom here we can refer directly to descriptiveStats in constructing the ggplot. When plotting you use the Corrected values .\n\np <- ggplot(descriptiveStats$Corrected, # using the corrected table to make the plots\n            aes(x = Week,\n                y = Migraines, \n                group=1))\n\nAnd now to construct the plot. rather than using summary_stat() we directly call each geom. For example, adding the means and as points:\n\np <- p + geom_point(size = 2)\np\n\n\n\n\nConnecting those points with lines:\n\np <- p + geom_line()\np\n\n\n\n\nAdding error bars (SE):\n\np <- p + geom_errorbar(aes(ymin=Migraines-se, ymax=Migraines+se), width=0)\nshow(p)\n\n\n\n\nnote that above I set width of the error bars to 0 to keep consistent with how we’ve been plotting pointranges. However, if you want caps, you can change the width. I recommend a width no higher than 0.2.\nFrom here you can use your familiar commands to whip it into APA format!\n\n40.4.6 building the model using afex:\nRunning a WS ANOVA is just like running a BS ANOVA in afex (see the last walkthrough). We call in our within subjects factors using the within= argument.\n\nwithin_aov <- afex::aov_ez(id = \"Subject\", \n                           dv = \"Migraines\", \n                           data = example1_long,\n                           between = NULL,\n                           within = \"Week\",\n                           type = 3, \n                           return = \"afex_aov\", \n                           anova_table=list(es = \"pes\")\n                           )\n\n\n40.4.7 assumptions checks: normality\nWe can then use the model, within_aov to run the requisite assumption checks. As before, the model plays nicely with performance, but the blanket check_model will not work with afex. Instead we can do a piecemeal check for normality along with visualizations.\n\n# visualization against histogram\nwithin_aov %>% performance::check_normality() %>% plot()\n\n\n\n# qq visualization\nwithin_aov %>% performance::check_normality() %>% plot(\"qq\")\n\n\n\n# shapiro-wilkes test\nwithin_aov %>% performance::check_normality() %>% show()\n\nOK: residuals appear as normally distributed (p = 0.206).\n\n\n\n40.4.8 assumptions checks: sphericity\nJust like the paired \\(t\\) test from a few weeks back, the RM-ANOVA does not have an explicit check for homogeneity of variance. Instead, RM ANOVA introduces a new test of assumptions that you must run, the test of Sphericity. Indeed, this test is run instead of the check for homogeneity. This is because technically a WS ANOVA violates the assumption of independence of scores, and thus inflates the likelihood of a homogeneity of variances violation. Essentially, we give up on trying to hold onto this assumption and instead direct our interests towards maintaining Sphericity. Importantly, if we do violate sphericity, there is a clean and relatively agreed upon step that we must take with our analysis.\nThere are two ways to test for sphericity, (1) simply using and adjusting based upon the summary output of our ANOVA model, or (2) we can again use the performance library. I typically just use method 1, so that’s all I will show you here to avoid confusion.\nThere is an argument to be made that you should always make a correction to guard against deviations from sphericity (see below). The correction becomes larger the further your data is from sphericity. However, standard practice in the psychology literature is to only apply the correction if our data fail the Mauchly’s Test (p < .05) link. The outcome of this test can be obtained by using the summary() function on our model.\n\nsummary(within_aov)\n\nWarning in summary.Anova.mlm(object$Anova, multivariate = FALSE): HF eps > 1\ntreated as 1\n\n\n\nUnivariate Type III Repeated-Measures ANOVA Assuming Sphericity\n\n            Sum Sq num Df Error SS den Df F value    Pr(>F)    \n(Intercept) 7893.7      1   486.71      8 129.747 3.186e-06 ***\nWeek        2449.2      4   230.40     32  85.042 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n     Test statistic p-value\nWeek        0.28236 0.53699\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n      GG eps Pr(>F[GG])    \nWeek 0.68446  5.773e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n       HF eps  Pr(>F[HF])\nWeek 1.075624 1.39444e-16\n\n\nThis gives us our original ANOVA table with non-corrected degrees of freedom, the Mauchly Test, and the Greenhouse-Geisser and Huynh-Feldt corrections. The Mauchly Test is our test for Sphericity. In this case the Mauchly Test p = .54, so no corrections are necessary. If your elect to make the correction (i.e., the data fails Mauchly’s Test) then you could multiply your original degrees of freedom by the GG eps (or HF eps). The p-value next to the correction values tells you the p-value for your ANOVA effect assuming corrections. In this case even with the corrections the effect of Week is significant.\nIf you want to get the corrected degrees of freedom, you can simply take a look at the model itself:\n\nwithin_aov\n\nAnova Table (Type 3 tests)\n\nResponse: Migraines\n  Effect          df   MSE         F  pes p.value\n1   Week 2.74, 21.90 10.52 85.04 *** .914   <.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\nSphericity correction method: GG \n\n\n* note that the output includes the sphericity correction: GG.\nAlternatively you could just rerun the afex function and specifying the correction= call. You can either specify the “GG” correction:\n\n## GG correction:\nwithin_aov_gg <- afex::aov_ez(id = \"Subject\", \n                           dv = \"Migraines\", \n                           data = example1_long,\n                           between = NULL,\n                           within = \"Week\",\n                           type = 3, \n                           return = \"afex_aov\", \n                           anova_table=list(es = \"pes\",\n                                            correction=\"GG\"))\n\nwithin_aov_gg\n\nAnova Table (Type 3 tests)\n\nResponse: Migraines\n  Effect          df   MSE         F  pes p.value\n1   Week 2.74, 21.90 10.52 85.04 *** .914   <.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\nSphericity correction method: GG \n\n\nor the “HF” correction:\n\n## HF correction (note that any HF eps > 1 will be treated as 1):\nwithin_aov_hf <- afex::aov_ez(id = \"Subject\", \n                           dv = \"Migraines\", \n                           data = example1_long,\n                           between = NULL,\n                           within = \"Week\",\n                           type = 3, \n                           return = \"afex_aov\", \n                           anova_table=list(es = \"pes\", \n                                            correction=\"HF\"))\n\nWarning: HF eps > 1 treated as 1\n\nwithin_aov_hf\n\nAnova Table (Type 3 tests)\n\nResponse: Migraines\n  Effect    df  MSE         F  pes p.value\n1   Week 4, 32 7.20 85.04 *** .914   <.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\nSphericity correction method: HF \n\n\n\n40.4.9 running post-hoc analyses and follow-ups\nPost-hoc comparisons of means take a different form for repeated measures ANOVA. Typical methods such as Tukey HSD were designed for between-subjects effects, where it makes sense (assuming homogeneity of variance) to use a pooled error term. However, for within-subjects (or repeated measures) effects, the error term is the Treatment x Subjects interaction, and the nature of the TxS interaction across all treatment levels can be very different than it is for any particular pair of treatment levels.\nThe typical recommendation (more detail on this in the following walkthrough) is to carry out pair-wise contrasts for a within-subjects factor using ordinary paired tests with an error term based only on the levels being compared .\nBecause of this, post-hoc analysis involving repeated measures requires an additional argument in emmeans(); invoking model=\"multivariate\"\n\nemmeans(within_aov, specs = pairwise~Week, model=\"multivariate\")\n\n$emmeans\n Week emmean   SE df lower.CL upper.CL\n Wk1   22.33 1.53  8    18.81    25.86\n Wk2   22.00 1.78  8    17.90    26.10\n Wk3    9.33 1.13  8     6.73    11.94\n Wk4    5.78 1.14  8     3.15     8.41\n Wk5    6.78 1.37  8     3.61     9.94\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast  estimate    SE df t.ratio p.value\n Wk1 - Wk2    0.333 1.700  8   0.196  0.9996\n Wk1 - Wk3   13.000 1.247  8  10.423  <.0001\n Wk1 - Wk4   16.556 1.375  8  12.036  <.0001\n Wk1 - Wk5   15.556 1.608  8   9.672  0.0001\n Wk2 - Wk3   12.667 1.179  8  10.748  <.0001\n Wk2 - Wk4   16.222 0.909  8  17.837  <.0001\n Wk2 - Wk5   15.222 1.441  8  10.562  <.0001\n Wk3 - Wk4    3.556 0.818  8   4.345  0.0154\n Wk3 - Wk5    2.556 1.156  8   2.211  0.2663\n Wk4 - Wk5   -1.000 0.882  8  -1.134  0.7857\n\nP value adjustment: tukey method for comparing a family of 5 estimates \n\n\nNote that the df (df = 8) of these tests are NOT the error df (df = 22) from the omnibus ANOVA.\n\n40.4.10 Example 1 write up\n\n# adding to the plot we created above\n\np + theme_cowplot()\n\n\n\n\nFigure 1. Number of migraines as a function of Week. Note that Weeks 1 and 2 are baseline while Weeks 3, 4, and 5 are post relaxation therapy treatment.\n\nTo test whether relaxation therapy had a positive effect for migraine suffers, we analyzed the number of migraines each individual reported over a 5 week period. The first two weeks were used as a baseline to establish a typical week for each person. The remaining three weeks each person was lead through relaxation therapy techniques. These data were submitted to a within-subjects ANOVA with Week as a factor.\nPost hoc comparisions demonstrated the effectiveness of our treatment. While the number of migraines remained indifferent during the first two weeks (TukeyHSD, p >.05) there was a significant decrease in the number of migraines post treatment. As seen in Figure 1, the number of migraines in Weeks 3, 4, 5 we significantly lower than Weeks 1 and 2 (ps < .05).\n\n** note that you don’t have to worry about the sphericity violation as it relates to the post hoc analyses—as you are only comparing two levels."
  },
  {
    "objectID": "week12/12_2-repeated_measures_anova.html#example-2",
    "href": "week12/12_2-repeated_measures_anova.html#example-2",
    "title": "\n40  Within Subjects / Repeated Measures ANOVA\n",
    "section": "\n40.5 EXAMPLE 2",
    "text": "40.5 EXAMPLE 2\nLet’s take a look at another example, using a experimental paradigm we are familiar with, scores as a function of lecture type.\n\n40.5.1 loading in the data\nYou note that this data is already in long format so no need to adjust.\n\nexample2_long <- read_csv(\"https://raw.githubusercontent.com/tehrandavis/graduate_statistics/main/practice_datasets/withinEx2.csv\")\n\nRows: 36 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Lecture\ndbl (2): Subject, Score\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nexample2_long\n\n# A tibble: 36 × 3\n   Subject Lecture  Score\n     <dbl> <chr>    <dbl>\n 1       1 Physical    53\n 2       2 Physical    49\n 3       3 Physical    47\n 4       4 Physical    42\n 5       5 Physical    51\n 6       6 Physical    34\n 7       7 Physical    44\n 8       8 Physical    48\n 9       9 Physical    35\n10      10 Physical    18\n# ℹ 26 more rows\n\n\n\n40.5.2 plotting the data\nLet’s plot this data. Below, I put together the code for the means plot. But why don’t you try to create a Spaghetti plot.\nThis time for the means plot I’ll make my points a little larger, lines a little thicker, and add caps to the error bars:\n\ndescriptiveStats <- withinSummary(data=example2_long,measurevar = \"Score\",withinvars = \"Lecture\",idvar = \"Subject\")\n\nAutomatically converting the following non-factors to factors: Lecture\n\np <- ggplot(descriptiveStats$Corrected,mapping = aes(x = Lecture,y = Score, group=1)) +\n  geom_point(size=3) + \n  geom_line(size=2) +\n  geom_errorbar(aes(ymin=Score-se, ymax=Score+se), width=0.15) +\n  theme_cowplot() +\n  theme(\n    axis.title = element_text(size = 16, face = \"bold\", lineheight = .55),\n    axis.text = element_text(size = 12),\n    legend.title = element_text(size = 12, face = \"bold\"),\n    legend.position = c(.25,.25)) +\n  scale_color_manual(values=c(\"black\",\"grey50\")) + \n  xlab(\"Lecture\") + \n  ylab (\"Score\") +\n  theme(plot.margin=unit(c(.25,.25,.25,.25),\"in\")) + \n  \n  # stack legend boxes horizontally:\n  theme(legend.box = \"horizontal\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nshow(p)\n\n\n\n\nMaybe not the best plot, but wanted to show you how to tweak things.\n\n40.5.3 running the ANOVA:\nAs before, let’s run this using afex:\n\nwithin_aov <- afex::aov_ez(\n  id = \"Subject\", \n  dv = \"Score\", \n  data = example2_long,\n  between = NULL,\n  within = \"Lecture\",\n  type = 3, \n  return = \"afex_aov\", \n  anova_table=list(es = \"pes\", correction=\"none\"))\n\nsummary(within_aov)\n\n\nUnivariate Type III Repeated-Measures ANOVA Assuming Sphericity\n\n            Sum Sq num Df Error SS den Df F value    Pr(>F)    \n(Intercept)  40401      1   3531.7     11 125.836 2.319e-07 ***\nLecture       1194      2   1067.3     22  12.305  0.000259 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n        Test statistic  p-value\nLecture        0.41524 0.012345\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n         GG eps Pr(>F[GG])   \nLecture 0.63101    0.00225 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n           HF eps Pr(>F[HF])\nLecture 0.6748954 0.00173583\n\n\nYou’ll notice that in this example the data failed the Mauchly Test for Sphericity (\\(p\\) = .0123). As I mentioned above in this case you’ll need to make the appropriate corrections. GG corrections are the “industry standard” (you typically see these in psych literature). HF corrections are not as conservative, and are appropriate in instances where the GG eps > 0.75. In this case we’ll use the GG correction. My advice, just rerun the ANOVA with correction=\"GG\":\n\nwithin_aov <- afex::aov_ez(\n  id = \"Subject\", \n  dv = \"Score\", \n  data = example2_long,\n  between = NULL,\n  within = \"Lecture\",\n  type = 3, \n  return = \"afex_aov\", \n  anova_table=list(es = \"pes\", \n                   correction=\"GG\",\n                   sig_symbols = rep(\"\", 4)\n                   )\n  )\n\n\nsummary(within_aov)\n\n\nUnivariate Type III Repeated-Measures ANOVA Assuming Sphericity\n\n            Sum Sq num Df Error SS den Df F value    Pr(>F)    \n(Intercept)  40401      1   3531.7     11 125.836 2.319e-07 ***\nLecture       1194      2   1067.3     22  12.305  0.000259 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n        Test statistic  p-value\nLecture        0.41524 0.012345\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n         GG eps Pr(>F[GG])   \nLecture 0.63101    0.00225 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n           HF eps Pr(>F[HF])\nLecture 0.6748954 0.00173583\n\n\n\n40.5.4 post-hocs and follow-ups\nRemember again, that any simple effects / post hoc comparisons need to be “multivariate”)\n\nemmeans(within_aov, \n        spec= \"Lecture\", \n        model = \"multivariate\") %>% \n  pairs(adjust=\"tukey\")\n\n contrast           estimate   SE df t.ratio p.value\n Physical - Social      14.0 3.01 11   4.651  0.0019\n Physical - History      5.5 1.52 11   3.618  0.0104\n Social - History       -8.5 3.59 11  -2.368  0.0875\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\n\n40.5.5 Example write up\nAgain I need to get my summary stats for the write up. Since I’m not doing any custom contrasts I can just pull the cell values from my summary table:\n\ndescriptiveStats$Actual\n\n   Lecture  N Score        sd       se       ci\n1  History 12  34.5  8.393721 2.423058 5.333116\n2 Physical 12  40.0 10.795622 3.116428 6.859211\n3   Social 12  26.0 15.201675 4.388345 9.658683\n\n\nAnd now for the write-up:\n… to test this hypothesis we ran a within subjects ANOVA. Due to a violation of the sphericity assumption, we used Greenhouse-Geisser corrected degress of freedom. Our analysis revealed a significant effect for Lecture, \\(F\\)(1.26, 13.88) = 12.31, \\(p\\) = .002, \\(\\eta_p^2\\) = .53. Post-hoc analyses revealed participants scores in the Physical condition (\\(M\\) ± \\(SE\\): 40.0 ± 3.12) was significantly greater than both History (34.5 ± 2.4) and Social (26.0 ± 4.4) scores (\\(p\\)s < .05). History and Social were not different from one another.\nTake a look at the next walkthrough for an example of factorial repeated measures ANOVA."
  },
  {
    "objectID": "week12/12_3-factorial_rma.html#loading-in-the-necessary-packages-and-data",
    "href": "week12/12_3-factorial_rma.html#loading-in-the-necessary-packages-and-data",
    "title": "\n41  Factorial RM-ANOVA and simple effects\n",
    "section": "\n41.1 loading in the necessary packages and data:",
    "text": "41.1 loading in the necessary packages and data:\n\npacman::p_load(tidyverse,\n               afex,\n               cowplot,\n               emmeans)\n\nsource(\"https://raw.githubusercontent.com/tehrandavis/graduate_statistics/main/custom_functions/withinSummary.R\")\n\nfactorial_df <- read_csv(\"https://raw.githubusercontent.com/tehrandavis/graduate_statistics/main/practice_datasets/withinEx3.csv\")\n\nRows: 90 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): ProcessDepth, Day\ndbl (2): Subject, Recalled\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfactorial_df\n\n# A tibble: 90 × 4\n   Subject Recalled ProcessDepth Day  \n     <dbl>    <dbl> <chr>        <chr>\n 1       1       14 High         Day1 \n 2       2       10 High         Day1 \n 3       3       13 High         Day1 \n 4       4       14 High         Day1 \n 5       5       12 High         Day1 \n 6       6        8 High         Day1 \n 7       7       13 High         Day1 \n 8       8       11 High         Day1 \n 9       9        9 High         Day1 \n10      10       12 High         Day1 \n# ℹ 80 more rows\n\n\nOne thing to note is that Subject is being treated as a <dbl> number. It should be treated as a factor.\n\nfactorial_df$Subject <-  as.factor(factorial_df$Subject)"
  },
  {
    "objectID": "week12/12_3-factorial_rma.html#plotting-the-data",
    "href": "week12/12_3-factorial_rma.html#plotting-the-data",
    "title": "\n41  Factorial RM-ANOVA and simple effects\n",
    "section": "\n41.2 plotting the data",
    "text": "41.2 plotting the data\n\n41.2.1 individual subjects plots: Spaghetti\nSince I have two factors here, one strategy may be to facet my spaghetti plots. In this case I am creating seperate spaghetti plots for each level of ProcessDepth:\n\np <- ggplot(data = factorial_df, aes(x=Day, y=Recalled, group = Subject)) +\n  geom_point(aes(col=Subject)) + \n  geom_line(aes(col=Subject)) +\n  facet_wrap(~ProcessDepth)\n\nplotly::ggplotly(p)\n\n\n\n\n\nFWIW, on thing that this plot has clued my in on is that the order of my Processing Depth levels is High, Lo, Med. I might actually want that to be Lo, Med, High:\n\nfactorial_df$ProcessDepth <- fct_relevel(factorial_df$ProcessDepth, \n                                     c(\"Lo\",\"Med\",\"High\")\n                              )   \n\nand now re-plotting…\n\np <- ggplot(data = factorial_df, aes(x=Day, y=Recalled, group = Subject)) +\n  geom_point(aes(col=Subject)) + \n  geom_line(aes(col=Subject)) +\n  facet_wrap(~ProcessDepth)\n\nplotly::ggplotly(p)\n\n\n\n\n\n\n41.2.1.1 means plots\nAgain, we need to make the appropriate correction for our error bars. In this case the number of within groups is the number of cells that is formed by crossing Day (1,2,3) and Processing Depth (Lo, Med, Hi). As this is a 3 * 3 design there are 9 cells, or nWithinGroups=9. As before this is handled auto-magically in summarySEwithin2, or our custom function withinSummary\n\ndescriptiveStats <- withinSummary(data=factorial_df,\n                                  measurevar = \"Recalled\",\n                                  withinvars = c(\"Day\",\"ProcessDepth\"),\n                                  idvar = \"Subject\")\n\nAutomatically converting the following non-factors to factors: Day\n\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\np <- ggplot(descriptiveStats$Corrected, aes(x = Day,\n                                            y = Recalled, \n                                            group=ProcessDepth)) +\n  geom_point(size=2.5, aes(shape = ProcessDepth)) + \n  geom_line(size=1, aes(linetype=ProcessDepth)) +\n  geom_errorbar(aes(ymin=Recalled-se, ymax=Recalled+se), width=0.1) +\n  theme_cowplot() +\n  theme(\n    axis.title = element_text(size = 16, face = \"bold\", lineheight = .55),\n    axis.text = element_text(size = 12),\n    legend.title = element_text(size = 12, face = \"bold\"),\n    legend.position = c(.25,.75)) +\n  scale_color_manual(values=c(\"black\",\"grey50\")) + \n  xlab(\"Lecture\") + \n  ylab (\"Score\") +\n  theme(plot.margin=unit(c(.25,.25,.25,.25),\"in\")) + \n  \n  # stack legend boxes horizontally:\n  theme(legend.box = \"horizontal\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nshow(p)\n\n\n\n\n\n41.2.2 running the omnibus ANOVA:\n\nomnibus_aov <- afex::aov_ez(id = \"Subject\", \n                            dv = \"Recalled\", \n                            data = factorial_df,\n                            between = NULL,\n                            within = c(\"ProcessDepth\",\"Day\"),\n                            type = 3, \n                            return = \"afex_aov\", \n                            include_aov = TRUE, \n                            anova_table=list(es = \"pes\"))\n\nsummary(omnibus_aov)\n\nWarning in summary.Anova.mlm(object$Anova, multivariate = FALSE): HF eps > 1\ntreated as 1\n\n\n\nUnivariate Type III Repeated-Measures ANOVA Assuming Sphericity\n\n                  Sum Sq num Df Error SS den Df  F value    Pr(>F)    \n(Intercept)      13493.4      1   41.067      9 2957.153 1.209e-12 ***\nProcessDepth       432.2      2   61.400     18   63.345 7.137e-09 ***\nDay                254.5      2   95.733     18   23.925 8.521e-06 ***\nProcessDepth:Day   110.4      4   93.400     36   10.636 8.567e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n                 Test statistic p-value\nProcessDepth            0.94884 0.81054\nDay                     0.81573 0.44277\nProcessDepth:Day        0.22026 0.27277\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n                  GG eps Pr(>F[GG])    \nProcessDepth     0.95133  1.559e-08 ***\nDay              0.84440  3.539e-05 ***\nProcessDepth:Day 0.71617  0.0001178 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                   HF eps   Pr(>F[HF])\nProcessDepth     1.199509 7.136593e-09\nDay              1.018163 8.520701e-06\nProcessDepth:Day 1.085791 8.566883e-06\n\n\nHere we have two main effects and an interaction. Let’s unpack the interaction by taking a look at whether Recall increases over successive days.\n\n41.2.3 running the simple effects ANOVAs\n\n41.2.3.1 Recommended method (assumed violation of spherecity)\nThe Cohen chapter assigned this week recommends that one avoid the use of pooled error terms when performing simple effects analysis of within-subjects ANOVA. This is definitely the case when the spherecity assumption is violated, but as a general rule, even when the assumption is not technically violated, deviations from spherecity can artificially inflate Type I error. So in this case, simple follow up ANOVAs that refer to their own error terms are justified. That said as Type I error goes down, Type II goes up. Something to consider.\nOne way to do this would be, as we have done in the past, separate out the data and run separate simple effects ANOVAs using emmeans. This time, we DO NOT use the error term from the omnibus ANOVA, we treat each simple effects ANOVA as self contained. This can be accomplished using joint_tests() from emmeans telling it how we want to separate our original model. As before, we need to add model=\"multivariate\" to our call (this tells R to use the simple effects error term and not the omnibus).\nHere I’m taking a look at the effect of Day on each level of Processing Depth. A better way of saying this is whether you see different rates of learning over Days depending on what level of Processing is involved.\n\njoint_tests(omnibus_aov, by = \"ProcessDepth\", model=\"multivariate\")\n\nProcessDepth = Lo:\n model term df1 df2 F.ratio p.value\n Day          2   9   1.487  0.2768\n\nProcessDepth = Med:\n model term df1 df2 F.ratio p.value\n Day          2   9  30.146  0.0001\n\nProcessDepth = High:\n model term df1 df2 F.ratio p.value\n Day          2   9  32.045  0.0001\n\n\n\n41.2.3.2 Assuming spherecity\nIf we wanted to throw Cohen’s caution to the wind, we would simply run the simple effects tests with a univariate model. Note that this method attempts to deal with potential violations of our spherecity assumptions by using Satterthwaite degrees of freedom. Keep in mind, we are less protected from Type 1 error in this case.\n\njoint_tests(omnibus_aov, by = \"ProcessDepth\", model=\"univariate\")\n\nProcessDepth = Lo:\n model term df1  df2 F.ratio p.value\n Day          2 47.6   0.581  0.5635\n\nProcessDepth = Med:\n model term df1  df2 F.ratio p.value\n Day          2 47.6  11.049  0.0001\n\nProcessDepth = High:\n model term df1  df2 F.ratio p.value\n Day          2 47.6  40.457  <.0001\n\n\nIn both cases we find an effect for Day on the High and Med ProcessDepth.\nAs a follow up we would run the pairwise tests. Here’s the multivariate method:\n\n# multivariate method\nemmeans(omnibus_aov, \n        by = \"ProcessDepth\", \n        specs = pairwise ~ Day, \n        model=\"multivariate\", \n        adjust=\"tukey\")\n\n$emmeans\nProcessDepth = Lo:\n Day  emmean    SE df lower.CL upper.CL\n Day1    9.1 0.482  9     8.01     10.2\n Day2    9.5 0.477  9     8.42     10.6\n Day3   10.0 0.394  9     9.11     10.9\n\nProcessDepth = Med:\n Day  emmean    SE df lower.CL upper.CL\n Day1   10.5 0.522  9     9.32     11.7\n Day2   12.0 0.730  9    10.35     13.7\n Day3   14.4 0.581  9    13.09     15.7\n\nProcessDepth = High:\n Day  emmean    SE df lower.CL upper.CL\n Day1   11.6 0.653  9    10.12     13.1\n Day2   14.1 0.737  9    12.43     15.8\n Day3   19.0 0.715  9    17.38     20.6\n\nConfidence level used: 0.95 \n\n$contrasts\nProcessDepth = Lo:\n contrast    estimate    SE df t.ratio p.value\n Day1 - Day2     -0.4 0.819  9  -0.488  0.8785\n Day1 - Day3     -0.9 0.547  9  -1.646  0.2768\n Day2 - Day3     -0.5 0.687  9  -0.728  0.7539\n\nProcessDepth = Med:\n contrast    estimate    SE df t.ratio p.value\n Day1 - Day2     -1.5 1.035  9  -1.449  0.3585\n Day1 - Day3     -3.9 0.547  9  -7.134  0.0001\n Day2 - Day3     -2.4 0.859  9  -2.794  0.0498\n\nProcessDepth = High:\n contrast    estimate    SE df t.ratio p.value\n Day1 - Day2     -2.5 0.833  9  -3.000  0.0361\n Day1 - Day3     -7.4 0.933  9  -7.929  0.0001\n Day2 - Day3     -4.9 1.090  9  -4.496  0.0038\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nand the univariate (pooled error term):\n\nemmeans(omnibus_aov, \n        by = \"ProcessDepth\", \n        specs = pairwise ~ Day, \n        model=\"univariate\", \n        adjust=\"tukey\")\n\n$emmeans\nProcessDepth = Lo:\n Day  emmean  SE df lower.CL upper.CL\n Day1    9.1 0.6 74      7.9     10.3\n Day2    9.5 0.6 74      8.3     10.7\n Day3   10.0 0.6 74      8.8     11.2\n\nProcessDepth = Med:\n Day  emmean  SE df lower.CL upper.CL\n Day1   10.5 0.6 74      9.3     11.7\n Day2   12.0 0.6 74     10.8     13.2\n Day3   14.4 0.6 74     13.2     15.6\n\nProcessDepth = High:\n Day  emmean  SE df lower.CL upper.CL\n Day1   11.6 0.6 74     10.4     12.8\n Day2   14.1 0.6 74     12.9     15.3\n Day3   19.0 0.6 74     17.8     20.2\n\nWarning: EMMs are biased unless design is perfectly balanced \nConfidence level used: 0.95 \n\n$contrasts\nProcessDepth = Lo:\n contrast    estimate    SE   df t.ratio p.value\n Day1 - Day2     -0.4 0.837 47.6  -0.478  0.8820\n Day1 - Day3     -0.9 0.837 47.6  -1.075  0.5339\n Day2 - Day3     -0.5 0.837 47.6  -0.597  0.8222\n\nProcessDepth = Med:\n contrast    estimate    SE   df t.ratio p.value\n Day1 - Day2     -1.5 0.837 47.6  -1.792  0.1831\n Day1 - Day3     -3.9 0.837 47.6  -4.660  0.0001\n Day2 - Day3     -2.4 0.837 47.6  -2.868  0.0166\n\nProcessDepth = High:\n contrast    estimate    SE   df t.ratio p.value\n Day1 - Day2     -2.5 0.837 47.6  -2.987  0.0121\n Day1 - Day3     -7.4 0.837 47.6  -8.842  <.0001\n Day2 - Day3     -4.9 0.837 47.6  -5.855  <.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\n\n41.2.4 Writing this up\n… To test this hypothesis we ran a 3 (Process Depth) × 3 (Day) repeated-measures ANOVA. Where there were violations of spherecity, we used Greenhouse-Geisser corrected degrees of freedom. This analysis revealed a significant interaction, \\(F\\)(2.86, 25.78) = 10.64, \\(p\\) < .001, \\(\\eta_p^2\\) = .54. To test this interaction we ran seperate simple effects ANOVA for Day on each level of Process Depth. A simple effect for Day was found on High processing depth, \\(F\\)(2, 9) = 32.05, \\(p\\) < .001, where there were statistically significant increases in performance from each Day to the next (Tukey HSD, \\(p\\) < .05). A simple effect for Day was also found on Med processing depth, \\(F\\)(2, 9) = 30.15, \\(p\\) < .001. While scores increased across the 3 days, only Day 3 was significantly greater than the other two (\\(p\\)s < .05). No simple effect was observed for Day in the Lo processing depth condition.\nSee the next walkthrough for how to do this in SPSS."
  },
  {
    "objectID": "week12/12_4-sphericity.html",
    "href": "week12/12_4-sphericity.html",
    "title": "\n42  Sphericity: Variances and Co-Variances\n",
    "section": "",
    "text": "43 Between Subjects Example\nFor this example, let’s assume that we have three groups of participants that we are comparing against one another. Let’s generate the data for these groups—A, B, & C.Each group has the same number of members and same variances (sds), but different means:\nLets start by generating a Group-Condition, A. We’ll create the same values as before\nHowever, this time, instead of generating B and C independently, we will psuedo-randomly generate these scores based on A: where B is 5 greater than A plus or minus some random noise, and C is 5 greater than B plus or minus noise.\nNow lets run our auto-correlations and between conditions correlations:\nand finally build the correlation matrix:\nAs we see in this case, scores in each condition are very strongly correlated with one another (the off-diagonal correlations). We can then take a look at the covariance matrix by cross multiplying by our group standard deviations. (Or, again, since we had the original data, I suppose we could have just taken the cov() of each column)\nThe covariance matrix above represents a case on near-compound-symmetry. This would be as close to the perfect sphericity as we would expect given generated data. I really wouldn’t expect these clean of values ever from my empirical data.\nHopefully, the above example captures how we can use the variance / co-variance matrix to assess how strongly the data in our conditions (and groups) is correlated with one another. For example, I can easily transform back and forth between covariances and correlations using the standard deviations of my groups. For example, going from covariance to correlation:\nSo, our assessment of sphericity is an assement of the degree to data in each condition’s data is correlated with one another, where the ideal scenario is that data in each group is perfectly correlated with the other groups (note this does not mean that the data is indentical, just strongly correlated).\nWith this in mind we can build a calculator example to better understand how devations in correlations result in deviations away from the sphericity assumption. Here we can start with our correlations, use those values to generate hypothetical data, and work our way back. In the code below, you can alter the group means mu and standard deviations stddev, as well as the between condition correlations."
  },
  {
    "objectID": "week12/12_4-sphericity.html#the-correlation-matrix",
    "href": "week12/12_4-sphericity.html#the-correlation-matrix",
    "title": "\n42  Sphericity: Variances and Co-Variances\n",
    "section": "\n43.1 The correlation matrix",
    "text": "43.1 The correlation matrix\n\n43.1.1 Auto-correlation\nAuto correlation simply means the correlation of data with itself. In practical circumstances there is an additional wrinkle to auto-correlation like introducing a “lag” in the data, but for our purposes we are just going to correlate the original data from each group against itself. This will always result in a correlation of 1:\n\naa <- cor(A,A)\nbb <- cor(B,B)\ncc <- cor(C,C)\n\nc(\"correlation A\" = aa,\n  \"correlation B\" = bb,\n  \"correlation C\" = cc)\n\ncorrelation A correlation B correlation C \n            1             1             1 \n\n\n\n43.1.2 across groups correlations\nNow we need to test for the correlations of each group to the other groups. Notice that when we generated our data in Chunk 1, each group was randomly generated, independent of one another. This captures these two critical assumptions of Between Subjects ANOVA. This also means that there should be no correlations (inter-dependencies) between our groups. While theoretically this means that any between group correlations should equal zero, in practice we should get a very low number. (Keep in mind your values may be different from mine due to random generation, unless you set the seed as I did to 1):\n\nab <- cor(A,B)\nac <- cor(A,C)\nbc <- cor(B,C)\n\nc(\"correlation AB\" = ab, \"correlation AC\" = ac, \"correlation BC\" = bc)\n\ncorrelation AB correlation AC correlation BC \n  -0.039087178    0.026287227    0.003552006 \n\n\nFrom here, we can build our correlation matrix by first assigning the mirrors to our correlations above (e.g., the correlation of A to C is the same as C to A).\n\nba <- ab # mirrors\nca <- ac\ncb <- bc\n\nAnd building the matrix\n\ncorrelation_matrix <- matrix(c(aa, ab, ac,\n                               ba, bb, bc,\n                               ca, cb, cc),\n                             ncol = 3)\nrownames(correlation_matrix) = c(\"A\", \"B\", \"C\")\ncolnames(correlation_matrix) = c(\"A\", \"B\", \"C\")\n\ncorrelation_matrix %>% round(3)\n\n       A      B     C\nA  1.000 -0.039 0.026\nB -0.039  1.000 0.004\nC  0.026  0.004 1.000"
  },
  {
    "objectID": "week12/12_4-sphericity.html#the-covariance-matrix",
    "href": "week12/12_4-sphericity.html#the-covariance-matrix",
    "title": "\n42  Sphericity: Variances and Co-Variances\n",
    "section": "\n43.2 The covariance matrix",
    "text": "43.2 The covariance matrix\nWe can translate the correlation matrix to a covariance matrix by cross multiplying by our group standard deviations. (Or, since we had the original data, I suppose we could have just taken the cov() of each column)\n\nstdevs <- c(sd(A), sd(B), sd(C))\n\n# need to transpose for matrix multiplication\ncovariance_matrix <- correlation_matrix * stdevs %*% t(stdevs) \n\ncovariance_matrix\n\n           A          B          C\nA 33.8695775 -1.5427086  0.9635283\nB -1.5427086 45.9927476  0.1517166\nC  0.9635283  0.1517166 39.6670215\n\n\nThe important thing to note here, that our simulations reinforce is that for the sake of the Between Subjects ANOVAs all values off of the diagonal are effectively zero. Therefore the Between Subjects ANOVA is only concerned with the relationships (equivalence) of values along the diagonal.\nLet kick this up to within subjects (repeated measures) designs."
  },
  {
    "objectID": "week12/12_4-sphericity.html#build-correlation-matrix",
    "href": "week12/12_4-sphericity.html#build-correlation-matrix",
    "title": "\n42  Sphericity: Variances and Co-Variances\n",
    "section": "\n46.1 Build correlation matrix",
    "text": "46.1 Build correlation matrix\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nlibrary(tidyverse)\n\nset.seed(5)\n\n# YOU CAN CHANGE ME\nmu <- c(4.23, 3.01, 2.91) # group means\nstddev <- c(2, 2, 2) # group standard devs\n\n# correlations (variances, DO NOT CHANGE)\naa = 1\nbb = 1\ncc = 1\n\n# correlations (covariances, a to b, a to c, b to c, CHANGE)\nab = .3\nac = .6\nbc = .1\n\nBased upon your changes you can not build the appropriate correlation matrix\n\n# DO NOT ALTER\n\nba = ab #mirror\nca = ac\ncb = bc\n\ncorMat <- matrix(c(aa, ab, ac,\n                   ba, bb, bc,\n                   ca, cb, cc),\n                 ncol = 3)\n\ncorMat\n\n     [,1] [,2] [,3]\n[1,]  1.0  0.3  0.6\n[2,]  0.3  1.0  0.1\n[3,]  0.6  0.1  1.0"
  },
  {
    "objectID": "week12/12_4-sphericity.html#build-a-covariance-matrix",
    "href": "week12/12_4-sphericity.html#build-a-covariance-matrix",
    "title": "\n42  Sphericity: Variances and Co-Variances\n",
    "section": "\n46.2 build a covariance matrix",
    "text": "46.2 build a covariance matrix\nNow to create the covariance matrix\n\ncovMat <- stddev %*% t(stddev) * corMat\ncovMat\n\n     [,1] [,2] [,3]\n[1,]  4.0  1.2  2.4\n[2,]  1.2  4.0  0.4\n[3,]  2.4  0.4  4.0"
  },
  {
    "objectID": "week12/12_4-sphericity.html#build-simulated-data",
    "href": "week12/12_4-sphericity.html#build-simulated-data",
    "title": "\n42  Sphericity: Variances and Co-Variances\n",
    "section": "\n46.3 build simulated data",
    "text": "46.3 build simulated data\nyou can use this code to build out simulated data based on the parameters you have entered above:\n\ndf <- mvrnorm(n = 212, # number of participants\n              mu = mu, # means from above\n              Sigma = covMat, # covariance matrix from above\n              empirical = TRUE) %>%\n  data.frame()\nnames(df) <- c(\"A\",\"B\",\"C\")\ndf\n\n               A           B           C\n1    4.277062678  3.90719459  4.45579227\n2    4.641274608  1.49615783  1.42113739\n3    3.827340659  3.83714702  5.24109404\n4    5.851299251  5.04517587  2.13024154\n5    4.246887804  0.05766786  1.90040053\n6    4.979052424  2.92928818  6.02874995\n7    6.093413904  5.03705424  4.14889026\n8    6.957267914  6.01743282  4.43155681\n9    4.127715697  3.16528570  3.57542196\n10   3.995481497  2.58402796  2.89951762\n11   5.159923190 -1.11939545  6.27617987\n12   2.835283218  3.66516386  2.74910499\n13  -0.005171068  3.08004143  0.64309191\n14   1.604748598  0.93847074  2.93725316\n15   0.104871288  3.46868401  0.21707385\n16   2.978828416  2.71749637  2.21460297\n17   3.511566669  4.64110544  1.64587304\n18   0.969214756  8.69093535 -2.51418694\n19   4.006297746  2.87981840  2.17936103\n20   4.286101350  2.22302861  5.02910647\n21   3.095643825  1.01454567  1.53614891\n22   3.565675803  1.48623563  1.37257349\n23   2.526985556 -0.62642339  1.32087133\n24   7.546741165  2.33181844  6.23095835\n25   3.903580984 -0.71824722  5.30337738\n26   3.309238135  3.69308917  1.76454027\n27   3.941415739  0.52761729  1.74132239\n28   7.374147523  1.68879043  4.44490095\n29   4.548794063  4.13414632  3.92912026\n30   3.843303195  4.39002312  3.23504463\n31   4.665123667  2.93900057  2.74286733\n32   6.816769693  1.45988113  5.23032884\n33   5.554110070  2.21077803 -0.94466206\n34   3.177194088  0.67750507  1.13568517\n35   7.328580371  2.64875995  3.09567325\n36   4.031199119  1.31165763  2.20948641\n37   2.946212473  5.23347820  1.33831893\n38   5.638877551  7.53906151  4.77114360\n39   2.524047840  4.99203285  3.45085161\n40   5.952927868  5.31870294  2.54216798\n41   2.644049929 -1.61338551  2.60930780\n42   2.675571255  3.54267916  2.70994174\n43   0.689786180  2.24912497 -0.38416886\n44   5.546689126  2.88443793 -0.90832427\n45   2.559404085  3.05165811  2.17096064\n46   4.916914595  3.80676397  1.09483360\n47   3.007383173  2.42163012  4.99117018\n48   4.484086905  4.16310585  3.19081765\n49   6.437561152  8.48952865  0.53957314\n50   4.798749088  3.57288866  3.22632821\n51   6.741865472  1.39823211  4.11913155\n52   2.041130755  1.36884085  1.84317573\n53   4.914176044  1.68598773  2.64389017\n54   6.403865046  3.22546950  7.48096292\n55   4.583086014  2.48682853  4.59807826\n56   2.366962683  3.66978645  2.49962789\n57   2.616965642  2.24802193 -0.37949205\n58   3.761107399  3.77498769  1.69129705\n59   5.191838186  4.55744819  2.35175825\n60   3.079915252  2.82050020  1.04896171\n61   5.888754446  7.73430658  2.14805398\n62   7.115827043  2.22991475  5.33892244\n63   7.488374891  4.71283481  6.79726047\n64   5.934307128  3.74617467  2.74320888\n65   1.226151281  3.35782120  0.90249027\n66   3.893422498  3.12289455  3.49477790\n67   4.858224567  7.13468474  4.61313986\n68   4.698820470  4.20263249  2.92976471\n69   4.842739506  5.80489466  3.88425063\n70   4.384869661  4.74484519  1.68006882\n71   0.724852026  1.38955548  1.27280809\n72   5.466053126  3.31746769  4.96210063\n73   5.539570315  4.22359237  2.00879115\n74   1.525839944  1.47877832  1.48290147\n75   6.564609727  2.60421403  5.44700856\n76   7.540720669  6.49204720  3.05327059\n77   7.330073078  1.69561488  6.00749110\n78   3.288495378  2.82239924  1.67319019\n79   2.465953025  1.26157259  2.55677739\n80   4.955757395  3.83721184  4.59742601\n81   7.114691053  4.17457903  3.71117308\n82   4.165306104  6.08176310  4.04956514\n83   2.489275635 -0.54667597  2.68746169\n84   1.991995396  1.90605485  1.68026325\n85   6.685800702  2.82682678  4.48201578\n86   4.080930455  2.85201370  5.27139543\n87   4.658162092 -1.24823958  2.18653743\n88   6.366383739  5.59593453  3.73195364\n89   4.408515004  1.68368996  5.38051089\n90   5.935792738  5.38396806  3.60509507\n91   6.361088298  0.68857606  6.27594523\n92   3.502997064  4.09979322  3.83930421\n93   1.742894734  1.73502994 -0.21645273\n94   4.437940905  2.63980610  1.04405939\n95   7.647354334  2.82269152  4.79618384\n96   2.890270047  1.51172167  2.16817978\n97   4.715529686  4.35682082  2.87475186\n98   5.885987498  4.86234182  4.32861724\n99  -1.590478407 -1.09971579  1.61207753\n100  6.553873310  3.69122536  5.37320979\n101  1.918659612  3.43882916  5.54923542\n102  4.579194887  2.73765586  0.36680869\n103  4.052586532  1.50417752  2.82379931\n104  6.618348581  2.75663495  5.94125595\n105  8.094050768  5.18835527  5.32161960\n106  3.961341824  1.62088755  1.86097402\n107 -0.100848882  1.09591631  0.65965677\n108  2.669225212  4.65964963  4.74361915\n109  3.068611435  3.79168696  2.73302864\n110  6.736171603  1.19701224  4.96283307\n111  5.687754926  5.80268835  4.02387176\n112  5.513496066  0.28466870  3.54947645\n113  5.572249747  0.77714155 -0.10014258\n114  1.733975553  1.86674136  0.88253509\n115  4.070846031  4.50797576  4.91923864\n116  1.719285522 -0.57555341  2.24807024\n117  3.181364809  6.75887872  1.20234405\n118  3.061936122  2.06096413  1.37681613\n119  3.735262445  3.98179630  0.85369327\n120  2.777023228  2.28939315  2.74248268\n121  3.177633288  0.01405719  2.21224057\n122  3.759727726  2.04134204  1.02867964\n123  5.621811340  6.16973910  4.55326269\n124  3.162682761  6.22359498  2.20368518\n125  1.495419998  0.22749135  0.01224626\n126  2.910962828  1.11037135  3.05024568\n127  7.155604423  5.96300673  4.04218792\n128  5.235852596  1.29471411  2.93691950\n129 -1.039288431  4.39288460 -1.69485987\n130 -2.636641009  2.23463377 -0.56215565\n131  2.920555500  1.61447124 -0.93195323\n132  4.338234264  5.67143001  5.59065801\n133  2.211628570  4.79637589  1.69012464\n134  3.963014235  4.15356977  1.40229644\n135  6.811097035  4.70724849  1.82883493\n136  2.153369868  1.89678588  0.41912203\n137  4.578847704  2.63597591  3.13992896\n138  5.899353521  5.01470267  5.78889325\n139  4.301508346  4.76790374 -1.34641167\n140  4.891574666  2.75154904  3.36074262\n141  4.607337631  3.57348446  5.22655528\n142  2.861177851  2.59722616  1.88608748\n143  3.228850162 -1.97992565  1.94869896\n144  4.047054265  0.35846755  3.55206849\n145  7.991273330  4.76406833  6.54820328\n146  2.854297065  5.07641001 -0.39918021\n147  4.024171978  2.36517097  3.36387222\n148  3.834959195  2.68843119  3.68081879\n149  3.565449325  6.14544180  2.14334857\n150 10.688234597  7.23269190  6.95257620\n151  6.923292220  3.28840604  3.31089347\n152  7.029896676  2.88571454  4.62254573\n153  4.713844901  5.27697059  2.10969860\n154  5.566531794  1.34982026  4.65104416\n155  4.454156578  5.35119775  1.96025240\n156  5.403492577  2.10472226  2.03748430\n157  5.966224188  4.64794615  4.10956116\n158  6.156569509  2.93978422  1.99974788\n159  2.842394834  2.37827631  3.45407983\n160  6.663340690  3.66086927  4.80004140\n161  0.633401685  6.95937648  0.82884060\n162  5.912348389 -0.23307545  2.87259005\n163  3.137054659  2.49275246  2.02008801\n164  6.298191436  0.35241044  4.48701655\n165  3.070904489  3.54745178  2.32606343\n166  4.496609150  3.01485493  5.43954936\n167  5.447971819  3.60431561  3.95908532\n168  1.912932672  1.32998677  2.67801784\n169  6.811280570  2.66941277  7.02987690\n170  5.374025770  3.40827075  5.51909968\n171  5.185190160  5.39666199  5.43621164\n172  4.462388665  1.79474697  4.61470103\n173  6.332813675  5.08785560  6.76456048\n174  5.348235164  5.38288086  6.64319561\n175  5.501143713  3.30213108  4.44019326\n176  3.744667973  2.87792429  1.54938298\n177  2.293072024 -0.69028542  0.77429111\n178  2.117169544  3.31822499  3.10882611\n179  1.979164023 -0.30049849  1.84419768\n180  7.317472314  6.84005805  3.63116400\n181  4.345408665  0.08494558  0.53363204\n182  5.210681323  4.24477246  4.74047766\n183  4.750237470  1.68813633  3.26638967\n184 -0.171006222  0.64899783 -2.54210016\n185  2.814608215  4.80567279  2.55537546\n186  5.563067719  4.13854408  4.09863588\n187  3.763718560  4.99995695  0.21172177\n188  1.220724887  1.31075671  2.60446568\n189  5.373438916  3.39646036  1.72730598\n190  5.753128372  6.02352237  4.46976059\n191  4.533720364  4.09091974 -0.14326423\n192  3.916882338  3.28931152  1.09503713\n193  4.681860230  3.78965022  4.14961571\n194  1.843652778  0.84803951  1.49521177\n195  3.115964156  2.80808528  2.42176079\n196  9.287889591  2.69526989  6.08213853\n197  5.318852446  3.31253505  1.53751168\n198  1.749218662  3.17095635 -0.75159785\n199  3.234556324  1.85368972  5.35946551\n200  3.141401684  1.77327028  1.07158954\n201  1.206393970  1.69343124  1.20810150\n202  1.482106323 -0.01184140  4.53612541\n203  6.242192992  3.18006831  2.41977491\n204  5.712270352  2.83039353  7.68170170\n205  5.398089473  4.26064970  1.98862815\n206  2.910493463  0.71633051  3.22860748\n207  5.047575310  2.63646436  2.76197619\n208  2.954976966  2.49189066 -0.83984593\n209  3.579038340  3.16974229  3.09029601\n210  5.659629276  0.57006782  6.08220828\n211  2.344912445 -1.67070231  3.31976643\n212  2.434378236  1.74160205  0.47390910"
  },
  {
    "objectID": "week12/12_4-sphericity.html#analyze-for-homogeneity-of-sums-minus-covariances",
    "href": "week12/12_4-sphericity.html#analyze-for-homogeneity-of-sums-minus-covariances",
    "title": "\n42  Sphericity: Variances and Co-Variances\n",
    "section": "\n46.4 Analyze for Homogeneity of Sums minus Covariances:",
    "text": "46.4 Analyze for Homogeneity of Sums minus Covariances:\nAnd finally check for the similarity of the Sums minus Covariances of each pairwise comparison. Note how the resulting values deviate from one another as you change the variances of each group and the correlations between groups from the start.\n\nvarA <- var(df$A)\nvarB <- var(df$B)\nvarC <- var(df$C)\n\ncovAB <- cov(df$A, df$B)\ncovAC <- cov(df$A, df$C)\ncovBC <- cov(df$B, df$C)\n\nAB <- varA + varB - 2*covAB\nAC <- varA + varC - 2*covAC\nBC <- varB + varC - 2*covBC\n\nlist(\"A to B\" = AB, \"A to C\" = AC,\"B to C\" = BC)\n\n$`A to B`\n[1] 5.6\n\n$`A to C`\n[1] 3.2\n\n$`B to C`\n[1] 7.2"
  },
  {
    "objectID": "week12/12_5-spss_rma.html#how-to-perform-a-within-subjects-analysis-in-spss",
    "href": "week12/12_5-spss_rma.html#how-to-perform-a-within-subjects-analysis-in-spss",
    "title": "43  Running a RM-ANOVA in SPSS",
    "section": "43.1 How to perform a within subjects analysis in SPSS:",
    "text": "43.1 How to perform a within subjects analysis in SPSS:\nI’ll likely put together my own vid, but it won’t be much better than this:"
  }
]