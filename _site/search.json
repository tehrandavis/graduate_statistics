[
  {
    "objectID": "walkthroughs.html",
    "href": "walkthroughs.html",
    "title": "PSYC 7014",
    "section": "",
    "text": "This is a landing page for the PSYC 7014 Walkthroughs. Individual weeks and sections can be found on the left sidebar."
  },
  {
    "objectID": "week06/6_1_t-test.html",
    "href": "week06/6_1_t-test.html",
    "title": "Testing differences in means / t-test",
    "section": "",
    "text": "This week we cover when and how to conduct a \\(t-test\\). We use a t-test to assess whether the observed difference between sample means is greater than would be predicted be chance. Both the Navarro text and Poldrack text do a wonderful job of explaining t-tests conceptually so I will defer to those experts on matters of the underlying their statistical basis. Instead my goal this week is to walk through some examples on performing, interpreting, and reporting t-tests using R.\nThis walkthough assumes that the following packages are installed and loaded on your computer:"
  },
  {
    "objectID": "week06/6_1_t-test.html#things-to-consider-before-running-the-t-test",
    "href": "week06/6_1_t-test.html#things-to-consider-before-running-the-t-test",
    "title": "Testing differences in means / t-test",
    "section": "Things to consider before running the t-test",
    "text": "Things to consider before running the t-test\nBefore running a t.test there are a few practical and statistical considerations that must be taken. In fact, these considerations extend to every type of analysis that we will encounter for the remainder of the semester (and indeed the rest of your career) so it would be good to get in the habit of running through your checks. In what proceeds here I will walk step by step with how I condunct a t.test (while also highlighting certain decision points as they come up).\nWhat is the nature of your sample data?\nIn other words where is the data coming from? Is it coming from a single sample of participants? Is it coming from multiple samples of the SAME participants? Is it coming from multiple groups of participants. This will not only determine what analysis you choose to run, but in also how you go about the business of preparing to run this analysis. Of course, truth be told this information should already be known before you even start collecting your data, which reinforces an important point, your central analyses should already be selected BEFORE you start collecting data! As you design your experiments you should do so in a way in which the statistics that you run are built into the design, not settled upon afterwards. This enables you to give the tests you perform the most power, as you are making predictions about the outcomes of your test a priori. This will become a major theme on the back half of the class, but best to introduce it now.\nFor this week, it will determine what test we will elect to perform. Let’s grab some sample data from an experiment by Hand, et al. (1994)\n\nHand, et al., 1994, reported on family therapy as a treatment for anorexia. There were 17 girls in this experiment, and they were weighed before and after treatment. The weights of the girls, in pounds, is provided in the data below:\n\n\nanorexia_data <- read_delim(\"https://www.uvm.edu/~statdhtx/methods8/DataFiles/Tab7-3.dat\", \n                     \"\\t\", escape_double = FALSE, trim_ws = TRUE)\n\nRows: 17 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): ID\ndbl (2): Before, After\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nSo what is known: we have 17 total participants from (hypothetically) the same population that are measured twice (once Before treatment, and once After treatment). Based upon the experimental question we need to run a paired-sample (matched-sample) test. (Although I’ll use this data to provide an example of a one-sample test later on).\nWhat is the structure of your data file?\nBefore doing anything you should always take a look at your data:\n\nshow(anorexia_data)\n\n# A tibble: 17 × 3\n   ID    Before After\n   <chr>  <dbl> <dbl>\n 1 01      83.8  95.2\n 2 02      83.3  94.3\n 3 03      86    91.5\n 4 04      82.5  91.9\n 5 05      86.7 100. \n 6 06      79.6  76.7\n 7 07      76.9  76.7\n 8 08      94.2 102. \n 9 09      73.4  94.9\n10 10      80.5  75.2\n11 11      81.6  77.8\n12 12      82.1  95.5\n13 13      77.6  90.7\n14 14      83.5  92.6\n15 15      89.9  93.8\n16 16      86    91.7\n17 17      87.3  98  \n\n\nSo what do we have here, three columns:\n\n\nID: the participant number\n\nBefore: participants’ weights before treatment\n\nAfter: participants’ weights after treatment\n\nMost important for present purposes this data is in WIDE format—each line represents a participant. While this might be intuitive for tabluar visualization, many statistical softwares prefer when LONG format, where each line represents a single observation (or some mixed of WIDE and LONG like SPSS).\nI spoke a little bit about this issue in Week 2, Walkthrough 0.\nGetting data from WIDE to LONG\nSo the data are in WIDE format, each line has multiple observations of data that are being compared. Here both Before scores and After scores are on the same line. In order to make life easier for analysis and plotting in ggplot, we need to get the data into LONG format (Before scores and After scores are on different lines). This can be done using the pivot_longer() function from the tidyr package.\nBefore gathering, one thing to consider is whether or not you have a column that defines each subject. In this case we have ID. This tells R that these data are coming from the same subject and will allow R to connect these data when performing analysis. That said, for t.test() this is not crucially important—t.test() assumes that the order of lines represents the order of subjects, e.g., the first Before line is matched to the first After line. Later on when we are doing ANOVA, however, this participant column will be important an we will need to add if it is missing.\nUsing pivot_longer(): This function takes a number of arguments, but for us right now, the most important are data: your dataframe; cols: which columns to gather; names_to: what do you want the header of the collaped nonminal variables to be? Here, we might ask what title would encapsulate both Before and After. I’ll choose treatment ; values_to: what do the values represent, here I choose weight. I’m just going to overwrite the original data frame:\n\nanorexia_data <- pivot_longer(anorexia_data,cols = c(\"Before\",\"After\"),names_to = \"treatment\", values_to = \"weight\")\nanorexia_data\n\n# A tibble: 34 × 3\n   ID    treatment weight\n   <chr> <chr>      <dbl>\n 1 01    Before      83.8\n 2 01    After       95.2\n 3 02    Before      83.3\n 4 02    After       94.3\n 5 03    Before      86  \n 6 03    After       91.5\n 7 04    Before      82.5\n 8 04    After       91.9\n 9 05    Before      86.7\n10 05    After      100. \n# ℹ 24 more rows\n\n\nOk data is structured correctly, on to the next step.\nTesting assumptions\nRemember that you should always test to see if the data fit the assumptions of the test you intend to perform. In this case, we need to assess two things:\nIs the data normally distributed?\nKnowing the design of your experiment also has implications for testing your assumptions. For example, whether you have a paired (matched) sample design (e.g., two samples from the same participants) or an independent sample design (e.g., two groups) determines how you go about the business of testing the normality assumption. If you have an independent samples test, you test each sample separately, noting measures of skew, kurtosis, inspecting the qqPlot, and Shapiro-Wilkes test (though acknowledging that SW is very sensitive). However, if you are running a paired (matched) samples test, you need to be concerned with the distribution of the difference scores. In the present example we are comparing participants’ weights Before treatment to their weight After. This is a paired design, so I need to test the differences between each participant’s Before and After for normality.\nFirst, let me filter() my data accordingly for Before and After (essentially creating separate vectors for each condition):\n\nbeforeTreatment <- filter(anorexia_data, treatment==\"Before\") \nafterTreatment <- filter(anorexia_data, treatment==\"After\") \n\nAnd now compute the difference scores, and run my assumption tests:\n\ndiffWeights <- beforeTreatment$weight - afterTreatment$weight\n\npsych::describe(diffWeights)\n\n   vars  n  mean   sd median trimmed  mad   min max range skew kurtosis   se\nX1    1 17 -7.26 7.17   -9.1   -7.15 5.93 -21.5 5.3  26.8 0.18    -0.77 1.74\n\ncar::qqPlot(diffWeights)\n\n\n\n\n[1]  9 10\n\nshapiro.test(diffWeights)\n\n\n    Shapiro-Wilk normality test\n\ndata:  diffWeights\nW = 0.9528, p-value = 0.5023\n\n\nWhat conclusions might we draw about normality?\nGetting the descriptive stats and plotting the means.\nFinally, as we will be performing a test of difference in means, it would be a good idea to get descriptive measures of means and variability for each group. Indeed, these data were already obatined when we used psych::describe() to assess the normality of each sample. Here I’ll just do it again to get these values:\n\npsych::describeBy(anorexia_data$weight,group = anorexia_data$treatment)\n\n\n Descriptive statistics by group \ngroup: After\n   vars  n  mean   sd median trimmed  mad  min   max range  skew kurtosis   se\nX1    1 17 90.49 8.49   92.6   90.77 3.85 75.2 101.6  26.4 -0.74    -0.93 2.06\n------------------------------------------------------------ \ngroup: Before\n   vars  n  mean   sd median trimmed  mad  min  max range skew kurtosis   se\nX1    1 17 83.23 5.02   83.3   83.15 4.15 73.4 94.2  20.8 0.15    -0.29 1.22\n\n\nTypically along with the mean, you need to report a measure of variability of your sample. This can be either the SD, SEM, or if you choose the 95% CI, although this is more rare in the actual report. See the supplied HW example and APA examples for conventions on how to report these in your results section.\nPlotting in ggplot\nI’ve mentioned the limits and issues with plotting bar plots, but they remain a standard, so we will simply proceed using these plots. But I’ll note that boxplots, violin plots, bean plots, and pirate plots are all modern alternatives to bar plots and are easy to execute in ggplot(). Try a Google search.\nIn the meantime, to produce a bar plot in R we simply modify a few of the arguments that we are familiar width.\nHere is the code for plotting these two groups:\n\nggplot(data = anorexia_data, aes(x=treatment, y=weight)) +\n  stat_summary(fun = \"mean\", geom = \"col\") + \n  stat_summary(fun.data = \"mean_se\", geom = \"errorbar\", width = .1) +\n  scale_y_continuous(expand = c(0,0)) + \n  theme_cowplot()\n\n\n\n\nBreaking this down line-by-line:\n\nggplot(data = anorexia_data, aes(x=treatment, y=weight)): standard fare for starting a ggplot.\nstat_summary(fun = \"mean\", geom = \"col\"): stat_summary() gets summary statistics and projects them onto the geom of your choice. In this case we are getting the mean values, fun = \"mean\" and using them to create a column plot geom = \"col\" .\nstat_summary(fun.data = \"mean_se\", geom = \"errorbar\", width = .1) : here we are creating error bars, geom = \"errorbar\". Important to note here is that error bars require knowing three values: mean, upper limit, and lower limit. Whenever you are asking for a single value, like a mean, you use fun. When multiple values are needed you use fun.data. Here fun.data = \"mean_se\" requests Standard error bars. Other alternatives include 95% CI \"mean_cl_normal\" and Standard deviation \"mean_sdl\". The width argument adjusts the width of the error bars.\nscale_y_continuous(expand = c(0,0)): Typically R will do this strange thing where it places a gap bewteen the data and the x-axis. This line is a hack to remove this default. It says along the y-axis add 0 expansion (or gap).\ntheme_cowplot(): quick APA aesthetics.\n\nYou may also feel that the zooming factor is off. This may especially be true in cases where there is little visual discrepency between the bars. To “zoom in” on the data you can use coord_cartesian(). For example, you might want to only show the range between 70 lbs and 100 lbs. When doing this, be careful not to truncate the upper limits of your bars and importantly your error bars.\n\nggplot(data = anorexia_data, aes(x=treatment, y=weight)) +\n  stat_summary(fun = \"mean\", geom = \"col\") + \n  stat_summary(fun.data = \"mean_se\", geom = \"errorbar\", width = .1) +\n  scale_y_continuous(expand = c(0,0)) + \n  theme_cowplot() +\n  coord_cartesian(ylim = c(70,100))\n\n\n\n\nAdditionally, to get this into true APA format I would need to adjust my axis labels. Here capitalization is needed. Also, because the weight has a unit measure, I need to be specific about that:\n\nggplot(data = anorexia_data, aes(x=treatment, y=weight)) +\n  stat_summary(fun = \"mean\", geom = \"col\") + \n  stat_summary(fun.data = \"mean_se\", geom = \"errorbar\", width = .1) +\n  scale_y_continuous(expand = c(0,0)) + \n  theme_cowplot() +\n  coord_cartesian(ylim = c(70,100)) +\n  xlab(\"Treatment\") + \n  ylab(\"Weight (lbs)\")\n\n\n\n\nFinally, you may have notice that the order of Treatment on the plot is opposite of what we might like to logically present. In this case the “After” data comes prior to the “Before” data on the x-axis. This is because R defaults to alphabetical order when loading in data. To correct this I can use scale_x_discrete() and specify the order that I want in limits:\n\nggplot(data = anorexia_data, aes(x=treatment, y=weight)) +\n  stat_summary(fun = \"mean\", geom = \"col\") + \n  stat_summary(fun.data = \"mean_se\", geom = \"errorbar\", width = .1) +\n  scale_y_continuous(expand = c(0,0)) + \n  theme_cowplot() +\n  coord_cartesian(ylim = c(70,100)) +\n  xlab(\"Treatment\") + \n  ylab(\"Weight (lbs)\") + \n  scale_x_discrete(limits=c(\"Before\",\"After\"))\n\n\n\n\nAlternatively I can correct the order of the levels of a factor within the dataframe itself using fct_relevel() (this gets loaded with tidyverse). Note that here I am overwriting the original treatment column. If you do this proceed at your own risk! You could also just mutate a new column if you would rather not overwrite.\n\nanorexia_data$treatment <- fct_relevel(anorexia_data$treatment, \"Before\", \"After\")\n\nggplot(data = anorexia_data, aes(x=treatment, y=weight)) +\n  stat_summary(fun = \"mean\", geom = \"col\", \n               fill = \"lightgray\", color = \"black\") + # adding some color mods here.\n  stat_summary(fun.data = \"mean_se\", geom = \"errorbar\", width = .1) +\n  scale_y_continuous(expand = c(0,0)) + \n  theme_cowplot() +\n  coord_cartesian(ylim = c(70,100)) +\n  xlab(\"Treatment\") + \n  ylab(\"Weight (lbs)\")\n\n\n\n# note that I don't have to do the `scale_x_discrete(limits=c(\"Before\",\"After\"))` correction\n\nAll good (well maybe check with Sierra first)! One other thing to consider (although please do not worry about it here) is the recent argument that when dealing with repeated measures data you need to adjust you error bars. See this pdf by Richard Morey (2005) for more information on this issue. We’ll revisit this issue when running Repeated Measures ANOVA.\nAn aside… other types of plots\nAs I mentioned barplots (especially those that use standard error bars) have more recently come under criticism for “hiding” the true name of the data. There is currently a movement to make data more transparent using other kinds of plots that convey more information about your sample. For example let’s contrast out barplot from above with a combination “pointrange” and violin plot. The pointrange simply provides the mean as a point with error bars extending as specified (here I choose standard error). The violin plot is essentially a histogram of the data turned on its side, centered on the mean, and then mirrored… it gives us info about the TRUE distribution of scores.\nLet’s take a look side by side\n\nbarplot <- ggplot(data = anorexia_data, aes(x=treatment, y=weight)) +\n  stat_summary(fun = \"mean\", geom = \"col\", \n               fill = \"lightgray\", color = \"black\") + # adding some color mods here.\n  stat_summary(fun.data = \"mean_se\", geom = \"errorbar\", width = .1) +\n  scale_y_continuous(expand = c(0,0)) + \n  theme_cowplot() +\n  coord_cartesian(ylim = c(70,110)) +\n  xlab(\"Treatment\") + \n  ylab(\"Weight (lbs)\")\n\n# note that order matters. I need to do the violin before the pointrange or else the violin will \"paint over\" the pointrange.\n\npoint_violin_plot <- ggplot(data = anorexia_data, aes(x=treatment, y=weight)) +\n  geom_violin() +\n  stat_summary(fun.data = \"mean_se\", geom = \"pointrange\", color = \"black\") + # adding some color mods here.\n  scale_y_continuous(expand = c(0,0)) + \n  theme_cowplot() +\n  coord_cartesian(ylim = c(70,110)) +\n  xlab(\"Treatment\") + \n  ylab(\"Weight (lbs)\")\n\ncowplot::plot_grid(barplot, point_violin_plot)\n\n\n\n\nAs you can see the second plot gives me more information about what’s truly going on with my data."
  },
  {
    "objectID": "week06/6_1_t-test.html#performing-the-t-test-paired-sample-t-test",
    "href": "week06/6_1_t-test.html#performing-the-t-test-paired-sample-t-test",
    "title": "Testing differences in means / t-test",
    "section": "Performing the t-test (Paired sample t-test)",
    "text": "Performing the t-test (Paired sample t-test)\nOkay, now that we’ve done all of our preparation, we’re now ready to perform the test. We can do so using the t.test() function. In this case, the experimental question warrants a paired samples t-test. Given that our Levene’s test failed to reject the null, we will assume that our variances are equal.\nSince we’ve got long-format data we will use the formula syntax. This reads “predicting changes in weight as a function of treatment.\n\nt.test(weight~treatment,data=anorexia_data,paired=T, var.equal=T)\n\n\n    Paired t-test\n\ndata:  weight by treatment\nt = -4.1802, df = 16, p-value = 0.0007072\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -10.948840  -3.580571\nsample estimates:\nmean difference \n      -7.264706 \n\n\nThe output provides us with our \\(t\\) value, the \\(df\\) and the \\(p\\) value. It also includes a measure of the 95% CI, and the mean difference. Remember that the null hypothesis is that there is no difference between our two samples. In the case of repeated measures especially, it makes sense to think of this in terms of a difference score of change, where the null is 0. The resulting interpretation is that on average participants’ weight increased 7.26 pounds due to the treatment, with a 95% likelihood that the true mean change is between 3.58 lbs and 10.95 lbs. Important for us is that 0 is not in the 95% CI, reinforcing that there was indeed a non-zero change (rejecting the null)."
  },
  {
    "objectID": "week06/6_1_t-test.html#other-t-tests",
    "href": "week06/6_1_t-test.html#other-t-tests",
    "title": "Testing differences in means / t-test",
    "section": "Other \\(t\\) tests:",
    "text": "Other \\(t\\) tests:\nOne sample:\nThe data in our example warranted running a paired t-test. However, as noted we can run a t.test() to compare a single sample to a single value. For example it might be reasonable to ask whether or not the 17 adolescent girls that Hand, et al., 1994 treated were different from what would be considered the average weight of a teenaged girl. A quick Google search suggests that the average weight of girls 12-17 in 2002 was 130 lbs. How does this compare to Hand et al.’s participants Before treatment? We can run a one sample t-test to answer this question:\n\nbeforeTreatment <- filter(anorexia_data,treatment==\"Before\")\nt.test(beforeTreatment$weight, mu = 130)\n\n\n    One Sample t-test\n\ndata:  beforeTreatment$weight\nt = -38.44, df = 16, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 130\n95 percent confidence interval:\n 80.65007 85.80876\nsample estimates:\nmean of x \n 83.22941 \n\n\nYes, this group of girls was significantly underweight compared to the national average.\nIndependent samples example\nWe run an independent samples t-test when we have reason to believe that the data in the two samples is NOT meaningfully related in any fashion. Consider this example regarding Joshua Aronson’s work on stereotype threat:\n\nJoshua Aronson has done extensive work on what he refers to as “stereotype threat,” which refers to the fact that “members of stereotyped groups often feel extra pressure in situations where their behavior can confirm the negative reputation that their group lacks a valued ability” (Aronson, Lustina, Good, Keough, Steele, & Brown, 1998). This feeling of stereo- type threat is then hypothesized to affect performance, generally by lowering it from what it would have been had the individual not felt threatened. Considerable work has been done with ethnic groups who are stereotypically reputed to do poorly in some area, but Aronson et al. went a step further to ask if stereotype threat could actually lower the performance of white males—a group that is not normally associated with stereotype threat.\n\n\nAronson et al. (1998) used two independent groups of college students who were known to excel in mathematics, and for whom doing well in math was considered important. They assigned 11 students to a control group that was simply asked to complete a difficult mathematics exam. They assigned 12 students to a threat condition, in which they were told that Asian students typically did better than other students in math tests, and that the purpose of the exam was to help the experimenter to understand why this difference exists. Aronson reasoned that simply telling white students that Asians did better on math tests would arousal feelings of stereotype threat and diminish the students’ performance.\n\nHere we have two mutually exclusive groups of white men, those that are controls and those under induced threat. Importantly we have no reason to believe that any one control man’s score is more closely tied to any individual experimental group counterpart than any others (we’ll return to this idea in a bit).\nHere is the data:\n\nstereotype_data <- read_delim(\"https://www.uvm.edu/~statdhtx/methods8/DataFiles/Tab7-7.dat\", delim = \"\\t\")\n\nRows: 23 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): ID\ndbl (2): Score, Group\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAs before, let’s take a look at the file structure:\n\nshow(stereotype_data)\n\n# A tibble: 23 × 3\n   ID    Score Group\n   <chr> <dbl> <dbl>\n 1 01        4     1\n 2 02        9     1\n 3 03       12     1\n 4 04        8     1\n 5 05        9     1\n 6 06       13     1\n 7 07       12     1\n 8 08       13     1\n 9 09       13     1\n10 10        7     1\n# ℹ 13 more rows\n\n\nI want to look at this example as it give is an opportunity to deal with another common issue in data cleaning. If you take a look at Group you see it’s either 1 or 2. In this case Group 1 are the control subjects and Group 2 are the threat subjects. Using numbers instead of names to identify levels of a factor is a convention from older methods and software. In more modern software you don’t need to do this sort of number coding (the software works this out in the background).\nIf you want to change this, you can use the recode_factor() function from dplyr package in the tidyverse (https://dplyr.tidyverse.org/reference/recode.html). For what it’s worth there are several other ways to do this including a recode() function in car. See http://rprogramming.net/recode-data-in-r/ for examples.\nHere I’m just going to mutate a new column, namedGroup column with the recoded names:\n\nstereotype_data <- stereotype_data %>% \n  mutate(\"namedGroup\" = dplyr::recode_factor(Group,\n                                            \"1\"=\"Control\", \n                                            \"2\"=\"Threat\")\n                            )\nstereotype_data\n\n# A tibble: 23 × 4\n   ID    Score Group namedGroup\n   <chr> <dbl> <dbl> <fct>     \n 1 01        4     1 Control   \n 2 02        9     1 Control   \n 3 03       12     1 Control   \n 4 04        8     1 Control   \n 5 05        9     1 Control   \n 6 06       13     1 Control   \n 7 07       12     1 Control   \n 8 08       13     1 Control   \n 9 09       13     1 Control   \n10 10        7     1 Control   \n# ℹ 13 more rows\n\n\nAn now to run the requisite assumption tests. Note that in this case I am running an Indepednent samples test, so I need to test the assumptions on each sample separately. Here I’m going be a little more critical about how I test for normality.\nUsing psych::describeBy:\n\npsych::describeBy(stereotype_data,group = stereotype_data$namedGroup)\n\n\n Descriptive statistics by group \ngroup: Control\n            vars  n mean   sd median trimmed  mad min max range  skew kurtosis\nID*            1 11 6.00 3.32      6    6.00 4.45   1  11    10  0.00    -1.53\nScore          2 11 9.64 3.17      9    9.89 4.45   4  13     9 -0.31    -1.48\nGroup          3 11 1.00 0.00      1    1.00 0.00   1   1     0   NaN      NaN\nnamedGroup*    4 11 1.00 0.00      1    1.00 0.00   1   1     0   NaN      NaN\n              se\nID*         1.00\nScore       0.96\nGroup       0.00\nnamedGroup* 0.00\n------------------------------------------------------------ \ngroup: Threat\n            vars  n mean   sd median trimmed  mad min max range  skew kurtosis\nID*            1 12 6.50 3.61    6.5     6.5 4.45   1  12    11  0.00    -1.50\nScore          2 12 6.58 3.03    7.0     6.9 2.22   0  10    10 -0.86    -0.39\nGroup          3 12 2.00 0.00    2.0     2.0 0.00   2   2     0   NaN      NaN\nnamedGroup*    4 12 2.00 0.00    2.0     2.0 0.00   2   2     0   NaN      NaN\n              se\nID*         1.04\nScore       0.87\nGroup       0.00\nnamedGroup* 0.00\n\n\nUsing DescTools:\ncontrol group\n\ncontrol_group <- stereotype_data %>% filter(namedGroup==\"Control\") \n\ncontrol_skew <- DescTools::Skew(x=control_group$Score,method = 2,conf.level = .95,ci.type = \"bca\",R = 1000)\ncontrol_skew_ses <- (control_skew[3] - control_skew[2])/3.92\ncontrol_skew[1]/control_skew_ses\n\n      skew \n-0.6060895 \n\ncontrol_kurt <- DescTools::Kurt(x=control_group$Score,method = 2,conf.level = .95,ci.type = \"bca\",R = 1000)\ncontrol_kurt_ses <- (control_kurt[3] - control_kurt[2])/3.92\ncontrol_kurt[1]/control_kurt_ses\n\n     kurt \n-1.094786 \n\n\nthreat group\n\nthreat_group <- stereotype_data %>% filter(namedGroup==\"Threat\") \n\nthreat_skew <- DescTools::Skew(x=threat_group$Score,method = 2,conf.level = .95,ci.type = \"bca\",R = 1000)\nthreat_skew_ses <- (threat_skew[3] - threat_skew[2])/3.92\nthreat_skew[1]/threat_skew_ses\n\n     skew \n-1.867989 \n\nthreat_kurt <- DescTools::Kurt(x=threat_group$Score,method = 2,conf.level = .95,ci.type = \"bca\",R = 1000)\nthreat_kurt_ses <- (threat_kurt[3] - threat_kurt[2])/3.92\nthreat_kurt[1]/threat_kurt_ses\n\n     kurt \n0.4176778 \n\n\nThis is a pain… can we make a function for this!!!!\nQQ-Plot\n\npacman::p_load(qqplotr)\n\nggplot(stereotype_data, aes(sample=Score, group=namedGroup, color=namedGroup)) + \n  geom_qq() + geom_qq_line()\n\n\n\n\nHomogeniety of Variance:\nWhile the paired samples test doesn’t make this assumption, the Independence samples test assumes the variability of scores for the two groups is roughly homogeneous.\nFor a t-test this can be tested by using the leveneTest() from the car package:\n\n# using long-format enter as a formula:\ncar::leveneTest(Score~namedGroup, data=stereotype_data, center=\"mean\")\n\nLevene's Test for Homogeneity of Variance (center = \"mean\")\n      Df F value Pr(>F)\ngroup  1  0.4306 0.5188\n      21               \n\n\nYou’ll note above I elected to mean center my samples. This is consistent with typical practice although “median” centering may be more robust.\n\ncar::leveneTest(data=stereotype_data, Score~namedGroup)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  1  0.4639 0.5033\n      21               \n\n\nT-test Given that my obtained Pr(>F), or p-value of Levene’s F-test, is greater than .05, I may elect to assume that my variances are equal. However, if you remained skeptical, there are adjustments that you may make. This includes adjusting the degrees of freedom according to Welch-Satterthwaite recommendation (see below). Recall later on that we are looking at our obtained \\(t\\) value with respect to the number of \\(df\\). This adjustment effectively reduces the \\(df\\) in turn making your test more conservative.\nThe Levene’s test failed to reject the null so I may proceed with my t.test assuming variances are equal. Note that paired=FALSE for independent sample tests:\n\nt.test(data=stereotype_data, Score~namedGroup, paired=FALSE, var.equal=T)\n\n\n    Two Sample t-test\n\ndata:  Score by namedGroup\nt = 2.3614, df = 21, p-value = 0.02795\nalternative hypothesis: true difference in means between group Control and group Threat is not equal to 0\n95 percent confidence interval:\n 0.3643033 5.7417573\nsample estimates:\nmean in group Control  mean in group Threat \n             9.636364              6.583333 \n\n\nThis output gives us the \\(t\\)-value, \\(df\\) and \\(p\\)-value. Based on this output I may conclude that the mean score in the Control group is significantly greater than the Threat group.\nJust as an example, let’s set var.equal to FALSE:\n\nt.test(data=stereotype_data, Score~namedGroup, paired=FALSE, var.equal=F)\n\n\n    Welch Two Sample t-test\n\ndata:  Score by namedGroup\nt = 2.3565, df = 20.614, p-value = 0.02843\nalternative hypothesis: true difference in means between group Control and group Threat is not equal to 0\n95 percent confidence interval:\n 0.3556143 5.7504463\nsample estimates:\nmean in group Control  mean in group Threat \n             9.636364              6.583333 \n\n\nComparing the outputs you see that in this case R has indicated that it has run the test with the Welsh correction. Note that this changes the \\(df\\) and consequently the resulting \\(p\\) value. That this change was negligible reinforces that the variances were very similar to one another. However in cases where they are not close to one another you may see dramatic changes in \\(df\\).\nIn R, the t.test() function sets var.equal=FALSE by default. Why you ask? Well, you can make the argument that the variances are ALWAYS unequal, its only a matter of degree. Assuming variances are unequal makes your test more conservative, meaning that if the test suggests that you should reject the null, you can be slightly more confident that you are not committing Type I error. At the same time, it could be argued that setting your var.equal=TRUE in this case (where the Levene test failed to reject the null) makes your test more powerful, and you should take advantage of that power to avoid Type II error."
  },
  {
    "objectID": "week06/6_1_t-test.html#independent-or-paired-sample",
    "href": "week06/6_1_t-test.html#independent-or-paired-sample",
    "title": "Testing differences in means / t-test",
    "section": "Independent or Paired Sample?",
    "text": "Independent or Paired Sample?\nIt is safe to assume that anytime that you are collecting data samples from the same person at two different points in time that you need to run a paired-samples test. However, it would not be safe to assume that if the samples are coming from different groups of people that you always run an independent samples test. Remember the important qualifier mentioned above: That there no reason to believe that any one participant in the first group is is more closely related to any single counterpart in the second group than the remaining of others. In our Independent test example we have no reason to assume this is the case, we assume that members of the Control and Threat groups were randomly selected. But what if we instead recruited brothers or twins? In this case, it may make sense to treat members of the two groups as paired; brothers have a shared history (education, socio-economic level, family dynamic, etc) that would make their scores more likely to be related to one another than by random chance."
  },
  {
    "objectID": "week09/9_1-posthocs.html",
    "href": "week09/9_1-posthocs.html",
    "title": "Multiple comparisons in One-way ANOVA, pt. 1: post hoc tests",
    "section": "",
    "text": "This walkthrough requires the following to be installed / loaded in R\n\nBe sure that the column that contains your IV is indeed being treated as a factor. If it is levels(IV) will list your levels, also the column containing the IV will contain <fct>.\nbuild an ANOVA model using lm(), aov(), or afex() (output to lm)\nrun your post-hoc comparisions using the p.adjustment of your choice (tukey, holm, bonferroni)\n\nExample (I recommend running this line-by-line)\n\n# Preliminaries\n## load in data\ndataset <- read_table2(\"https://www.uvm.edu/~statdhtx/methods8/DataFiles/Tab12-1.dat\")\n\nWarning: `read_table2()` was deprecated in readr 2.0.0.\nℹ Please use `read_table()` instead.\n\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  ID = col_character(),\n  Group = col_double(),\n  Time = col_double()\n)\n\n# Step 1\n## check data, identify columns\ndataset\n\n# A tibble: 40 × 3\n   ID    Group  Time\n   <chr> <dbl> <dbl>\n 1 01        1     3\n 2 02        1     5\n 3 03        1     1\n 4 04        1     8\n 5 05        1     1\n 6 06        1     1\n 7 07        1     4\n 8 08        1     9\n 9 09        2     2\n10 10        2    12\n# ℹ 30 more rows\n\n## Group is numerically coded. Fixing this and turning to a factor\ndataset$Group <- recode_factor(dataset$Group, \"1\"=\"MS\",\"2\"=\"MM\",\"3\"=\"SS\",\"4\"=\"SM\",\"5\"=\"McM\")\n\n# Step 2: Run the model\nmodel_aov <- lm(Time~Group, data = dataset)\n\n# Step 3: Test the model for significant F-value\nmodel_aov %>% sjstats::anova_stats()\n\nterm      | df |    sumsq |  meansq | statistic | p.value | etasq | partial.etasq | omegasq | partial.omegasq | epsilonsq | cohens.f | power\n--------------------------------------------------------------------------------------------------------------------------------------------\nGroup     |  4 | 3497.600 | 874.400 |    27.325 |  < .001 | 0.757 |         0.757 |   0.725 |           0.725 |     0.730 |    1.767 |     1\nResiduals | 35 | 1120.000 |  32.000 |           |         |       |               |         |                 |           |          |      \n\n# Step 4: post-hoc analysis in this example \"tukey\", but could also be \"bonf\" or \"holm\"\nemmeans(object = model_aov,specs = ~Group, adjust=\"tukey\") %>% pairs()\n\n contrast estimate   SE df t.ratio p.value\n MS - MM        -6 2.83 35  -2.121  0.2340\n MS - SS        -7 2.83 35  -2.475  0.1198\n MS - SM       -20 2.83 35  -7.071  <.0001\n MS - McM      -25 2.83 35  -8.839  <.0001\n MM - SS        -1 2.83 35  -0.354  0.9965\n MM - SM       -14 2.83 35  -4.950  0.0002\n MM - McM      -19 2.83 35  -6.718  <.0001\n SS - SM       -13 2.83 35  -4.596  0.0005\n SS - McM      -18 2.83 35  -6.364  <.0001\n SM - McM       -5 2.83 35  -1.768  0.4078\n\nP value adjustment: tukey method for comparing a family of 5 estimates"
  },
  {
    "objectID": "week09/9_1-posthocs.html#comparing-means-in-the-anova-model",
    "href": "week09/9_1-posthocs.html#comparing-means-in-the-anova-model",
    "title": "Multiple comparisons in One-way ANOVA, pt. 1: post hoc tests",
    "section": "Comparing means in the ANOVA model",
    "text": "Comparing means in the ANOVA model\nIn AoV, part 1, introduced the One-Way ANOVA. ANOVA is useful when we are comparing 3 or more group means such that the null hypothesis is:\n\\[\\mu_1=\\mu_2=\\mu_3...=\\mu_n\\].\nIn this case, if a single mean is revealed to be significantly different from the others, then the null is rejected. However, rejecting the null only tells us that at least one mean was different from the others; it does not tell us which one or how many. For example with just three means, it could be the case that:\n\n\\(\\mu_1≠\\mu_2=\\mu_3\\)\n\\(\\mu_1=\\mu_2≠\\mu_3\\)\n\\(\\mu_1=\\mu_3≠\\mu_2\\)\n\\(\\mu_1≠\\mu_2≠\\mu_3\\)\n\nSimply getting a significant F-value does not tell us this at all. In order to suss out any differences in our groups we are going to need to make direct comparisons between them.\nEnter multiple contrasts. Multiple contrasts are a way of testing the potential inequalities between group means like those above. As always, both Navarro and Poldrack do wonderful jobs of laying out the mathematics and logic of multiple comparisons. As with Part 1 I focus on practical implementation and spend some time focusing a bit on potential landmines and theoretical concerns as I see them.\nThis vignette assumes that you have the following packages installed and loaded in R:\n\n# use pacman to check, install, and load necessary packages\npacman::p_load(agricolae,\n               cowplot, \n               tidyverse, \n               emmeans,\n               multcomp,\n               psych,\n               sjstats)"
  },
  {
    "objectID": "week09/9_1-posthocs.html#the-data-siegels-1975-study-on-the-effects-of-morphine",
    "href": "week09/9_1-posthocs.html#the-data-siegels-1975-study-on-the-effects-of-morphine",
    "title": "Multiple comparisons in One-way ANOVA, pt. 1: post hoc tests",
    "section": "The data: Siegel’s 1975 study on the effects of morphine",
    "text": "The data: Siegel’s 1975 study on the effects of morphine\nTo start, lets download Siegel’s (1975) data set on Morphine Tolerance. This data set can be found on the web. Before diving into the data, check a description of the experiment in the Siegel_summary.pdf file in the walkthroughs folder. When you are done, come back a we’ll work on analyzing this data.\n\n# grab data from online location:\ndataset <- read_table2(\"https://www.uvm.edu/~statdhtx/methods8/DataFiles/Tab12-1.dat\")\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  ID = col_character(),\n  Group = col_double(),\n  Time = col_double()\n)\n\n# convert dataset$Group number codes to named factor levels:\ndataset$Group <- recode_factor(dataset$Group, \"1\"=\"MS\",\"2\"=\"MM\",\"3\"=\"SS\",\"4\"=\"SM\",\"5\"=\"McM\")\n\n# get descriptive stats for this data by Group\npsych::describeBy(dataset$Time,dataset$Group)\n\n\n Descriptive statistics by group \ngroup: MS\n   vars n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 8    4 3.16    3.5       4 3.71   1   9     8 0.43    -1.59 1.12\n------------------------------------------------------------ \ngroup: MM\n   vars n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 8   10 5.13   10.5      10 4.45   2  19    17 0.15    -0.99 1.81\n------------------------------------------------------------ \ngroup: SS\n   vars n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 8   11 6.72   10.5      11 8.15   3  21    18 0.23    -1.69 2.38\n------------------------------------------------------------ \ngroup: SM\n   vars n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 8   24 6.37     23      24 5.93  17  36    19 0.59    -1.07 2.25\n------------------------------------------------------------ \ngroup: McM\n   vars n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 8   29 6.16   28.5      29 5.93  20  40    20 0.28    -1.07 2.18\n\n\nAnd a quick peek at this data:\n\nggplot(data = dataset,aes(x=Group,y=Time)) +\n  stat_summary(fun = mean, geom = \"bar\") +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", aes(width=.25)) +\n  scale_y_continuous(expand = c(0,0)) + expand_limits(y=c(0,35)) + theme_cowplot()"
  },
  {
    "objectID": "week09/9_1-posthocs.html#running-the-one-way-anova",
    "href": "week09/9_1-posthocs.html#running-the-one-way-anova",
    "title": "Multiple comparisons in One-way ANOVA, pt. 1: post hoc tests",
    "section": "Running the One-way ANOVA",
    "text": "Running the One-way ANOVA\nNow that our data is properly coded we can run our omnibus ANOVA. My own personal preference is to run the ANOVA using lm(). This makes like a lot easier when dealing with contrasts, especially if you decide to employ the method that Field suggests in his guide. FWIW, I typically use another method as seen below, but I’ll talk a little bit about why I prefer it to Fields method. That said, recall from Part 1 that using the aov() function gives you the same result. Depending on which you choose, you can use the summary(morphine_mdl) or anova(morphine_mdl) to switch back and forth to get the info that you desire:\n\n# running the ANOVA using lm:\nmorphine_mdl <- lm(formula = Time~Group,data = dataset)\n# using the anova() function to display as ANOVA table\nanova(morphine_mdl)\n\nAnalysis of Variance Table\n\nResponse: Time\n          Df Sum Sq Mean Sq F value    Pr(>F)    \nGroup      4 3497.6   874.4  27.325 2.443e-10 ***\nResiduals 35 1120.0    32.0                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# full anova output (preferred)\nsjstats::anova_stats(morphine_mdl)\n\nterm      | df |    sumsq |  meansq | statistic | p.value | etasq | partial.etasq | omegasq | partial.omegasq | epsilonsq | cohens.f | power\n--------------------------------------------------------------------------------------------------------------------------------------------\nGroup     |  4 | 3497.600 | 874.400 |    27.325 |  < .001 | 0.757 |         0.757 |   0.725 |           0.725 |     0.730 |    1.767 |     1\nResiduals | 35 | 1120.000 |  32.000 |           |         |       |               |         |                 |           |          |      \n\n\nSo we see here that we have: \\(F(4,35)=27.33,p<.001,\\eta_p^2=.75\\)\nRemember again that the only thing that the omnibus ANOVA tells us is that there is an inequality in our means. In this respect, the omnibus begs more questions than it answers—which means are different from which. In order to get this answer we need to run direct comparisons between our means. There are two ways of going about this, we can either (1) plan beforehand what differences in means are especially relevant for us and focus on those, or (2) take a look at all potential differences without any specified predictions. In Case 1, we are performing planned contrasts; in Case 2, we use post hoc tests. More often than not, you will see researchers analyzing differences in means using post hoc tests—that is they run the ANOVA, find that it is significant, and run a battery of pairwise comparisons. It is sometimes the case that of that battery of comparisons, only a select few are actually theoretically relevant. However, if there is a theory-driven case to be made that you are predicting differences between a few select means in your data, then there is an argument to be made that you should run your planned contrasts independent of your ANOVA. That is, you are technically only permitted to run post-hoc tests if your ANOVA is significant (you can only go looking for differences in means if your ANOVA tells you that they exist), whereas planned contrasts can be run regardless of the outcome of the omnibus ANOVA (indeed, some argue that they obviate the need to run the omnibus ANOVA altogether).\nMy guess is that most of you have experience with post-hoc tests. They are more commonly performed tend to be touched upon in introductory stats courses. So we will spend a little time on these first before proceeding to a more in depth treatment of planned contrasts."
  },
  {
    "objectID": "week09/9_1-posthocs.html#post-hoc-tests",
    "href": "week09/9_1-posthocs.html#post-hoc-tests",
    "title": "Multiple comparisons in One-way ANOVA, pt. 1: post hoc tests",
    "section": "Post-hoc tests",
    "text": "Post-hoc tests\nWe use a post-hoc test when we want to test for differences in means that we have not explicitly predicted prior to conducting our experiment. As a result, whenever we perform a post-hoc test, we need to adjust our critical p-values to correct for inflation of Type 1 error. Recall from earlier discussions that the odds of committing a Type 1 error (falsely rejecting the null) is \\(1-(1-\\alpha)^c\\) where \\(\\alpha\\) is you critical p-value and \\(c\\) is the number of comparisons that are to be performed. Typically we keep this at .05, so when conducting a single test, the likelihood of committing a Type 1 error is: \\(1-(1-.05)^1=1-0.95^1=0.05\\)\nHowever as we increase the number of comparisons, assuming an \\(\\alpha\\) of 0.05:\n\n2 comparisons = \\(1-.95^2=0.0975\\)\n\n3 comparisons = \\(1-.95^3=0.1426\\)\n\n4 comparisons = \\(1-.95^4=0.1855\\)\n\n5 comparisons = \\(1-.95^5=0.2262\\)\n\n\nObviously, we need to control for this. The post-hoc methods that were introduced this week are all similar in that they involve comparing two means (a la t-test) but differ in how the error is controlled. For example a Bonferroni-Dunn correction (which is often used as a post-hoc correction, although initially intended for correcting planned comparisons) adjusts for this by partitioning the significance (by diving your original alpha by the number of comparisons). A popular variant of this method, the Holm test, is a multistage test. It proceeds by ordering the obtained t-values from smallest to largest. We then evaluate the largest t according to the Bonferroni-Dunn correction \\(\\alpha/c\\). Each subsequent comparison t value, \\(n\\) is evaluated against the correction \\(\\alpha/(c-n)\\). Please note I mention the these two methods with post-hoc analyses, although in true they are intended for planned comparisons. However, in instances in which the number of comparisons is relatively small, I’ve often seen them employed as post-hocs.\nSo how many comparisons is relatively small? I’d suggest best form is to use the above methods when you have 5 or fewer comparisons, meaning that your critical \\(\\alpha\\) is .01. That said, with a post hoc test, you really do not have a choice in the number of comparisons you can make, you need to test for all possible comparisons on the IV. Why? well if not you are simply cherry picking your data. For example it would be poor form to take a look at our data like so:\nPlot:\n\nggplot(data = dataset,aes(x=Group,y=Time)) +\n  stat_summary(fun.y = mean, geom = \"bar\") +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", aes(width=.25)) +\n  scale_y_continuous(expand = c(0,0)) + expand_limits(y=c(0,35)) + \n  theme_cowplot()\n\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\nℹ Please use the `fun` argument instead.\n\n\n\n\n\nand then decide that you only want to compare ‘McM’ to ‘MS’ because that’s where you see the greatest differences. Or that you simply want to take a look at “MM” and “SS” without considering the rest.\nSince you did not plan for or explicitly predict these differences from the outset, you are simply banking on what I like to say might be a “historical accident”, that you simply stumbled into these results. As such, it’s deemed as proper for to test all contingencies.\nIn the case above there are \\((5!)/(2!)(5-2)!\\) = 10 combinations. If we were to run a Bonferroni correction in this case or critical \\(p\\) would need to be \\(.05/10=.005\\) which is an extremely conservative value, and thus dramatically inflates the likelihood of Type II error. In cases like this, Tukey’s HSD is the traditionally preferred method, as it takes into account the characteristics of your data (in particular the standard error of the distribution) when calculating the critical \\(p\\) value. As such in cases where many post-hoc, pairwise comparisons are made, Tukey’s HSD is less conservative than a Bonferroni adjustment.\nOne final method that is becoming more en vogue is the Ryan, Einot, Gabriel, Welsch method (REGWQ). Whereas Tukey’s method holds the critical \\(p\\) constant for all comparisons (at the loss of power) the REGWQ allows for an adjustment for the number of comparisons. It is currently being promoted as the most desirable post-hoc method.\nBonferonni-Dunn and Holm tests\nIn R there are several ways in which we can call post hoc corrections. For example we can call the Bonferonni and Holm adjustments using pairwise.t.test() function from the base package (already installed). The pairwise.t.test() method asks you to input:\n\n\nx = your DV\n\ng = your grouping factor\n\np.adjust.method = the name of your desired correction in string format\n\nFirst let’s run the pairwise.t.tests with no adjustment (akin to uncorrected \\(p\\) values):\n\npairwise.t.test(x = dataset$Time, g = dataset$Group,p.adjust.method = \"none\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  dataset$Time and dataset$Group \n\n    MS      MM      SS      SM   \nMM  0.041   -       -       -    \nSS  0.018   0.726   -       -    \nSM  3.1e-08 1.9e-05 5.4e-05 -    \nMcM 1.9e-10 8.9e-08 2.6e-07 0.086\n\nP value adjustment method: none \n\n\nYou see above that we get a cross-matrix containing the \\(p\\) values for each cross pair (row × column). Remember this is something we would never do in a post hoc (no corrections) but I wanted to first run this to illustrate a point. Now let’s run the the Bonferroni and Holm corrections:\nBonferroni example (pairwise.t.test())\n\npairwise.t.test(x = dataset$Time, g = dataset$Group,p.adjust.method = \"bonferroni\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  dataset$Time and dataset$Group \n\n    MS      MM      SS      SM     \nMM  0.41051 -       -       -      \nSS  0.18319 1.00000 -       -      \nSM  3.1e-07 0.00019 0.00054 -      \nMcM 1.9e-09 8.9e-07 2.6e-06 0.85818\n\nP value adjustment method: bonferroni \n\n\nYou’ll note that the p-values displayed here are 10x the p-values from the uncorrected matrix. To demonstrate this:\n\nuncorrected <- pairwise.t.test(x = dataset$Time, g = dataset$Group,p.adjust.method = \"none\")\nbonf_corrected <- pairwise.t.test(x = dataset$Time, g = dataset$Group,p.adjust.method = \"bonferroni\")\n\nbonf_corrected$p.value/uncorrected$p.value\n\n    MS        MM SS SM\nMM  10        NA NA NA\nSS  10  1.377801 NA NA\nSM  10 10.000000 10 NA\nMcM 10 10.000000 10 10\n\n\n\nnote that the 1.378 value is the result of p being capped at \\(p=1\\) in the Bonferroni corrected comparison.\n\nRemember from a few paragraphs back that there are 10 possible combinations so the Bonferonni test would need to divide the critical alpha by 10. What this means is that anytime you perform a correction, R actually adjusts the \\(p\\) values for you; therefore you may interpret the output against your original (familywise) \\(\\alpha\\). So here, any values that are still less than .05 after the corrections are significant.\nMoving on…\nHolm example (pairwise.t.test())\n\npairwise.t.test(x = dataset$Time, g = dataset$Group,p.adjust.method = \"holm\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  dataset$Time and dataset$Group \n\n    MS      MM      SS      SM     \nMM  0.12315 -       -       -      \nSS  0.07327 0.72579 -       -      \nSM  2.8e-07 0.00011 0.00027 -      \nMcM 1.9e-09 7.1e-07 1.8e-06 0.17164\n\nP value adjustment method: holm"
  },
  {
    "objectID": "week09/9_1-posthocs.html#tukey-hsd-and-regwq-tests",
    "href": "week09/9_1-posthocs.html#tukey-hsd-and-regwq-tests",
    "title": "Multiple comparisons in One-way ANOVA, pt. 1: post hoc tests",
    "section": "Tukey HSD and REGWQ tests",
    "text": "Tukey HSD and REGWQ tests\nIn order to run Tukey’s HSD and REGWQ methods we call upon the agricolae package. In this case, we need to input our lm() model into the function, as well as identify our “treatment” (in this case our “Group” factor). For example:\nTukey HSD example (agricolae)\n\nmorphine_mdl <- lm(Time~Group,data = dataset) # from above\nagricolae::HSD.test(morphine_mdl,trt = \"Group\",group = T,console = T) \n\n\nStudy: morphine_mdl ~ \"Group\"\n\nHSD Test for Time \n\nMean Square Error:  32 \n\nGroup,  means\n\n    Time      std r Min Max\nMcM   29 6.164414 8  20  40\nMM    10 5.126960 8   2  19\nMS     4 3.162278 8   1   9\nSM    24 6.369571 8  17  36\nSS    11 6.718843 8   3  21\n\nAlpha: 0.05 ; DF Error: 35 \nCritical Value of Studentized Range: 4.065949 \n\nMinimun Significant Difference: 8.131899 \n\nTreatments with the same letter are not significantly different.\n\n    Time groups\nMcM   29      a\nSM    24      a\nSS    11      b\nMM    10      b\nMS     4      b\n\n\nNote that the group and console arguments pertain to the output. You typically will want to keep console set to TRUE as that simply prints the output of your test. The group argument controls how the output is presented. Above we set it to TRUE. This results in an output that groups the treatment means into subsets where treatments with the same letter are not significantly different from one another, known as compact letter displays. For example, as are not significantly different from each other, bs are not significantly different from each other, but as are different from bs. Conversely if you wanted to see each comparison you can set this to FALSE:\n\nagricolae::HSD.test(morphine_mdl,trt = \"Group\",group = FALSE,console = TRUE) \n\n\nStudy: morphine_mdl ~ \"Group\"\n\nHSD Test for Time \n\nMean Square Error:  32 \n\nGroup,  means\n\n    Time      std r Min Max\nMcM   29 6.164414 8  20  40\nMM    10 5.126960 8   2  19\nMS     4 3.162278 8   1   9\nSM    24 6.369571 8  17  36\nSS    11 6.718843 8   3  21\n\nAlpha: 0.05 ; DF Error: 35 \nCritical Value of Studentized Range: 4.065949 \n\nComparison between treatments means\n\n         difference pvalue signif.        LCL        UCL\nMcM - MM         19 0.0000     ***  10.868101  27.131899\nMcM - MS         25 0.0000     ***  16.868101  33.131899\nMcM - SM          5 0.4078          -3.131899  13.131899\nMcM - SS         18 0.0000     ***   9.868101  26.131899\nMM - MS           6 0.2340          -2.131899  14.131899\nMM - SM         -14 0.0002     *** -22.131899  -5.868101\nMM - SS          -1 0.9965          -9.131899   7.131899\nMS - SM         -20 0.0000     *** -28.131899 -11.868101\nMS - SS          -7 0.1198         -15.131899   1.131899\nSM - SS          13 0.0005     ***   4.868101  21.131899\n\n\nFinally, if you do decide to group (group=TRUE), you can take the outcome of this function and use it to generate a nice group plot. This is useful for quick visual inspection.\n\nagricolae::HSD.test(morphine_mdl,trt = \"Group\",group = T,console = T) %>% plot()\n\n\nStudy: morphine_mdl ~ \"Group\"\n\nHSD Test for Time \n\nMean Square Error:  32 \n\nGroup,  means\n\n    Time      std r Min Max\nMcM   29 6.164414 8  20  40\nMM    10 5.126960 8   2  19\nMS     4 3.162278 8   1   9\nSM    24 6.369571 8  17  36\nSS    11 6.718843 8   3  21\n\nAlpha: 0.05 ; DF Error: 35 \nCritical Value of Studentized Range: 4.065949 \n\nMinimun Significant Difference: 8.131899 \n\nTreatments with the same letter are not significantly different.\n\n    Time groups\nMcM   29      a\nSM    24      a\nSS    11      b\nMM    10      b\nMS     4      b\n\n\n\n\n\nREGWQ example (agricolae)\nThe same applies to REGW, using the REGW.test() function (with group=F, I’m showing all of the comparisons):\n\nagricolae::REGW.test(morphine_mdl,trt = \"Group\",group = F,console = T) \n\n\nStudy: morphine_mdl ~ \"Group\"\n\nRyan, Einot and Gabriel and Welsch multiple range test\nfor Time \n\nMean Square Error:  32 \n\nGroup,  means\n\n    Time      std r Min Max\nMcM   29 6.164414 8  20  40\nMM    10 5.126960 8   2  19\nMS     4 3.162278 8   1   9\nSM    24 6.369571 8  17  36\nSS    11 6.718843 8   3  21\n\nComparison between treatments means\n\n         difference pvalue signif.         LCL        UCL\nMcM - MM         19 0.0000     ***  12.1234674  25.876533\nMcM - MS         25 0.0000     ***  17.4611210  32.538879\nMcM - SM          5 0.3056          -2.6279930  12.627993\nMcM - SS         18 0.0000     ***   9.8681013  26.131899\nMM - MS           6 0.0995       .  -0.8765326  12.876533\nMM - SM         -14 0.0001     *** -21.5388790  -6.461121\nMM - SS          -1 0.9846          -8.6279930   6.627993\nMS - SM         -20 0.0000     *** -26.8765326 -13.123467\nMS - SS          -7 0.0771       . -14.5388790   0.538879\nSM - SS          13 0.0001     ***   6.1234674  19.876533\n\n\nCompact letter displays are nice (SPSS generates them, too), but as seems to always be the case, there is some controversy as to whether we should use them. Taken from this vignette:\n\nCLD displays promote visually the idea that two means that are “not significantly different” are to be judged as being equal; and that is a very wrong interpretation. In addition, they draw an artificial “bright line” between P values on either side of alpha, even ones that are very close."
  },
  {
    "objectID": "week09/9_1-posthocs.html#alternative-to-cld",
    "href": "week09/9_1-posthocs.html#alternative-to-cld",
    "title": "Multiple comparisons in One-way ANOVA, pt. 1: post hoc tests",
    "section": "Alternative to CLD()",
    "text": "Alternative to CLD()\nOne thing to note, the curator of the emmeans() package is not a fan of cld (Compact Letter Displays) and instead has created pwpp as a visualization tool.\n\nemmeans(morphine_mdl, specs = ~Group) %>% pwpp()\n\n\n\n\nYou can visit this vignette to get a feel for what is at issue here."
  },
  {
    "objectID": "week09/9_1-posthocs.html#effect-sizes",
    "href": "week09/9_1-posthocs.html#effect-sizes",
    "title": "Multiple comparisons in One-way ANOVA, pt. 1: post hoc tests",
    "section": "Effect sizes",
    "text": "Effect sizes\nTypically when reporting the effect size of the difference between two means we use Cohen’s \\(d\\). However, calculating Cohen’s $d$ in a posthoc contrast is slightly more involved than the method used for a regular t-test. This is because with a regular t-test you only have 2 means from 2 samples that you have collected. In the case of pairwise contrasts in ANOVA, while you are only comparing two means, those means are nested within a larger group (e.g., when comparing MS and MM, we still need to account for the fact that we also collected samples from SS, SM, and McM). That is, you need to understand the difference between the two means in the context of the entire model.\nSimply put, in our calculations we need to account for the influence of all of our collected groups. This is done by placing the contrasted difference in the context of the Root Mean Square Error, or the square root of the Mean Square Error of the residuals in our ANOVA model. Recall that typically Cohen’s \\(d\\) is the difference between the two means divided by their pooled standard deviation. Here, \\(d\\) is the difference between the two means divided by sigma, or the estimated standard deviation of the errors of the linear model.\nTo do this we’re going to need two things from the original model. Let’s take a look at the model summary:\n\nmorphine_mdl %>% summary()\n\n\nCall:\nlm(formula = Time ~ Group, data = dataset)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -9.00  -3.25   0.00   3.00  12.00 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    4.000      2.000   2.000   0.0533 .  \nGroupMM        6.000      2.828   2.121   0.0411 *  \nGroupSS        7.000      2.828   2.475   0.0183 *  \nGroupSM       20.000      2.828   7.071 3.09e-08 ***\nGroupMcM      25.000      2.828   8.839 1.93e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.657 on 35 degrees of freedom\nMultiple R-squared:  0.7574,    Adjusted R-squared:  0.7297 \nF-statistic: 27.32 on 4 and 35 DF,  p-value: 2.443e-10\n\n\nThe values line that is important to us is the Residual standard error ___ on __ degrees of freedom—in this case the values 5.657 and 35 respectively.\nAssuming you have run your comparisons using emmeans() you can calculate your effect sizes for each comparison using emmeans::eff_size().\neff_size() takes three arguments:\n\nthe emmeans object\nthe estimated standard deviation of the errors of the linear model, sigma\n\nthe residual degrees of freedom from the model, df.residual\n\n\nThe functions sigma() and df.residual() allow us to get this information directly from the model. A typical call would look something like this\n\n# save your contrasts to an object\nmdl_contrasts <- emmeans(morphine_mdl, specs = ~Group)\n\n# use the saved object in the following function\neff_size(mdl_contrasts,sigma = sigma(morphine_mdl), df.residual(morphine_mdl))\n\n contrast effect.size    SE df lower.CL upper.CL\n MS - MM       -1.061 0.516 35    -2.11  -0.0135\n MS - SS       -1.237 0.521 35    -2.30  -0.1789\n MS - SM       -3.536 0.655 35    -4.86  -2.2065\n MS - McM      -4.419 0.727 35    -5.90  -2.9428\n MM - SS       -0.177 0.500 35    -1.19   0.8392\n MM - SM       -2.475 0.581 35    -3.65  -1.2955\n MM - McM      -3.359 0.641 35    -4.66  -2.0570\n SS - SM       -2.298 0.570 35    -3.46  -1.1400\n SS - McM      -3.182 0.628 35    -4.46  -1.9067\n SM - McM      -0.884 0.511 35    -1.92   0.1536\n\nsigma used for effect sizes: 5.657 \nConfidence level used: 0.95 \n\n\nThat said, there is some debate as to whether this is the most appropriate way to calculate posthoc effect sizes, or whether posthoc effect sizes are in general a proper thing to calculate. I’ve personally never had a reviewer ask for one, BUT if I had to provide on I would use this method.\nFWIW, this won’t be the last time that we need to call back to the original (omnibus) ANOVA when conducting posthoc tests. Things get a little messier next week!"
  },
  {
    "objectID": "week09/9_1-posthocs.html#reporting-and-anova-with-post-hoc-analyses.",
    "href": "week09/9_1-posthocs.html#reporting-and-anova-with-post-hoc-analyses.",
    "title": "Multiple comparisons in One-way ANOVA, pt. 1: post hoc tests",
    "section": "Reporting and ANOVA with post hoc analyses.",
    "text": "Reporting and ANOVA with post hoc analyses.\nIn your report, you need to include information for the main ANOVA as well as information related to\n\nthe omnibus ANOVA\na statement on what multiple comparisons were run and how corrected\nnote which comparisons were significant.\n\nIf we work off of our last example, you’ll note that there are quite a few comparisons that we could discuss (10 in fact). In cases like these you could either put this information into a formatted table, or simply highlight a few that are especially relevant. For example, looking at the emmeans() outcome as well as the CLD plot from agricolae::HSD.test we see that McM and SM (a’s) in the CLD plot are both significantly greater than the remain conditions (b’s). Based on this I would write something like:\n… Our ANOVA revealed a significant effect for Morphine treatment group, F(4, 35) = 27.325, p < .001. Tukey pairwise comparisons revealed that the mean tolerance times for the both the SM (\\(M±SD\\): 24.00 ± 6.37) and McM (29.00 ± 6.16) groups were greater than the remaining three groups (\\(ps\\) < .05), but not different from one another. The mean times for the remaining three conditions were not significantly different from one another, \\(ps > .05\\) (see Figure 1)\n\nassuming Figure 1 is a camera ready plot you’ve created in ggplot(). Any of the barplots in this walkthrough will suffice."
  }
]